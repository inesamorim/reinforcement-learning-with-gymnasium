{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUdYox0x5iZ7"
      },
      "source": [
        "# Customizing OpenAI Gym Environments and Implementing Reinforcement Learning Agents with Stable Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS6bS9Lz5iZ9"
      },
      "source": [
        "### Theme: Car Racing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9iRRhHh5iZ-"
      },
      "source": [
        "- Constança Fernandes, 202205398\n",
        "- Daniela Osório, 202208679\n",
        "- Inês Amorim, 202108108"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOvR_5_v5iZ_"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8d4NC7O5iZ_"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN_zSzgG5iZ_"
      },
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA6X11435iaB"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vwzw224B5pxE",
        "outputId": "68d7a365-45b0-4085-c59f-2488f3e9fc1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jupyter_http_over_ws"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl9Xc51B5zCO",
        "outputId": "af58fe14-cf1d-4cd1-94dc-2da6266b4546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jupyter_http_over_ws\n",
            "  Downloading jupyter_http_over_ws-0.0.8-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: notebook>=5.0 in /usr/local/lib/python3.10/dist-packages (from jupyter_http_over_ws) (6.5.5)\n",
            "Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter_http_over_ws) (1.17.0)\n",
            "Requirement already satisfied: tornado>=4.5 in /usr/local/lib/python3.10/dist-packages (from jupyter_http_over_ws) (6.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=5.0->jupyter_http_over_ws) (3.1.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=5.0->jupyter_http_over_ws) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=5.0->jupyter_http_over_ws) (23.1.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=5.0->jupyter_http_over_ws) (5.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=5.0->jupyter_http_over_ws) (5.7.2)\n",
            "Requirement already satisfied: jupyter-client<8,>=5.3.4 in /usr/local/lib/python3.10/dist-packages (from notebook>=5.0->jupyter_http_over_ws) (6.1.12)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from notebook>=5.0->jupyter_http_over_ws) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=5.0->jupyter_http_over_ws) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=5.0->jupyter_http_over_ws) (7.16.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=5.0->jupyter_http_over_ws) (1.6.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from notebook>=5.0->jupyter_http_over_ws) (5.5.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=5.0->jupyter_http_over_ws) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=5.0->jupyter_http_over_ws) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=5.0->jupyter_http_over_ws) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=5.0->jupyter_http_over_ws) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<8,>=5.3.4->notebook>=5.0->jupyter_http_over_ws) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=5.0->jupyter_http_over_ws) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=5.0->jupyter_http_over_ws) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=5.0->jupyter_http_over_ws) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=5.0->jupyter_http_over_ws) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=5.0->jupyter_http_over_ws) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=5.0->jupyter_http_over_ws) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=5.0->jupyter_http_over_ws) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=5.0->jupyter_http_over_ws) (3.0.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=5.0->jupyter_http_over_ws) (0.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=5.0->jupyter_http_over_ws) (24.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=5.0->jupyter_http_over_ws) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=5.0->jupyter_http_over_ws) (2.18.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=5.0->jupyter_http_over_ws) (1.4.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=5.0->jupyter_http_over_ws) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=5.0->jupyter_http_over_ws) (4.23.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook>=5.0->jupyter_http_over_ws) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=5.0->jupyter_http_over_ws) (21.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->notebook>=5.0->jupyter_http_over_ws) (7.34.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert>=5->notebook>=5.0->jupyter_http_over_ws) (0.5.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_http_over_ws) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_http_over_ws)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_http_over_ws) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_http_over_ws) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_http_over_ws) (3.0.48)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_http_over_ws) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_http_over_ws) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_http_over_ws) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=5.0->jupyter_http_over_ws) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=5.0->jupyter_http_over_ws) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=5.0->jupyter_http_over_ws) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=5.0->jupyter_http_over_ws) (0.22.3)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=5.0->jupyter_http_over_ws) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=5.0->jupyter_http_over_ws) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=5.0->jupyter_http_over_ws) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=5.0->jupyter_http_over_ws) (2.22)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_http_over_ws) (0.8.4)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=5.0->jupyter_http_over_ws) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=5.0->jupyter_http_over_ws) (1.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_http_over_ws) (0.2.13)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=5.0->jupyter_http_over_ws) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=5.0->jupyter_http_over_ws) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=5.0->jupyter_http_over_ws) (1.2.2)\n",
            "Downloading jupyter_http_over_ws-0.0.8-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, jupyter_http_over_ws\n",
            "Successfully installed jedi-0.19.2 jupyter_http_over_ws-0.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install stable-baselines3 gym[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B5024lf53Rf",
        "outputId": "1ee31229-123b-40c1-82ed-37c5bee509ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.3.0)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Collecting pygame>=2.1.3 (from gymnasium[box2d])\n",
            "  Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
            "Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.1.0\n",
            "    Uninstalling pygame-2.1.0:\n",
            "      Successfully uninstalled pygame-2.1.0\n",
            "Successfully installed pygame-2.6.1\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.0.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.8.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.3.5)\n",
            "Collecting pygame==2.1.0 (from gym[box2d])\n",
            "  Using cached pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (4.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (3.0.2)\n",
            "Using cached pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.6.1\n",
            "    Uninstalling pygame-2.6.1:\n",
            "      Successfully uninstalled pygame-2.6.1\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/reinforcement-learning-with-gymnasium-main')"
      ],
      "metadata": {
        "id": "IRRp_p3j59xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import GrayscaleObservation, ResizeObservation, RecordEpisodeStatistics, RecordVideo, TimeLimit\n",
        "from stable_baselines3 import DQN, PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.atari_wrappers import WarpFrame\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, VecVideoRecorder\n",
        "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.vec_env import VecTransposeImage\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "import os\n",
        "import gc\n",
        "from eval import *\n",
        "from custom_cr import EnhancedCarRacing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Libjb3iz6QW_",
        "outputId": "4d21172d-0242-49eb-e29b-11798589df89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
            "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
            "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8q7FYAk5iaD"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfuJLngj5iaE"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq1gBMhR5iaF"
      },
      "source": [
        "The CarRacing-v3 environment from Gymnasium (previously Gym) is part of the Box2D environments, and it offers an interesting challenge for training reinforcement learning agents. It's a top-down racing simulation where the track is randomly generated at the start of each episode. The environment offers both continuous and discrete action spaces, making it adaptable to different types of reinforcement learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0s1FaxW5iaF",
        "outputId": "6d7fdaae-d923-40fd-f62f-56cb94c1916e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pygame/pkgdata.py:27: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CarRacing-v3\", continuous=False, render_mode='rgb_array')\n",
        "obs, info = env.reset()\n",
        "#continuous = False to use Discrete space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTE0MQXQ5iaF"
      },
      "source": [
        "- **Action Space:**\n",
        "\n",
        "   - **Continuous:** Three actions: steering, gas, and braking. Steering ranges from -1 (full left) to +1 (full right).\n",
        "   -  **Discrete:** Five possible actions: do nothing, steer left, steer right, gas, and brake.\n",
        "\n",
        "- **Observation Space:**\n",
        "\n",
        "    - The environment provides a 96x96 RGB image of the car and the track, which serves as the state input for the agent.\n",
        "\n",
        "- **Rewards:**\n",
        "\n",
        "    - The agent receives a -0.1 penalty for every frame, encouraging efficiency.\n",
        "    - It earns a positive reward for visiting track tiles: the formula is Reward=1000−0.1×framesReward=1000−0.1×frames, where \"frames\" is the number of frames taken to complete the lap. The reward for completing a lap depends on how many track tiles are visited.\n",
        "\n",
        "- **Episode Termination:**\n",
        "\n",
        "    - The episode ends either when all track tiles are visited or if the car goes off the track, which incurs a significant penalty (-100 reward)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNl1gE_25iaG",
        "outputId": "0713f85f-f288-4dc7-fa93-8f379088a068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['human', 'rgb_array', 'state_pixels']\n"
          ]
        }
      ],
      "source": [
        "#check render modes\n",
        "print(env.metadata[\"render_modes\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqluOwhK5iaG"
      },
      "source": [
        "- Checking if everything is okay and working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIUGHGsD5iaG",
        "outputId": "1a886f97-4b0a-4289-df8b-3d8cf07236c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Reset the environment and render the first frame\n",
        "obs, info = env.reset()\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n",
        "\n",
        "print(\"Environment initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bixidwdf5iaH",
        "outputId": "0a3f4c50-3776-46bc-d6b7-944fb768c132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action space: Discrete(5)\n"
          ]
        }
      ],
      "source": [
        "print(\"Action space:\", env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBIH0x1a5iaH",
        "outputId": "188feb7b-2a54-4320-88b9-f0041439220a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(5)\n",
            "Observation Space: Box(0, 255, (96, 96, 3), uint8)\n",
            "Environment Metadata: {'render_modes': ['human', 'rgb_array', 'state_pixels'], 'render_fps': 50}\n"
          ]
        }
      ],
      "source": [
        "print(\"Action Space:\", env.action_space)\n",
        "print(\"Observation Space:\", env.observation_space)\n",
        "print(\"Environment Metadata:\", env.metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr0d_0VP5iaI"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "for _ in range(10):\n",
        "    \"\"\"action = env.action_space.sample()  # Random action\n",
        "    print(f\"Action before step: {action}, Type: {type(action)}\")\n",
        "    obs, reward, done, info = env.step(action)\"\"\"\n",
        "    env.step(env.action_space.sample())\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOlB359V5iaI"
      },
      "source": [
        "---\n",
        "## 2. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s41my0H95iaI"
      },
      "source": [
        "Deep Q-Learning (DQN) is a reinforcement learning algorithm that extends the traditional Q-Learning method using neural networks to approximate the Q-values for state-action pairs.\n",
        "DQN is inherently designed for discrete action spaces, as the neural network outputs a separate Q-value for each action. For each state, the algorithm selects actions based on the highest Q-value, making it ideal for problems where actions are discrete and finite. This is a key advantage compared to other reinforcement learning methods, which may require modifications or different approaches for discrete action selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-fRMCak5iaI"
      },
      "outputs": [],
      "source": [
        "MODELS_DIR = '../2.2.1/models'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9dTiyQL5iaJ"
      },
      "source": [
        "### 2.1. Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3BXGUaY5iaK"
      },
      "source": [
        "#### 2.1.1. DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSJYVwO55iaK",
        "outputId": "92db2121-a84e-46c2-bb32-b7539f5d154a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EnvSpec(id='CarRacing-v3', entry_point='gymnasium.envs.box2d.car_racing:CarRacing', reward_threshold=900, nondeterministic=False, max_episode_steps=1000, order_enforce=True, disable_env_checker=False, kwargs={'continuous': False}, namespace=None, name='CarRacing', version=3, additional_wrappers=(), vector_entry_point=None)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CarRacing-v3\", continuous=False)\n",
        "print(env.spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ1tNABS5iaL",
        "outputId": "afad35e1-dc25-4d51-a2e2-51ac764d1b39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(96, 96, 3)\n"
          ]
        }
      ],
      "source": [
        "obs = env.reset()\n",
        "print(obs[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kP-q-r95iaL"
      },
      "outputs": [],
      "source": [
        "#create directories\n",
        "logs_dir = 'DQN_baseline_logs'\n",
        "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
        "os.makedirs(logs_path, exist_ok=True)\n",
        "\n",
        "video_dir = os.path.join(logs_path, \"videos\")\n",
        "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
        "model_dir = os.path.join(logs_path, \"models\")\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "os.makedirs(tensorboard_dir, exist_ok=True)\n",
        "os.makedirs(model_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvPRsAQ25iaM"
      },
      "outputs": [],
      "source": [
        "# Simple DQN architecture\n",
        "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
        "\n",
        "# Set up the model with simpler hyperparameters\n",
        "model = DQN('CnnPolicy', env, policy_kwargs=policy_kwargs,\n",
        "            verbose=1, buffer_size=50000, learning_starts=1000,\n",
        "            learning_rate=0.0005, batch_size=32, exploration_fraction=0.1,\n",
        "            exploration_final_eps=0.05)\n",
        "\n",
        "# Setup evaluation and checkpoint callbacks\n",
        "eval_callback = EvalCallback(env, best_model_save_path=model_dir,\n",
        "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
        "                             deterministic=True, render=False)\n",
        "\n",
        "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
        "                                         name_prefix='dqn_model_checkpoint')\n",
        "\n",
        "\n",
        "\n",
        "# Start training the model with callbacks for evaluation and checkpoints\n",
        "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
        "\n",
        "# Save the final model after training\n",
        "model.save(\"dqn_car_racing_model\")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNdYkXum5iaM"
      },
      "outputs": [],
      "source": [
        "env.close()\n",
        "del env\n",
        "foo = gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yigE9npk5iaM"
      },
      "source": [
        "#### 2.1.2. PPO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS_DIR = '/content/drive/MyDrive/reinforcement-learning-with-gymnasium-main/ppo_car_racing_model'"
      ],
      "metadata": {
        "id": "TS6A5O_D6lJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oLHJFVr5iaM",
        "outputId": "4e69e6f4-a35f-4f0c-9c7a-0cb2272bba61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation Space Size:  (96, 96, 3)\n",
            "Action Space Size:  ()\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CarRacing-v3\", continuous=False)\n",
        "print(\"Observation Space Size: \", env.observation_space.shape)\n",
        "print(\"Action Space Size: \", env.action_space.shape)\n",
        "obs = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_str = \"CarRacing-v3\"\n",
        "log_dir = \"./logs/{}\".format(env_str)\n",
        "gray_scale = True\n",
        "\n",
        "# If gray_scale True, convert obs to gray scale 84 x 84 image\n",
        "wrapper_class = WarpFrame if gray_scale else None"
      ],
      "metadata": {
        "id": "AvnTcTWU6rjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNqy6GNs5iaN"
      },
      "outputs": [],
      "source": [
        "#create directories\n",
        "logs_dir = 'PPO_baseline_logs'\n",
        "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
        "os.makedirs(logs_path, exist_ok=True)\n",
        "\n",
        "video_dir = os.path.join(logs_path, \"videos\")\n",
        "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
        "model_dir = os.path.join(logs_path, \"models\")\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "os.makedirs(tensorboard_dir, exist_ok=True)\n",
        "os.makedirs(model_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple PPO architecture\n",
        "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
        "\n",
        "# Create Training CarRacing environment\n",
        "env = make_vec_env(env_str, n_envs=1, wrapper_class=wrapper_class)\n",
        "env = VecFrameStack(env, n_stack=4)\n",
        "env = VecTransposeImage(env)\n",
        "#env = VecNormalize(env, norm_reward=True)\n",
        "\n",
        "# Create Evaluation CarRacing environment\n",
        "env_val = make_vec_env(env_str, n_envs=1, wrapper_class=wrapper_class)\n",
        "env_val = VecFrameStack(env_val, n_stack=4)\n",
        "env_val = VecTransposeImage(env_val)\n",
        "\n",
        "# Evaluation Callback\n",
        "eval_callback = EvalCallback(env, best_model_save_path=model_dir,\n",
        "                             log_path=model_dir, eval_freq=10_000, n_eval_episodes=5,\n",
        "                             deterministic=True, render=False)\n",
        "\n",
        "#PPO\n",
        "model = PPO('CnnPolicy', env, policy_kwargs=policy_kwargs,\n",
        "            n_steps=10_000, verbose=1, ent_coef=0.005,\n",
        "            learning_rate=0.0005, batch_size=32)\n",
        "\n",
        "\n",
        "checkpoint_callback = CheckpointCallback(save_freq=10_000, save_path=model_dir,\n",
        "                                         name_prefix='ppo_model_checkpoint')\n",
        "\n",
        "\n",
        "# Start training the model with callbacks for evaluation and checkpoints\n",
        "model.learn(total_timesteps=100_000, progress_bar=True, callback=[eval_callback, checkpoint_callback])\n",
        "\n",
        "model.save(\"ppo_env_model\")\n",
        "#model.save(os.path.join(log_dir, \"ppo_car_racing\"))\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20)\n",
        "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "env.close()\n",
        "env_val.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "430452755d6a43de8022bdc6814372e0",
            "84f59904e1ec48e3abdce717dc6d0b18"
          ]
        },
        "id": "aMGmqg1LCQdW",
        "outputId": "1d618e27-b35c-457d-9b70-e75a243697fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 32, but because the `RolloutBuffer` is of size `n_steps * n_envs = 10000`, after every 312 untruncated mini-batches, there will be a truncated mini-batch of size 16\n",
            "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
            "Info: (n_steps=10000 and n_envs=1)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "430452755d6a43de8022bdc6814372e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=10000, episode_reward=-93.11 +/- 0.57\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=10000, episode_reward=-93.11 +/- 0.57\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 1000.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 1000.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | -93.1    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10000    |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New best mean reward!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -62.5    |\n",
            "| time/              |          |\n",
            "|    fps             | 23       |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 420      |\n",
            "|    total_timesteps | 10000    |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=20000, episode_reward=-93.52 +/- 0.54\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=20000, episode_reward=-93.52 +/- 0.54\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 1000.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 1000.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -93.5       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 20000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006064357 |\n",
            "|    clip_fraction        | 0.0665      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.2        |\n",
            "|    explained_variance   | -0.0115     |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 0.351       |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00273    |\n",
            "|    std                  | 0.976       |\n",
            "|    value_loss           | 0.678       |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -57.3    |\n",
            "| time/              |          |\n",
            "|    fps             | 16       |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 1236     |\n",
            "|    total_timesteps | 20000    |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=30000, episode_reward=-93.28 +/- 0.59\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=30000, episode_reward=-93.28 +/- 0.59\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 1000.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 1000.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -93.3        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 30000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0074582486 |\n",
            "|    clip_fraction        | 0.0739       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.14        |\n",
            "|    explained_variance   | 0.0508       |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 0.566        |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00429     |\n",
            "|    std                  | 0.96         |\n",
            "|    value_loss           | 0.779        |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -51.1    |\n",
            "| time/              |          |\n",
            "|    fps             | 14       |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 2098     |\n",
            "|    total_timesteps | 30000    |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=40000, episode_reward=-93.15 +/- 0.46\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=40000, episode_reward=-93.15 +/- 0.46\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 1000.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 1000.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -93.2       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 40000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006437389 |\n",
            "|    clip_fraction        | 0.0589      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.11       |\n",
            "|    explained_variance   | 0.0406      |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 0.168       |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00356    |\n",
            "|    std                  | 0.956       |\n",
            "|    value_loss           | 0.993       |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -45.7    |\n",
            "| time/              |          |\n",
            "|    fps             | 13       |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 2962     |\n",
            "|    total_timesteps | 40000    |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=50000, episode_reward=-93.27 +/- 0.60\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=50000, episode_reward=-93.27 +/- 0.60\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -93.3        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 50000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0070858696 |\n",
            "|    clip_fraction        | 0.0705       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.08        |\n",
            "|    explained_variance   | 0.0362       |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 0.697        |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.0054      |\n",
            "|    std                  | 0.944        |\n",
            "|    value_loss           | 1.11         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -41.5    |\n",
            "| time/              |          |\n",
            "|    fps             | 13       |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 3812     |\n",
            "|    total_timesteps | 50000    |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=60000, episode_reward=-93.04 +/- 0.37\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=60000, episode_reward=-93.04 +/- 0.37\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 1000.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 1000.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -93          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 60000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064649154 |\n",
            "|    clip_fraction        | 0.0607       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.03        |\n",
            "|    explained_variance   | 0.031        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 0.461        |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.0037      |\n",
            "|    std                  | 0.927        |\n",
            "|    value_loss           | 1.25         |\n",
            "------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New best mean reward!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -39.2    |\n",
            "| time/              |          |\n",
            "|    fps             | 12       |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 4669     |\n",
            "|    total_timesteps | 60000    |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=70000, episode_reward=-93.33 +/- 0.33\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=70000, episode_reward=-93.33 +/- 0.33\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 1000.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 1000.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -93.3       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 70000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006622155 |\n",
            "|    clip_fraction        | 0.0651      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.96       |\n",
            "|    explained_variance   | 0.0309      |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 0.392       |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00455    |\n",
            "|    std                  | 0.911       |\n",
            "|    value_loss           | 1.19        |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -35.7    |\n",
            "| time/              |          |\n",
            "|    fps             | 12       |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 5522     |\n",
            "|    total_timesteps | 70000    |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=80000, episode_reward=-93.02 +/- 0.14\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=80000, episode_reward=-93.02 +/- 0.14\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 1000.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 1000.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -93          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 80000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051716054 |\n",
            "|    clip_fraction        | 0.051        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.94        |\n",
            "|    explained_variance   | 0.0219       |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 0.643        |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.00247     |\n",
            "|    std                  | 0.904        |\n",
            "|    value_loss           | 2.01         |\n",
            "------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New best mean reward!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -33.4    |\n",
            "| time/              |          |\n",
            "|    fps             | 12       |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 6399     |\n",
            "|    total_timesteps | 80000    |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=90000, episode_reward=-93.00 +/- 0.78\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=90000, episode_reward=-93.00 +/- 0.78\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -32.3    |\n",
            "| time/              |          |\n",
            "|    fps             | 12       |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 7235     |\n",
            "|    total_timesteps | 90000    |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval num_timesteps=100000, episode_reward=-93.16 +/- 0.14\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=100000, episode_reward=-93.16 +/- 0.14\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Episode length: 1000.00 +/- 0.00\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 1000.00 +/- 0.00\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -93.2       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 100000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004595625 |\n",
            "|    clip_fraction        | 0.0358      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.84       |\n",
            "|    explained_variance   | 0.019       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.45        |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00132    |\n",
            "|    std                  | 0.873       |\n",
            "|    value_loss           | 2.53        |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -31.7    |\n",
            "| time/              |          |\n",
            "|    fps             | 12       |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 8083     |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward: -93.10 +/- 0.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the saved model\n",
        "model_path = '/content/drive/MyDrive/reinforcement-learning-with-gymnasium-main/ppo_car_racing_model/PPO_baseline_logs/models/ppo_model_checkpoint_20000_steps.zip'\n",
        "\n",
        "# Load the model\n",
        "model = PPO.load(model_path)"
      ],
      "metadata": {
        "id": "CdVcqijsCTjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import matplotlib\n",
        "\n",
        "# Load the evaluations.npz file\n",
        "data = numpy.load(os.path.join(log_dir, \"/content/drive/MyDrive/reinforcement-learning-with-gymnasium-main/ppo_car_racing_model/PPO_baseline_logs/models/evaluations.npz\"))\n",
        "\n",
        "# Extract the relevant data\n",
        "timesteps = data['timesteps']\n",
        "results = data['results']\n",
        "\n",
        "# Calculate the mean and standard deviation of the results\n",
        "mean_results = numpy.mean(results, axis=1)\n",
        "std_results = numpy.std(results, axis=1)\n",
        "\n",
        "matplotlib.pyplot.figure()\n",
        "matplotlib.pyplot.plot(timesteps, mean_results)\n",
        "matplotlib.pyplot.fill_between(timesteps,\n",
        "                               mean_results - std_results,\n",
        "                               mean_results + std_results,\n",
        "                               alpha=0.3)\n",
        "\n",
        "matplotlib.pyplot.xlabel('Timesteps')\n",
        "matplotlib.pyplot.ylabel('Mean Reward')\n",
        "matplotlib.pyplot.title('PPO Performance')\n",
        "matplotlib.pyplot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "vzjfWO2hljzu",
        "outputId": "0f2798a8-ff88-4a85-88b0-7b06d13e5603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaFArPj95iaN"
      },
      "outputs": [],
      "source": [
        "env.close()\n",
        "del env\n",
        "foo = gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5XoMqJC5iaN"
      },
      "source": [
        "### 2.2. Environment and Reward Modifications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb3fj0Lw5iaO"
      },
      "source": [
        "| **Modification**            | **Description**                                                                                      | **Effect**                                     |\n",
        "|-----------------------------|------------------------------------------------------------------------------------------------------|-----------------------------------------------|\n",
        "| **Obstacles**               | Randomly placed obstacles on the track.                                                             | Requires avoidance and navigation skills.     |\n",
        "| **Track Width Variability** | Random track width adjustments between `[0.8, 1.2]`.                                                | Simulates narrow/wide tracks dynamically.     |\n",
        "| **Weather Conditions**      | Introduces \"rain\" and \"snow,\" which alter action effectiveness.                                     | Adds randomness and realism to driving.       |\n",
        "| **Off-Track Penalty**       | Reward reduced by `-10` if the car leaves the track.                                                | Encourages the agent to stay on track.        |\n",
        "| **Distance Reward**         | Positive reward based on the distance traveled per step.                                            | Incentivizes efficient driving.               |\n",
        "| **Obstacle Proximity Penalty** | Penalty inversely proportional to the distance to nearby obstacles (`1 / (d + 1e-6)`).             | Encourages the car to avoid obstacles safely. |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhmKrZhi5iaO"
      },
      "source": [
        "#### 2.2.1 DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9Iq8qZ55iaO",
        "outputId": "fed8f03f-82a5-4a48-e23a-1e43cfd67f03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial observation shape: (96, 96, 3)\n",
            "Initial info: {}\n",
            "\n",
            "Step 1:\n",
            "Action taken: 2\n",
            "Reward: 5.804336925215569\n",
            "Terminated: False\n",
            "Truncated: False\n",
            "Info: {}\n",
            "Current weather: rain\n",
            "\n",
            "Step 2:\n",
            "Action taken: 3\n",
            "Reward: -0.37469043040753186\n",
            "Terminated: False\n",
            "Truncated: False\n",
            "Info: {}\n",
            "Current weather: rain\n",
            "\n",
            "Step 3:\n",
            "Action taken: 0\n",
            "Reward: -0.4746904304075319\n",
            "Terminated: False\n",
            "Truncated: False\n",
            "Info: {}\n",
            "Current weather: rain\n",
            "\n",
            "Step 4:\n",
            "Action taken: 0\n",
            "Reward: -0.5746904304075319\n",
            "Terminated: False\n",
            "Truncated: False\n",
            "Info: {}\n",
            "Current weather: rain\n",
            "\n",
            "Step 5:\n",
            "Action taken: 1\n",
            "Reward: -0.6746904304075318\n",
            "Terminated: False\n",
            "Truncated: False\n",
            "Info: {}\n",
            "Current weather: rain\n",
            "\n",
            "Step 6:\n",
            "Action taken: 3\n",
            "Reward: -0.7746904304075318\n",
            "Terminated: False\n",
            "Truncated: False\n",
            "Info: {}\n",
            "Current weather: rain\n",
            "\n",
            "Step 7:\n",
            "Action taken: 1\n",
            "Reward: -0.8746904304075318\n",
            "Terminated: False\n",
            "Truncated: False\n",
            "Info: {}\n",
            "Current weather: rain\n",
            "\n",
            "Step 8:\n",
            "Action taken: 0\n",
            "Reward: -0.9746904304075318\n",
            "Terminated: False\n",
            "Truncated: False\n",
            "Info: {}\n",
            "Current weather: rain\n",
            "\n",
            "Step 9:\n",
            "Action taken: 4\n",
            "Reward: -1.0746904304075318\n",
            "Terminated: False\n",
            "Truncated: False\n",
            "Info: {}\n",
            "Current weather: rain\n",
            "\n",
            "Step 10:\n",
            "Action taken: 4\n",
            "Reward: -1.1746904304075318\n",
            "Terminated: False\n",
            "Truncated: False\n",
            "Info: {}\n",
            "Current weather: rain\n"
          ]
        }
      ],
      "source": [
        "custom_env = EnhancedCarRacing(render_mode=\"rgb_array\")\n",
        "obs, info = custom_env.reset()\n",
        "\n",
        "print(\"Initial observation shape:\", obs.shape)\n",
        "print(\"Initial info:\", info)\n",
        "\n",
        "for i in range(10):\n",
        "    action = custom_env.action_space.sample()  # Your agent would make a decision here\n",
        "    observation, reward, terminated, truncated, info = custom_env.step(action)\n",
        "    print(f\"\\nStep {i+1}:\")\n",
        "    print(\"Action taken:\", action)\n",
        "    print(\"Reward:\", reward)\n",
        "    print(\"Terminated:\", terminated)\n",
        "    print(\"Truncated:\", truncated)\n",
        "    print(\"Info:\", info)\n",
        "\n",
        "    if hasattr(custom_env, 'weather_condition'):\n",
        "        print(\"Current weather:\", custom_env.weather_condition)\n",
        "\n",
        "    if terminated or truncated:\n",
        "        print(\"Episode ended\")\n",
        "        break\n",
        "\n",
        "custom_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_-3RPna5iaP"
      },
      "outputs": [],
      "source": [
        "custom_env = GrayscaleObservation(custom_env, keep_dim=True)\n",
        "custom_env = TimeLimit(custom_env, max_episode_steps=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dV7OY_nO5iaP",
        "outputId": "e94ab79b-1152-4d51-fbe2-7e7f71eddafc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<TimeLimit<GrayscaleObservation<EnhancedCarRacing instance>>>\n"
          ]
        }
      ],
      "source": [
        "print(custom_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH5FA9ne5iaP",
        "outputId": "30541a05-23cd-4e96-dfcc-7965bd9ac68c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action space: Discrete(5)\n"
          ]
        }
      ],
      "source": [
        "print(\"Action space:\", custom_env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1gaya3B5iaQ"
      },
      "outputs": [],
      "source": [
        "#create directories\n",
        "logs_dir = 'DQN_env_mod_logs'\n",
        "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
        "os.makedirs(logs_path, exist_ok=True)\n",
        "\n",
        "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
        "model_dir = os.path.join(logs_path, \"models\")\n",
        "os.makedirs(tensorboard_dir, exist_ok=True)\n",
        "os.makedirs(model_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xcqH8pl5iaQ",
        "outputId": "70d592a2-2de6-4b64-aa6f-bbbcb4f7bf00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W1218 10:05:02.050906178 NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
            "/home/derpy/Documents/Fac/ano3/semestre1/ISIA/reinforcement-learning-with-gymnasium/.venv/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7eaf196b5eb0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7eaf159685c0>\n",
            "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.962     |\n",
            "| time/               |           |\n",
            "|    episodes         | 4         |\n",
            "|    fps              | 28        |\n",
            "|    time_elapsed     | 141       |\n",
            "|    total_timesteps  | 4000      |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 5.12      |\n",
            "|    n_updates        | 749       |\n",
            "-----------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval num_timesteps=5000, episode_reward=-50215.65 +/- 9.31\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.953     |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 5000      |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 3.93      |\n",
            "|    n_updates        | 999       |\n",
            "-----------------------------------\n",
            "New best mean reward!\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.924     |\n",
            "| time/               |           |\n",
            "|    episodes         | 8         |\n",
            "|    fps              | 17        |\n",
            "|    time_elapsed     | 463       |\n",
            "|    total_timesteps  | 8000      |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 3.19      |\n",
            "|    n_updates        | 1749      |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=-50167.65 +/- 36.14\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.905     |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 10000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 2.34      |\n",
            "|    n_updates        | 2249      |\n",
            "-----------------------------------\n",
            "New best mean reward!\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.886     |\n",
            "| time/               |           |\n",
            "|    episodes         | 12        |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 750       |\n",
            "|    total_timesteps  | 12000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 2.74      |\n",
            "|    n_updates        | 2749      |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=15000, episode_reward=-50211.32 +/- 8.06\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.858     |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 15000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 2.64      |\n",
            "|    n_updates        | 3499      |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.848     |\n",
            "| time/               |           |\n",
            "|    episodes         | 16        |\n",
            "|    fps              | 14        |\n",
            "|    time_elapsed     | 1127      |\n",
            "|    total_timesteps  | 16000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 2.41      |\n",
            "|    n_updates        | 3749      |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-50201.12 +/- 9.11\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.81      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 20000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 1.52      |\n",
            "|    n_updates        | 4749      |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.81      |\n",
            "| time/               |           |\n",
            "|    episodes         | 20        |\n",
            "|    fps              | 13        |\n",
            "|    time_elapsed     | 1526      |\n",
            "|    total_timesteps  | 20000     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.772     |\n",
            "| time/               |           |\n",
            "|    episodes         | 24        |\n",
            "|    fps              | 14        |\n",
            "|    time_elapsed     | 1682      |\n",
            "|    total_timesteps  | 24000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 3.47      |\n",
            "|    n_updates        | 5749      |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=25000, episode_reward=-50205.71 +/- 9.65\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.763     |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 25000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 4.99      |\n",
            "|    n_updates        | 5999      |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.734     |\n",
            "| time/               |           |\n",
            "|    episodes         | 28        |\n",
            "|    fps              | 14        |\n",
            "|    time_elapsed     | 1909      |\n",
            "|    total_timesteps  | 28000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 4.69      |\n",
            "|    n_updates        | 6749      |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-33812.91 +/- 20146.91\n",
            "Episode length: 768.60 +/- 286.51\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 769       |\n",
            "|    mean_reward      | -3.38e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.715     |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 30000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 4.51      |\n",
            "|    n_updates        | 7249      |\n",
            "-----------------------------------\n",
            "New best mean reward!\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.696     |\n",
            "| time/               |           |\n",
            "|    episodes         | 32        |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 2111      |\n",
            "|    total_timesteps  | 32000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 3.8       |\n",
            "|    n_updates        | 7749      |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=35000, episode_reward=-50206.97 +/- 7.08\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.668     |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 35000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 4.63      |\n",
            "|    n_updates        | 8499      |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.658     |\n",
            "| time/               |           |\n",
            "|    episodes         | 36        |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 2339      |\n",
            "|    total_timesteps  | 36000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 2.92      |\n",
            "|    n_updates        | 8749      |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-50201.85 +/- 12.14\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.62      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 40000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 2.66      |\n",
            "|    n_updates        | 9749      |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.62      |\n",
            "| time/               |           |\n",
            "|    episodes         | 40        |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 2567      |\n",
            "|    total_timesteps  | 40000     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.582     |\n",
            "| time/               |           |\n",
            "|    episodes         | 44        |\n",
            "|    fps              | 16        |\n",
            "|    time_elapsed     | 2696      |\n",
            "|    total_timesteps  | 44000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 3.39      |\n",
            "|    n_updates        | 10749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=45000, episode_reward=-50172.30 +/- 43.30\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.573     |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 45000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 3.68      |\n",
            "|    n_updates        | 10999     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.544     |\n",
            "| time/               |           |\n",
            "|    episodes         | 48        |\n",
            "|    fps              | 16        |\n",
            "|    time_elapsed     | 2971      |\n",
            "|    total_timesteps  | 48000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 4         |\n",
            "|    n_updates        | 11749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-50216.08 +/- 13.82\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.525     |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 50000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 3.76      |\n",
            "|    n_updates        | 12249     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.506     |\n",
            "| time/               |           |\n",
            "|    episodes         | 52        |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 3257      |\n",
            "|    total_timesteps  | 52000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 6         |\n",
            "|    n_updates        | 12749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=55000, episode_reward=-50112.29 +/- 20.32\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.01e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.478     |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 55000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 8.88      |\n",
            "|    n_updates        | 13499     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.468     |\n",
            "| time/               |           |\n",
            "|    episodes         | 56        |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 3561      |\n",
            "|    total_timesteps  | 56000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 3.84      |\n",
            "|    n_updates        | 13749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-50207.37 +/- 12.83\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.43      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 60000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 4.37      |\n",
            "|    n_updates        | 14749     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.43      |\n",
            "| time/               |           |\n",
            "|    episodes         | 60        |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 3874      |\n",
            "|    total_timesteps  | 60000     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.392     |\n",
            "| time/               |           |\n",
            "|    episodes         | 64        |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 4087      |\n",
            "|    total_timesteps  | 64000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 7.42      |\n",
            "|    n_updates        | 15749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=65000, episode_reward=-50208.35 +/- 11.54\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.383     |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 65000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 4.9       |\n",
            "|    n_updates        | 15999     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.354     |\n",
            "| time/               |           |\n",
            "|    episodes         | 68        |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 4352      |\n",
            "|    total_timesteps  | 68000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 5.87      |\n",
            "|    n_updates        | 16749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-50192.87 +/- 21.55\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.335     |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 70000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 5.99      |\n",
            "|    n_updates        | 17249     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.316     |\n",
            "| time/               |           |\n",
            "|    episodes         | 72        |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 4582      |\n",
            "|    total_timesteps  | 72000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 8.44      |\n",
            "|    n_updates        | 17749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=75000, episode_reward=-50206.63 +/- 16.51\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.288     |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 75000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 9.2       |\n",
            "|    n_updates        | 18499     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.278     |\n",
            "| time/               |           |\n",
            "|    episodes         | 76        |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 4837      |\n",
            "|    total_timesteps  | 76000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 5.99      |\n",
            "|    n_updates        | 18749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-50205.96 +/- 11.10\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.24      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 80000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 5.08      |\n",
            "|    n_updates        | 19749     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.24      |\n",
            "| time/               |           |\n",
            "|    episodes         | 80        |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 5146      |\n",
            "|    total_timesteps  | 80000     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.202     |\n",
            "| time/               |           |\n",
            "|    episodes         | 84        |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 5345      |\n",
            "|    total_timesteps  | 84000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 4.51      |\n",
            "|    n_updates        | 20749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=85000, episode_reward=-50171.45 +/- 25.93\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.193     |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 85000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 4.73      |\n",
            "|    n_updates        | 20999     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.164     |\n",
            "| time/               |           |\n",
            "|    episodes         | 88        |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 5611      |\n",
            "|    total_timesteps  | 88000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 4.12      |\n",
            "|    n_updates        | 21749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-50212.26 +/- 4.65\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.145     |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 90000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 5.17      |\n",
            "|    n_updates        | 22249     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.126     |\n",
            "| time/               |           |\n",
            "|    episodes         | 92        |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 5915      |\n",
            "|    total_timesteps  | 92000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 10.2      |\n",
            "|    n_updates        | 22749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=95000, episode_reward=-50202.68 +/- 18.22\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.0975    |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 95000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 3.79      |\n",
            "|    n_updates        | 23499     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.088     |\n",
            "| time/               |           |\n",
            "|    episodes         | 96        |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 6232      |\n",
            "|    total_timesteps  | 96000     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 9.39      |\n",
            "|    n_updates        | 23749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-8896.11 +/- 3242.86\n",
            "Episode length: 413.00 +/- 71.52\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 413      |\n",
            "|    mean_reward      | -8.9e+03 |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 100000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 6.79     |\n",
            "|    n_updates        | 24749    |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 100       |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 6458      |\n",
            "|    total_timesteps  | 100000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 104       |\n",
            "|    fps              | 15        |\n",
            "|    time_elapsed     | 6697      |\n",
            "|    total_timesteps  | 104000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 5.64      |\n",
            "|    n_updates        | 25749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=105000, episode_reward=-50208.89 +/- 7.96\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 105000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.3      |\n",
            "|    n_updates        | 25999     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 108       |\n",
            "|    fps              | 14        |\n",
            "|    time_elapsed     | 7687      |\n",
            "|    total_timesteps  | 108000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 5.63      |\n",
            "|    n_updates        | 26749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=110000, episode_reward=-50201.73 +/- 18.29\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 110000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.6      |\n",
            "|    n_updates        | 27249     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 112      |\n",
            "|    fps              | 13       |\n",
            "|    time_elapsed     | 8188     |\n",
            "|    total_timesteps  | 112000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 10       |\n",
            "|    n_updates        | 27749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=115000, episode_reward=-50202.84 +/- 10.85\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 115000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 8.98      |\n",
            "|    n_updates        | 28499     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 116      |\n",
            "|    fps              | 13       |\n",
            "|    time_elapsed     | 8754     |\n",
            "|    total_timesteps  | 116000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 10.6     |\n",
            "|    n_updates        | 28749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=-50211.57 +/- 13.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 120000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 6.73      |\n",
            "|    n_updates        | 29749     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 120      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 9395     |\n",
            "|    total_timesteps  | 120000   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 124      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 9793     |\n",
            "|    total_timesteps  | 124000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 11.5     |\n",
            "|    n_updates        | 30749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=125000, episode_reward=-50191.04 +/- 17.45\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 125000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.8      |\n",
            "|    n_updates        | 30999     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 128      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 10314    |\n",
            "|    total_timesteps  | 128000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 5.79     |\n",
            "|    n_updates        | 31749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=130000, episode_reward=-50197.73 +/- 11.70\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 130000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 9.03      |\n",
            "|    n_updates        | 32249     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 132      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 10866    |\n",
            "|    total_timesteps  | 131959   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 9.18     |\n",
            "|    n_updates        | 32739    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=135000, episode_reward=-13544.02 +/- 2978.08\n",
            "Episode length: 514.40 +/- 61.34\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 514       |\n",
            "|    mean_reward      | -1.35e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 135000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 9.11      |\n",
            "|    n_updates        | 33499     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 136      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 11222    |\n",
            "|    total_timesteps  | 136000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 8.33     |\n",
            "|    n_updates        | 33749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=140000, episode_reward=-50120.15 +/- 43.33\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.01e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 140000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 9         |\n",
            "|    n_updates        | 34749     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 140      |\n",
            "|    fps              | 11       |\n",
            "|    time_elapsed     | 11781    |\n",
            "|    total_timesteps  | 140000   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 144      |\n",
            "|    fps              | 11       |\n",
            "|    time_elapsed     | 12117    |\n",
            "|    total_timesteps  | 144000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 4.79     |\n",
            "|    n_updates        | 35749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=145000, episode_reward=-50199.28 +/- 11.01\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 145000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.4      |\n",
            "|    n_updates        | 35999     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 148      |\n",
            "|    fps              | 11       |\n",
            "|    time_elapsed     | 12571    |\n",
            "|    total_timesteps  | 148000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 9.67     |\n",
            "|    n_updates        | 36749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=150000, episode_reward=-50188.72 +/- 9.02\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 150000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 8.68      |\n",
            "|    n_updates        | 37249     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 152      |\n",
            "|    fps              | 11       |\n",
            "|    time_elapsed     | 12851    |\n",
            "|    total_timesteps  | 152000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 13.8     |\n",
            "|    n_updates        | 37749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=155000, episode_reward=-50208.45 +/- 15.43\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 155000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 6.76      |\n",
            "|    n_updates        | 38499     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 156      |\n",
            "|    fps              | 11       |\n",
            "|    time_elapsed     | 13089    |\n",
            "|    total_timesteps  | 156000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 6.42     |\n",
            "|    n_updates        | 38749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=-50194.16 +/- 5.66\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 160000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.2      |\n",
            "|    n_updates        | 39749     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 160      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 13326    |\n",
            "|    total_timesteps  | 160000   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 164      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 13461    |\n",
            "|    total_timesteps  | 164000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 13.3     |\n",
            "|    n_updates        | 40749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=165000, episode_reward=-50206.46 +/- 11.06\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 165000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 10.6      |\n",
            "|    n_updates        | 40999     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 168      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 13755    |\n",
            "|    total_timesteps  | 168000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 6.07     |\n",
            "|    n_updates        | 41749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=170000, episode_reward=-50197.88 +/- 13.63\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 170000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 9.97      |\n",
            "|    n_updates        | 42249     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 172      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 14065    |\n",
            "|    total_timesteps  | 172000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 11.3     |\n",
            "|    n_updates        | 42749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=175000, episode_reward=-35280.44 +/- 18393.08\n",
            "Episode length: 800.20 +/- 247.88\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 800       |\n",
            "|    mean_reward      | -3.53e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 175000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 8.32      |\n",
            "|    n_updates        | 43499     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 176      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 14309    |\n",
            "|    total_timesteps  | 176000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 7.75     |\n",
            "|    n_updates        | 43749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=180000, episode_reward=-5775.34 +/- 1716.65\n",
            "Episode length: 332.20 +/- 52.93\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 332       |\n",
            "|    mean_reward      | -5.78e+03 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 180000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13        |\n",
            "|    n_updates        | 44749     |\n",
            "-----------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 180      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 14518    |\n",
            "|    total_timesteps  | 180000   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 184      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 14713    |\n",
            "|    total_timesteps  | 184000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 13.1     |\n",
            "|    n_updates        | 45749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=185000, episode_reward=-50205.64 +/- 15.74\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 185000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15        |\n",
            "|    n_updates        | 45999     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 188      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 14969    |\n",
            "|    total_timesteps  | 188000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 7.79     |\n",
            "|    n_updates        | 46749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=190000, episode_reward=-50208.19 +/- 10.52\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 190000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 7.75      |\n",
            "|    n_updates        | 47249     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 192      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 15277    |\n",
            "|    total_timesteps  | 192000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 9.87     |\n",
            "|    n_updates        | 47749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=195000, episode_reward=-12453.23 +/- 4103.91\n",
            "Episode length: 488.80 +/- 86.46\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 489       |\n",
            "|    mean_reward      | -1.25e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 195000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 9.31      |\n",
            "|    n_updates        | 48499     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 196      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 15532    |\n",
            "|    total_timesteps  | 196000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 8.6      |\n",
            "|    n_updates        | 48749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=200000, episode_reward=-50193.29 +/- 9.07\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 200000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 10.5      |\n",
            "|    n_updates        | 49749     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 200      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 15801    |\n",
            "|    total_timesteps  | 200000   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 204      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 16023    |\n",
            "|    total_timesteps  | 204000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 10.4     |\n",
            "|    n_updates        | 50749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=205000, episode_reward=-50207.50 +/- 6.53\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 205000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11        |\n",
            "|    n_updates        | 50999     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 208      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 16343    |\n",
            "|    total_timesteps  | 208000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 9.61     |\n",
            "|    n_updates        | 51749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=210000, episode_reward=-50207.03 +/- 12.82\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 210000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.1      |\n",
            "|    n_updates        | 52249     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 212       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 16594     |\n",
            "|    total_timesteps  | 212000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.8      |\n",
            "|    n_updates        | 52749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=215000, episode_reward=-50199.78 +/- 8.63\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 215000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.1      |\n",
            "|    n_updates        | 53499     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 216       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 16949     |\n",
            "|    total_timesteps  | 216000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 6.23      |\n",
            "|    n_updates        | 53749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=220000, episode_reward=-50209.80 +/- 12.33\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 220000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 7.95      |\n",
            "|    n_updates        | 54749     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 220       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 17235     |\n",
            "|    total_timesteps  | 220000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 224       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 17476     |\n",
            "|    total_timesteps  | 224000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16.7      |\n",
            "|    n_updates        | 55749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=225000, episode_reward=-50203.18 +/- 9.59\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 225000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 19.8      |\n",
            "|    n_updates        | 55999     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 228       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 17747     |\n",
            "|    total_timesteps  | 228000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 10.3      |\n",
            "|    n_updates        | 56749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=230000, episode_reward=-50210.61 +/- 10.85\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 230000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 7.12      |\n",
            "|    n_updates        | 57249     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 232       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 18058     |\n",
            "|    total_timesteps  | 232000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.8      |\n",
            "|    n_updates        | 57749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=235000, episode_reward=-50197.35 +/- 13.68\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 235000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 7.38      |\n",
            "|    n_updates        | 58499     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 236       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 18404     |\n",
            "|    total_timesteps  | 236000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.2      |\n",
            "|    n_updates        | 58749     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 999       |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 240       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 18631     |\n",
            "|    total_timesteps  | 239917    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.5      |\n",
            "|    n_updates        | 59729     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=240000, episode_reward=-50201.37 +/- 16.23\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 240000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 10.4      |\n",
            "|    n_updates        | 59749     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 244       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 18947     |\n",
            "|    total_timesteps  | 244000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 8.88      |\n",
            "|    n_updates        | 60749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=245000, episode_reward=-50140.61 +/- 14.68\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.01e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 245000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 20.4      |\n",
            "|    n_updates        | 60999     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 248       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 19185     |\n",
            "|    total_timesteps  | 248000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.4      |\n",
            "|    n_updates        | 61749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=250000, episode_reward=-50199.17 +/- 10.10\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 250000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.7      |\n",
            "|    n_updates        | 62249     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 252       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 19424     |\n",
            "|    total_timesteps  | 252000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.7      |\n",
            "|    n_updates        | 62749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=255000, episode_reward=-50196.72 +/- 12.23\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 255000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 10.1      |\n",
            "|    n_updates        | 63499     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 256       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 19708     |\n",
            "|    total_timesteps  | 256000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.8      |\n",
            "|    n_updates        | 63749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=260000, episode_reward=-50212.10 +/- 19.26\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 260000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.7      |\n",
            "|    n_updates        | 64749     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 260       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 20027     |\n",
            "|    total_timesteps  | 260000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 264       |\n",
            "|    fps              | 13        |\n",
            "|    time_elapsed     | 20238     |\n",
            "|    total_timesteps  | 264000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 9.25      |\n",
            "|    n_updates        | 65749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=265000, episode_reward=-50192.29 +/- 23.88\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 265000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.3      |\n",
            "|    n_updates        | 65999     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 268       |\n",
            "|    fps              | 13        |\n",
            "|    time_elapsed     | 20503     |\n",
            "|    total_timesteps  | 268000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.2      |\n",
            "|    n_updates        | 66749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=270000, episode_reward=-50206.48 +/- 7.19\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 270000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.2      |\n",
            "|    n_updates        | 67249     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 272       |\n",
            "|    fps              | 13        |\n",
            "|    time_elapsed     | 20809     |\n",
            "|    total_timesteps  | 272000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.3      |\n",
            "|    n_updates        | 67749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=275000, episode_reward=-50203.23 +/- 14.23\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 275000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 9         |\n",
            "|    n_updates        | 68499     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 276       |\n",
            "|    fps              | 13        |\n",
            "|    time_elapsed     | 21096     |\n",
            "|    total_timesteps  | 276000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 9.21      |\n",
            "|    n_updates        | 68749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=280000, episode_reward=-50217.40 +/- 7.94\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 280000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 26        |\n",
            "|    n_updates        | 69749     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 280       |\n",
            "|    fps              | 13        |\n",
            "|    time_elapsed     | 21458     |\n",
            "|    total_timesteps  | 280000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 284       |\n",
            "|    fps              | 13        |\n",
            "|    time_elapsed     | 21695     |\n",
            "|    total_timesteps  | 284000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.9      |\n",
            "|    n_updates        | 70749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=285000, episode_reward=-50188.95 +/- 21.12\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 285000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.2      |\n",
            "|    n_updates        | 70999     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 288       |\n",
            "|    fps              | 13        |\n",
            "|    time_elapsed     | 22075     |\n",
            "|    total_timesteps  | 288000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 10        |\n",
            "|    n_updates        | 71749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=290000, episode_reward=-50205.28 +/- 8.83\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 290000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 10.7      |\n",
            "|    n_updates        | 72249     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 292       |\n",
            "|    fps              | 13        |\n",
            "|    time_elapsed     | 22438     |\n",
            "|    total_timesteps  | 292000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.6      |\n",
            "|    n_updates        | 72749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=295000, episode_reward=-50205.35 +/- 12.60\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 295000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.2      |\n",
            "|    n_updates        | 73499     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 296       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 22801     |\n",
            "|    total_timesteps  | 296000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 20.8      |\n",
            "|    n_updates        | 73749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=300000, episode_reward=-50201.09 +/- 6.74\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 300000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16.8      |\n",
            "|    n_updates        | 74749     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 300       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 23161     |\n",
            "|    total_timesteps  | 300000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 304       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 23397     |\n",
            "|    total_timesteps  | 304000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.1      |\n",
            "|    n_updates        | 75749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=305000, episode_reward=-50206.24 +/- 8.87\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 305000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.3      |\n",
            "|    n_updates        | 75999     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 308       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 23758     |\n",
            "|    total_timesteps  | 308000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.4      |\n",
            "|    n_updates        | 76749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=310000, episode_reward=-50206.38 +/- 10.66\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 310000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.4      |\n",
            "|    n_updates        | 77249     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 312       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 24120     |\n",
            "|    total_timesteps  | 312000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 8.26      |\n",
            "|    n_updates        | 77749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=315000, episode_reward=-50198.97 +/- 6.12\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 315000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.7      |\n",
            "|    n_updates        | 78499     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 316       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 24485     |\n",
            "|    total_timesteps  | 316000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 10.8      |\n",
            "|    n_updates        | 78749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=320000, episode_reward=-50204.55 +/- 9.67\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 320000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 9.34      |\n",
            "|    n_updates        | 79749     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 320       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 24848     |\n",
            "|    total_timesteps  | 320000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 324       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 25085     |\n",
            "|    total_timesteps  | 324000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.6      |\n",
            "|    n_updates        | 80749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=325000, episode_reward=-50201.25 +/- 12.63\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 325000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16        |\n",
            "|    n_updates        | 80999     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 997       |\n",
            "|    ep_rew_mean      | -4.98e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 328       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 25433     |\n",
            "|    total_timesteps  | 327697    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.2      |\n",
            "|    n_updates        | 81674     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=330000, episode_reward=-50213.95 +/- 4.77\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 330000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.6      |\n",
            "|    n_updates        | 82249     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -4.99e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 332       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 25814     |\n",
            "|    total_timesteps  | 332000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 19.4      |\n",
            "|    n_updates        | 82749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=335000, episode_reward=-50207.87 +/- 8.54\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 335000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.8      |\n",
            "|    n_updates        | 83499     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -4.99e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 336       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 26180     |\n",
            "|    total_timesteps  | 336000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.9      |\n",
            "|    n_updates        | 83749     |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=340000, episode_reward=-50192.00 +/- 22.18\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 340000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.8      |\n",
            "|    n_updates        | 84749     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 340      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 26546    |\n",
            "|    total_timesteps  | 340000   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 344      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 26785    |\n",
            "|    total_timesteps  | 344000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 13       |\n",
            "|    n_updates        | 85749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=345000, episode_reward=-50208.73 +/- 11.71\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 345000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 27        |\n",
            "|    n_updates        | 85999     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 348      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 27153    |\n",
            "|    total_timesteps  | 348000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 13.6     |\n",
            "|    n_updates        | 86749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=350000, episode_reward=-50210.24 +/- 6.68\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 350000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.8      |\n",
            "|    n_updates        | 87249     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 352      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 27516    |\n",
            "|    total_timesteps  | 352000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 14.5     |\n",
            "|    n_updates        | 87749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=355000, episode_reward=-50197.29 +/- 17.55\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 355000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.7      |\n",
            "|    n_updates        | 88499     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 356      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 27880    |\n",
            "|    total_timesteps  | 356000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 16       |\n",
            "|    n_updates        | 88749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=360000, episode_reward=-50214.35 +/- 11.06\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 360000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 24.8      |\n",
            "|    n_updates        | 89749     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 360      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 28245    |\n",
            "|    total_timesteps  | 360000   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 364      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 28483    |\n",
            "|    total_timesteps  | 364000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 13.6     |\n",
            "|    n_updates        | 90749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=365000, episode_reward=-41878.43 +/- 4469.47\n",
            "Episode length: 911.80 +/- 48.47\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 912       |\n",
            "|    mean_reward      | -4.19e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 365000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.4      |\n",
            "|    n_updates        | 90999     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 368      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 28834    |\n",
            "|    total_timesteps  | 368000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 11.6     |\n",
            "|    n_updates        | 91749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=370000, episode_reward=-50216.02 +/- 11.25\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 370000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.4      |\n",
            "|    n_updates        | 92249     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 372      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 29199    |\n",
            "|    total_timesteps  | 372000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 21.4     |\n",
            "|    n_updates        | 92749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=375000, episode_reward=-50195.37 +/- 11.31\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 375000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.1      |\n",
            "|    n_updates        | 93499     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 376      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 29565    |\n",
            "|    total_timesteps  | 376000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 28.1     |\n",
            "|    n_updates        | 93749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=380000, episode_reward=-36103.28 +/- 6437.66\n",
            "Episode length: 844.00 +/- 73.12\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 844       |\n",
            "|    mean_reward      | -3.61e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 380000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.2      |\n",
            "|    n_updates        | 94749     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 380      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 29909    |\n",
            "|    total_timesteps  | 380000   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 384      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 30145    |\n",
            "|    total_timesteps  | 384000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 14.6     |\n",
            "|    n_updates        | 95749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=385000, episode_reward=-50204.37 +/- 15.29\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 385000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16.7      |\n",
            "|    n_updates        | 95999     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 388      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 30509    |\n",
            "|    total_timesteps  | 388000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 12.2     |\n",
            "|    n_updates        | 96749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=390000, episode_reward=-50209.47 +/- 12.25\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 390000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.9      |\n",
            "|    n_updates        | 97249     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 392      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 30875    |\n",
            "|    total_timesteps  | 392000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 11.5     |\n",
            "|    n_updates        | 97749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=395000, episode_reward=-50208.11 +/- 7.86\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 395000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.4      |\n",
            "|    n_updates        | 98499     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 396      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 31241    |\n",
            "|    total_timesteps  | 396000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 12.3     |\n",
            "|    n_updates        | 98749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=400000, episode_reward=-50201.55 +/- 8.97\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 400000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16.7      |\n",
            "|    n_updates        | 99749     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 400      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 31608    |\n",
            "|    total_timesteps  | 400000   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 404      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 31849    |\n",
            "|    total_timesteps  | 404000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 11.2     |\n",
            "|    n_updates        | 100749   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=405000, episode_reward=-50211.67 +/- 3.57\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 405000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.2      |\n",
            "|    n_updates        | 100999    |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 408      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 32223    |\n",
            "|    total_timesteps  | 408000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 13.2     |\n",
            "|    n_updates        | 101749   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=410000, episode_reward=-50202.65 +/- 19.95\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 410000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.5      |\n",
            "|    n_updates        | 102249    |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 412      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 32595    |\n",
            "|    total_timesteps  | 412000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 14.2     |\n",
            "|    n_updates        | 102749   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=415000, episode_reward=-50214.33 +/- 9.58\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 415000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.6      |\n",
            "|    n_updates        | 103499    |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 416      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 32979    |\n",
            "|    total_timesteps  | 416000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 8.73     |\n",
            "|    n_updates        | 103749   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=420000, episode_reward=-50147.66 +/- 31.90\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.01e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 420000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 22.7      |\n",
            "|    n_updates        | 104749    |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 420      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 33351    |\n",
            "|    total_timesteps  | 420000   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 424      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 33595    |\n",
            "|    total_timesteps  | 424000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 16.8     |\n",
            "|    n_updates        | 105749   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=425000, episode_reward=-50205.00 +/- 13.29\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 425000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 8.23      |\n",
            "|    n_updates        | 105999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 428       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 33963     |\n",
            "|    total_timesteps  | 428000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.5      |\n",
            "|    n_updates        | 106749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=430000, episode_reward=-50181.91 +/- 14.91\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 430000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.1      |\n",
            "|    n_updates        | 107249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 432       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 34334     |\n",
            "|    total_timesteps  | 432000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.9      |\n",
            "|    n_updates        | 107749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=435000, episode_reward=-50208.04 +/- 13.11\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 435000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.5      |\n",
            "|    n_updates        | 108499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 436       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 34706     |\n",
            "|    total_timesteps  | 436000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.3      |\n",
            "|    n_updates        | 108749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=440000, episode_reward=-10668.34 +/- 7193.35\n",
            "Episode length: 439.00 +/- 136.50\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 439       |\n",
            "|    mean_reward      | -1.07e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 440000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.8      |\n",
            "|    n_updates        | 109749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 440       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 35003     |\n",
            "|    total_timesteps  | 440000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 444       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 35246     |\n",
            "|    total_timesteps  | 444000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 9.51      |\n",
            "|    n_updates        | 110749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=445000, episode_reward=-50214.01 +/- 7.50\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 445000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.4      |\n",
            "|    n_updates        | 110999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 448       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 35616     |\n",
            "|    total_timesteps  | 448000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.3      |\n",
            "|    n_updates        | 111749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=450000, episode_reward=-50187.17 +/- 20.24\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 450000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 19        |\n",
            "|    n_updates        | 112249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 452       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 35986     |\n",
            "|    total_timesteps  | 452000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.6      |\n",
            "|    n_updates        | 112749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=455000, episode_reward=-29709.04 +/- 21149.71\n",
            "Episode length: 692.60 +/- 334.04\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 693       |\n",
            "|    mean_reward      | -2.97e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 455000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.8      |\n",
            "|    n_updates        | 113499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 456       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 36318     |\n",
            "|    total_timesteps  | 456000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.5      |\n",
            "|    n_updates        | 113749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=460000, episode_reward=-50212.48 +/- 7.67\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 460000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.8      |\n",
            "|    n_updates        | 114749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 460       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 36687     |\n",
            "|    total_timesteps  | 460000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 464       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 36929     |\n",
            "|    total_timesteps  | 464000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 21.8      |\n",
            "|    n_updates        | 115749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=465000, episode_reward=-50200.33 +/- 11.08\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 465000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 22.6      |\n",
            "|    n_updates        | 115999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 468       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 37301     |\n",
            "|    total_timesteps  | 468000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.1      |\n",
            "|    n_updates        | 116749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=470000, episode_reward=-50210.62 +/- 12.48\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 470000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16.7      |\n",
            "|    n_updates        | 117249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 472       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 37676     |\n",
            "|    total_timesteps  | 472000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.5      |\n",
            "|    n_updates        | 117749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=475000, episode_reward=-50171.23 +/- 25.02\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 475000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 29.3      |\n",
            "|    n_updates        | 118499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 476       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 38048     |\n",
            "|    total_timesteps  | 476000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.9      |\n",
            "|    n_updates        | 118749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=480000, episode_reward=-50188.59 +/- 30.96\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 480000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.4      |\n",
            "|    n_updates        | 119749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 480       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 38418     |\n",
            "|    total_timesteps  | 480000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 484       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 38661     |\n",
            "|    total_timesteps  | 484000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 10.6      |\n",
            "|    n_updates        | 120749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=485000, episode_reward=-50210.66 +/- 15.94\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 485000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 19.9      |\n",
            "|    n_updates        | 120999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 488       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 39035     |\n",
            "|    total_timesteps  | 488000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14        |\n",
            "|    n_updates        | 121749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=490000, episode_reward=-50198.43 +/- 4.72\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 490000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.2      |\n",
            "|    n_updates        | 122249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 492       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 39409     |\n",
            "|    total_timesteps  | 492000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.4      |\n",
            "|    n_updates        | 122749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=495000, episode_reward=-50212.10 +/- 11.27\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 495000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.4      |\n",
            "|    n_updates        | 123499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 496       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 39779     |\n",
            "|    total_timesteps  | 496000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.4      |\n",
            "|    n_updates        | 123749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=500000, episode_reward=-50209.52 +/- 11.93\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 500000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.8      |\n",
            "|    n_updates        | 124749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 500       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 40168     |\n",
            "|    total_timesteps  | 500000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 504       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 40417     |\n",
            "|    total_timesteps  | 504000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.9      |\n",
            "|    n_updates        | 125749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=505000, episode_reward=-50199.54 +/- 15.31\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 505000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.1      |\n",
            "|    n_updates        | 125999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 508       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 40798     |\n",
            "|    total_timesteps  | 508000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 10.3      |\n",
            "|    n_updates        | 126749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=510000, episode_reward=-50215.74 +/- 10.88\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 510000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 19        |\n",
            "|    n_updates        | 127249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 512       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 41172     |\n",
            "|    total_timesteps  | 512000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.9      |\n",
            "|    n_updates        | 127749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=515000, episode_reward=-50194.78 +/- 13.80\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 515000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16.5      |\n",
            "|    n_updates        | 128499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 516       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 41547     |\n",
            "|    total_timesteps  | 516000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 19.6      |\n",
            "|    n_updates        | 128749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=520000, episode_reward=-50204.87 +/- 15.31\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 520000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16.9      |\n",
            "|    n_updates        | 129749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 520       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 41927     |\n",
            "|    total_timesteps  | 520000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 524       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 42174     |\n",
            "|    total_timesteps  | 524000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15        |\n",
            "|    n_updates        | 130749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=525000, episode_reward=-50212.83 +/- 14.05\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 525000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 20.2      |\n",
            "|    n_updates        | 130999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 528       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 42547     |\n",
            "|    total_timesteps  | 528000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 18.4      |\n",
            "|    n_updates        | 131749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=530000, episode_reward=-50217.56 +/- 8.95\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 530000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.4      |\n",
            "|    n_updates        | 132249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 532       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 42926     |\n",
            "|    total_timesteps  | 532000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 19.4      |\n",
            "|    n_updates        | 132749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=535000, episode_reward=-50216.35 +/- 5.83\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 535000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 18.7      |\n",
            "|    n_updates        | 133499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 536       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 43302     |\n",
            "|    total_timesteps  | 536000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 22.6      |\n",
            "|    n_updates        | 133749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=540000, episode_reward=-50165.65 +/- 26.51\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 540000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 22.3      |\n",
            "|    n_updates        | 134749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 540       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 43680     |\n",
            "|    total_timesteps  | 540000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 544       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 43929     |\n",
            "|    total_timesteps  | 544000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16.6      |\n",
            "|    n_updates        | 135749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=545000, episode_reward=-50213.11 +/- 15.09\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 545000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 18.5      |\n",
            "|    n_updates        | 135999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 548       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 44306     |\n",
            "|    total_timesteps  | 548000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 20.5      |\n",
            "|    n_updates        | 136749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=550000, episode_reward=-50215.36 +/- 12.14\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 550000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.8      |\n",
            "|    n_updates        | 137249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 552       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 44683     |\n",
            "|    total_timesteps  | 552000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.8      |\n",
            "|    n_updates        | 137749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=555000, episode_reward=-50208.38 +/- 10.12\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 555000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.1      |\n",
            "|    n_updates        | 138499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 556       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 45061     |\n",
            "|    total_timesteps  | 556000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14        |\n",
            "|    n_updates        | 138749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=560000, episode_reward=-47902.41 +/- 3268.16\n",
            "Episode length: 976.40 +/- 34.58\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 976       |\n",
            "|    mean_reward      | -4.79e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 560000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.4      |\n",
            "|    n_updates        | 139749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 560       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 45434     |\n",
            "|    total_timesteps  | 560000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 564       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 45684     |\n",
            "|    total_timesteps  | 564000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.7      |\n",
            "|    n_updates        | 140749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=565000, episode_reward=-50194.98 +/- 5.68\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 565000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.7      |\n",
            "|    n_updates        | 140999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 568       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 46064     |\n",
            "|    total_timesteps  | 568000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.2      |\n",
            "|    n_updates        | 141749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=570000, episode_reward=-50202.83 +/- 9.76\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 570000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 18.6      |\n",
            "|    n_updates        | 142249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 572       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 46448     |\n",
            "|    total_timesteps  | 572000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.8      |\n",
            "|    n_updates        | 142749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=575000, episode_reward=-50191.37 +/- 11.64\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 575000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 18.4      |\n",
            "|    n_updates        | 143499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 576       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 46825     |\n",
            "|    total_timesteps  | 576000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.9      |\n",
            "|    n_updates        | 143749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=580000, episode_reward=-50210.62 +/- 7.79\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 580000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.8      |\n",
            "|    n_updates        | 144749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 580       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 47203     |\n",
            "|    total_timesteps  | 580000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 584       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 47453     |\n",
            "|    total_timesteps  | 584000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 25.8      |\n",
            "|    n_updates        | 145749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=585000, episode_reward=-50217.94 +/- 10.36\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 585000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 19.1      |\n",
            "|    n_updates        | 145999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 588       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 47836     |\n",
            "|    total_timesteps  | 588000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 9.18      |\n",
            "|    n_updates        | 146749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=590000, episode_reward=-50205.60 +/- 19.41\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 590000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 22.5      |\n",
            "|    n_updates        | 147249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 592       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 48214     |\n",
            "|    total_timesteps  | 592000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 25.9      |\n",
            "|    n_updates        | 147749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=595000, episode_reward=-50209.31 +/- 8.29\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 595000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.7      |\n",
            "|    n_updates        | 148499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 596       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 48595     |\n",
            "|    total_timesteps  | 596000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.2      |\n",
            "|    n_updates        | 148749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=600000, episode_reward=-50202.02 +/- 12.21\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 600000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16.2      |\n",
            "|    n_updates        | 149749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 600       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 48983     |\n",
            "|    total_timesteps  | 600000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 604       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 49234     |\n",
            "|    total_timesteps  | 604000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 21.8      |\n",
            "|    n_updates        | 150749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=605000, episode_reward=-50208.29 +/- 13.49\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 605000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 21.9      |\n",
            "|    n_updates        | 150999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 608       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 49614     |\n",
            "|    total_timesteps  | 608000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16        |\n",
            "|    n_updates        | 151749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=610000, episode_reward=-50205.95 +/- 19.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 610000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 18.2      |\n",
            "|    n_updates        | 152249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 999       |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 612       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 49992     |\n",
            "|    total_timesteps  | 611941    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.5      |\n",
            "|    n_updates        | 152735    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=615000, episode_reward=-50201.18 +/- 18.62\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 615000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.7      |\n",
            "|    n_updates        | 153499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 616       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 50377     |\n",
            "|    total_timesteps  | 616000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 21.9      |\n",
            "|    n_updates        | 153749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=620000, episode_reward=-13082.57 +/- 5533.32\n",
            "Episode length: 494.20 +/- 123.18\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 494       |\n",
            "|    mean_reward      | -1.31e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 620000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 19.1      |\n",
            "|    n_updates        | 154749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 620       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 50693     |\n",
            "|    total_timesteps  | 620000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 624       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 50944     |\n",
            "|    total_timesteps  | 624000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16.3      |\n",
            "|    n_updates        | 155749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=625000, episode_reward=-50195.24 +/- 16.24\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 625000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 25.6      |\n",
            "|    n_updates        | 155999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 628       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 51328     |\n",
            "|    total_timesteps  | 628000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.8      |\n",
            "|    n_updates        | 156749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=630000, episode_reward=-50199.18 +/- 13.17\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 630000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.9      |\n",
            "|    n_updates        | 157249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 632       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 51714     |\n",
            "|    total_timesteps  | 632000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 19        |\n",
            "|    n_updates        | 157749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=635000, episode_reward=-50213.33 +/- 15.05\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 635000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11.5      |\n",
            "|    n_updates        | 158499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 636       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 52098     |\n",
            "|    total_timesteps  | 636000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.9      |\n",
            "|    n_updates        | 158749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=640000, episode_reward=-50206.66 +/- 7.01\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 640000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 24.8      |\n",
            "|    n_updates        | 159749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 640       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 52457     |\n",
            "|    total_timesteps  | 640000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 644       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 52693     |\n",
            "|    total_timesteps  | 644000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.1      |\n",
            "|    n_updates        | 160749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=645000, episode_reward=-50193.48 +/- 15.21\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 645000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 23.1      |\n",
            "|    n_updates        | 160999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 648       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 53017     |\n",
            "|    total_timesteps  | 648000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 24.5      |\n",
            "|    n_updates        | 161749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=650000, episode_reward=-50216.17 +/- 10.16\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 650000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 18.2      |\n",
            "|    n_updates        | 162249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 652       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 53305     |\n",
            "|    total_timesteps  | 652000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 26.3      |\n",
            "|    n_updates        | 162749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=655000, episode_reward=-50208.79 +/- 19.72\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 655000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.5      |\n",
            "|    n_updates        | 163499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 656       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 53593     |\n",
            "|    total_timesteps  | 656000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 22        |\n",
            "|    n_updates        | 163749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=660000, episode_reward=-50203.31 +/- 12.53\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 660000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.7      |\n",
            "|    n_updates        | 164749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 660       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 53843     |\n",
            "|    total_timesteps  | 660000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 664       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 54100     |\n",
            "|    total_timesteps  | 664000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 22.1      |\n",
            "|    n_updates        | 165749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=665000, episode_reward=-50216.05 +/- 12.17\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 665000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 10.4      |\n",
            "|    n_updates        | 165999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 668       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 54510     |\n",
            "|    total_timesteps  | 668000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 21.9      |\n",
            "|    n_updates        | 166749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=670000, episode_reward=-50196.67 +/- 6.17\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 670000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16.9      |\n",
            "|    n_updates        | 167249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 672       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 55077     |\n",
            "|    total_timesteps  | 672000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 20.2      |\n",
            "|    n_updates        | 167749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=675000, episode_reward=-50202.87 +/- 16.08\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 675000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 19.9      |\n",
            "|    n_updates        | 168499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 676       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 55545     |\n",
            "|    total_timesteps  | 676000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.2      |\n",
            "|    n_updates        | 168749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=680000, episode_reward=-50219.50 +/- 8.94\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 680000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16.1      |\n",
            "|    n_updates        | 169749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 680       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 55896     |\n",
            "|    total_timesteps  | 680000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 684       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 56113     |\n",
            "|    total_timesteps  | 684000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 43.2      |\n",
            "|    n_updates        | 170749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=685000, episode_reward=-50208.22 +/- 15.60\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 685000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 34.6      |\n",
            "|    n_updates        | 170999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 688       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 56446     |\n",
            "|    total_timesteps  | 688000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 21.3      |\n",
            "|    n_updates        | 171749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=690000, episode_reward=-50198.93 +/- 17.01\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 690000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 20.6      |\n",
            "|    n_updates        | 172249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 692       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 56782     |\n",
            "|    total_timesteps  | 692000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 11        |\n",
            "|    n_updates        | 172749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=695000, episode_reward=-50208.12 +/- 9.70\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 695000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 20.5      |\n",
            "|    n_updates        | 173499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 696       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 57140     |\n",
            "|    total_timesteps  | 696000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.5      |\n",
            "|    n_updates        | 173749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=700000, episode_reward=-30409.64 +/- 1976.13\n",
            "Episode length: 777.00 +/- 25.44\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 777       |\n",
            "|    mean_reward      | -3.04e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 700000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.3      |\n",
            "|    n_updates        | 174749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 700       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 57523     |\n",
            "|    total_timesteps  | 700000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 704       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 57802     |\n",
            "|    total_timesteps  | 704000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14        |\n",
            "|    n_updates        | 175749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=705000, episode_reward=-50212.34 +/- 7.32\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 705000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.9      |\n",
            "|    n_updates        | 175999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 708       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 58248     |\n",
            "|    total_timesteps  | 708000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.8      |\n",
            "|    n_updates        | 176749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=710000, episode_reward=-50205.71 +/- 6.32\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 710000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 27.9      |\n",
            "|    n_updates        | 177249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 712       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 58654     |\n",
            "|    total_timesteps  | 712000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.1      |\n",
            "|    n_updates        | 177749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=715000, episode_reward=-50202.98 +/- 11.08\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 715000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 26.4      |\n",
            "|    n_updates        | 178499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 716       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 59012     |\n",
            "|    total_timesteps  | 716000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 34        |\n",
            "|    n_updates        | 178749    |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 999      |\n",
            "|    ep_rew_mean      | -5e+04   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 720      |\n",
            "|    fps              | 12       |\n",
            "|    time_elapsed     | 59243    |\n",
            "|    total_timesteps  | 719871   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0005   |\n",
            "|    loss             | 22.9     |\n",
            "|    n_updates        | 179717   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=720000, episode_reward=-50212.21 +/- 12.55\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 720000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 22.6      |\n",
            "|    n_updates        | 179749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 724       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 59617     |\n",
            "|    total_timesteps  | 724000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 22.4      |\n",
            "|    n_updates        | 180749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=725000, episode_reward=-50196.83 +/- 8.75\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 725000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13        |\n",
            "|    n_updates        | 180999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 728       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 59986     |\n",
            "|    total_timesteps  | 728000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 21.1      |\n",
            "|    n_updates        | 181749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=730000, episode_reward=-50212.45 +/- 7.06\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 730000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 20.3      |\n",
            "|    n_updates        | 182249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 732       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 60334     |\n",
            "|    total_timesteps  | 732000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 23.8      |\n",
            "|    n_updates        | 182749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=735000, episode_reward=-50214.12 +/- 7.55\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 735000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.5      |\n",
            "|    n_updates        | 183499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 736       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 60672     |\n",
            "|    total_timesteps  | 736000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 9.53      |\n",
            "|    n_updates        | 183749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=740000, episode_reward=-43336.48 +/- 6079.35\n",
            "Episode length: 926.60 +/- 66.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 927       |\n",
            "|    mean_reward      | -4.33e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 740000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.4      |\n",
            "|    n_updates        | 184749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 740       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 61000     |\n",
            "|    total_timesteps  | 740000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 744       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 61220     |\n",
            "|    total_timesteps  | 744000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 18.7      |\n",
            "|    n_updates        | 185749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=745000, episode_reward=-50206.16 +/- 10.31\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 745000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 19.3      |\n",
            "|    n_updates        | 185999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 748       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 61557     |\n",
            "|    total_timesteps  | 748000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 22.9      |\n",
            "|    n_updates        | 186749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=750000, episode_reward=-50199.46 +/- 17.50\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 750000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 19        |\n",
            "|    n_updates        | 187249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 752       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 61896     |\n",
            "|    total_timesteps  | 752000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.7      |\n",
            "|    n_updates        | 187749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=755000, episode_reward=-31639.41 +/- 9715.70\n",
            "Episode length: 785.00 +/- 115.08\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 785       |\n",
            "|    mean_reward      | -3.16e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 755000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 21.1      |\n",
            "|    n_updates        | 188499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 756       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 62209     |\n",
            "|    total_timesteps  | 756000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.7      |\n",
            "|    n_updates        | 188749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=760000, episode_reward=-50204.60 +/- 9.54\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 760000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 14.9      |\n",
            "|    n_updates        | 189749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 760       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 62547     |\n",
            "|    total_timesteps  | 760000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 764       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 62767     |\n",
            "|    total_timesteps  | 764000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 33.6      |\n",
            "|    n_updates        | 190749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=765000, episode_reward=-50185.43 +/- 32.48\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 765000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.6      |\n",
            "|    n_updates        | 190999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 768       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 63106     |\n",
            "|    total_timesteps  | 768000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 30.1      |\n",
            "|    n_updates        | 191749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=770000, episode_reward=-50210.65 +/- 9.24\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 770000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 21.2      |\n",
            "|    n_updates        | 192249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 772       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 63448     |\n",
            "|    total_timesteps  | 772000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 33.9      |\n",
            "|    n_updates        | 192749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=775000, episode_reward=-50206.59 +/- 10.69\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 775000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13        |\n",
            "|    n_updates        | 193499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 776       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 63790     |\n",
            "|    total_timesteps  | 776000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 32        |\n",
            "|    n_updates        | 193749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=780000, episode_reward=-50199.40 +/- 29.16\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 780000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 15.1      |\n",
            "|    n_updates        | 194749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 780       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 64129     |\n",
            "|    total_timesteps  | 780000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 784       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 64347     |\n",
            "|    total_timesteps  | 784000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 33.6      |\n",
            "|    n_updates        | 195749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=785000, episode_reward=-50203.04 +/- 7.62\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 785000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 20.9      |\n",
            "|    n_updates        | 195999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 788       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 64687     |\n",
            "|    total_timesteps  | 788000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 23.5      |\n",
            "|    n_updates        | 196749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=790000, episode_reward=-50209.26 +/- 11.84\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 790000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17        |\n",
            "|    n_updates        | 197249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 792       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 65026     |\n",
            "|    total_timesteps  | 792000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.1      |\n",
            "|    n_updates        | 197749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=795000, episode_reward=-50194.26 +/- 9.47\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 795000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 38.4      |\n",
            "|    n_updates        | 198499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 796       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 65367     |\n",
            "|    total_timesteps  | 796000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 31.8      |\n",
            "|    n_updates        | 198749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=800000, episode_reward=-50212.25 +/- 13.89\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 800000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 27.1      |\n",
            "|    n_updates        | 199749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 800       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 65709     |\n",
            "|    total_timesteps  | 800000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 804       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 65930     |\n",
            "|    total_timesteps  | 804000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 10.7      |\n",
            "|    n_updates        | 200749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=805000, episode_reward=-50210.46 +/- 6.95\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 805000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 20.3      |\n",
            "|    n_updates        | 200999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 808       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 66274     |\n",
            "|    total_timesteps  | 808000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 9.56      |\n",
            "|    n_updates        | 201749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=810000, episode_reward=-50207.07 +/- 12.17\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 810000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 31        |\n",
            "|    n_updates        | 202249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 812       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 66620     |\n",
            "|    total_timesteps  | 812000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16.1      |\n",
            "|    n_updates        | 202749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=815000, episode_reward=-50210.20 +/- 8.85\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 815000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 26.7      |\n",
            "|    n_updates        | 203499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.01e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 816       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 66965     |\n",
            "|    total_timesteps  | 816000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 22        |\n",
            "|    n_updates        | 203749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=820000, episode_reward=-50183.96 +/- 25.42\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 820000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13        |\n",
            "|    n_updates        | 204749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 820       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 67296     |\n",
            "|    total_timesteps  | 820000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 824       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 67526     |\n",
            "|    total_timesteps  | 824000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 28.7      |\n",
            "|    n_updates        | 205749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=825000, episode_reward=-50204.71 +/- 15.07\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 825000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.7      |\n",
            "|    n_updates        | 205999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 828       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 67890     |\n",
            "|    total_timesteps  | 828000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 12.1      |\n",
            "|    n_updates        | 206749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=830000, episode_reward=-50204.52 +/- 14.21\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 830000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 17.3      |\n",
            "|    n_updates        | 207249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 832       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 68236     |\n",
            "|    total_timesteps  | 832000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 21.5      |\n",
            "|    n_updates        | 207749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=835000, episode_reward=-50208.19 +/- 5.67\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 835000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 27.3      |\n",
            "|    n_updates        | 208499    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 836       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 68559     |\n",
            "|    total_timesteps  | 836000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 18.3      |\n",
            "|    n_updates        | 208749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=840000, episode_reward=-50202.96 +/- 4.89\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 840000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16.8      |\n",
            "|    n_updates        | 209749    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 840       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 68879     |\n",
            "|    total_timesteps  | 840000    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 844       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 69081     |\n",
            "|    total_timesteps  | 844000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 16        |\n",
            "|    n_updates        | 210749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=845000, episode_reward=-50196.14 +/- 13.08\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 1e+03     |\n",
            "|    mean_reward      | -5.02e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 845000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 13.4      |\n",
            "|    n_updates        | 210999    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 848       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 69399     |\n",
            "|    total_timesteps  | 848000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 19.1      |\n",
            "|    n_updates        | 211749    |\n",
            "-----------------------------------\n",
            "Eval num_timesteps=850000, episode_reward=-41928.25 +/- 6133.67\n",
            "Episode length: 910.60 +/- 70.55\n",
            "-----------------------------------\n",
            "| eval/               |           |\n",
            "|    mean_ep_length   | 911       |\n",
            "|    mean_reward      | -4.19e+04 |\n",
            "| rollout/            |           |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    total_timesteps  | 850000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 23.3      |\n",
            "|    n_updates        | 212249    |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1e+03     |\n",
            "|    ep_rew_mean      | -5.02e+04 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 852       |\n",
            "|    fps              | 12        |\n",
            "|    time_elapsed     | 69705     |\n",
            "|    total_timesteps  | 852000    |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0005    |\n",
            "|    loss             | 19.4      |\n",
            "|    n_updates        | 212749    |\n",
            "-----------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Simple DQN architecture\n",
        "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
        "\n",
        "# Set up the model with simpler hyperparameters\n",
        "model = DQN('CnnPolicy', custom_env, policy_kwargs=policy_kwargs,\n",
        "            verbose=1, buffer_size=50000, learning_starts=1000,\n",
        "            learning_rate=0.0005, batch_size=32, exploration_fraction=0.1,\n",
        "            exploration_final_eps=0.05)\n",
        "\n",
        "# Setup evaluation and checkpoint callbacks\n",
        "eval_callback = EvalCallback(custom_env, best_model_save_path=model_dir,\n",
        "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
        "                             deterministic=True, render=False)\n",
        "\n",
        "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
        "                                         name_prefix='dqn_model_checkpoint')\n",
        "\n",
        "\n",
        "\n",
        "# Start training the model with callbacks for evaluation and checkpoints\n",
        "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
        "\n",
        "# Save the final model after training\n",
        "model.save(\"dqn_custom_env_model\")\n",
        "custom_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mumWC4SV5iaQ"
      },
      "outputs": [],
      "source": [
        "custom_env.close()\n",
        "del custom_env\n",
        "foo = gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3o-RjXd5iaQ"
      },
      "source": [
        "#### 2.2.2 PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP3j0t615iaR"
      },
      "outputs": [],
      "source": [
        "custom_env = EnhancedCarRacing(render_mode=\"rgb_array\")\n",
        "obs, info = custom_env.reset()\n",
        "custom_env = GrayscaleObservation(custom_env, keep_dim=True)\n",
        "custom_env = TimeLimit(custom_env, max_episode_steps=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvAd_D5B5iaR"
      },
      "outputs": [],
      "source": [
        "#create directories\n",
        "logs_dir = 'PPO_env_mod_logs'\n",
        "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
        "os.makedirs(logs_path, exist_ok=True)\n",
        "\n",
        "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
        "model_dir = os.path.join(logs_path, \"models\")\n",
        "os.makedirs(tensorboard_dir, exist_ok=True)\n",
        "os.makedirs(model_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjXJKbFu5iaS",
        "outputId": "ea3eb27c-aa8a-4634-ecd2-bef721f52ab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W1217 21:07:15.281412367 NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
            "/home/derpy/Documents/Fac/ano3/semestre1/ISIA/reinforcement-learning-with-gymnasium/.venv/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x76bd92db0620> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x76bd92db1970>\n",
            "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 77        |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 26        |\n",
            "|    total_timesteps | 2048      |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 55           |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 73           |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035775134 |\n",
            "|    clip_fraction        | 0.0174       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.22        |\n",
            "|    explained_variance   | 4.82e-05     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 4.2e+05      |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.000235    |\n",
            "|    std                  | 0.978        |\n",
            "|    value_loss           | 8.75e+05     |\n",
            "------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/derpy/Documents/Fac/ano3/semestre1/ISIA/reinforcement-learning-with-gymnasium/.venv/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval num_timesteps=5000, episode_reward=-50217.84 +/- 10.32\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 5000         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011489361 |\n",
            "|    clip_fraction        | 0.0149       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.18        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.68e+05     |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | 0.000451     |\n",
            "|    std                  | 0.975        |\n",
            "|    value_loss           | 8.78e+05     |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 36        |\n",
            "|    iterations      | 3         |\n",
            "|    time_elapsed    | 170       |\n",
            "|    total_timesteps | 6144      |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 37           |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 216          |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0055847196 |\n",
            "|    clip_fraction        | 0.0579       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.2         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 4.27e+05     |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00345     |\n",
            "|    std                  | 0.985        |\n",
            "|    value_loss           | 8.73e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=-50212.01 +/- 7.63\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 10000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005333224 |\n",
            "|    clip_fraction        | 0.0343      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.21       |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.34e+05    |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00142    |\n",
            "|    std                  | 0.981       |\n",
            "|    value_loss           | 8.68e+05    |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 33        |\n",
            "|    iterations      | 5         |\n",
            "|    time_elapsed    | 310       |\n",
            "|    total_timesteps | 10240     |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 35           |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 350          |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031201052 |\n",
            "|    clip_fraction        | 0.0192       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.22        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.94e+05     |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.00081     |\n",
            "|    std                  | 0.992        |\n",
            "|    value_loss           | 8.63e+05     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 36           |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 391          |\n",
            "|    total_timesteps      | 14336        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020285302 |\n",
            "|    clip_fraction        | 0.0167       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.61e+05     |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.000487    |\n",
            "|    std                  | 0.984        |\n",
            "|    value_loss           | 8.58e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=15000, episode_reward=-50208.68 +/- 10.32\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 15000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004211774 |\n",
            "|    clip_fraction        | 0.0249      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.21       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.21e+05    |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.000776   |\n",
            "|    std                  | 0.983       |\n",
            "|    value_loss           | 8.53e+05    |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 33        |\n",
            "|    iterations      | 8         |\n",
            "|    time_elapsed    | 488       |\n",
            "|    total_timesteps | 16384     |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 34          |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 529         |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003739717 |\n",
            "|    clip_fraction        | 0.0218      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.2        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.53e+05    |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.000129   |\n",
            "|    std                  | 0.983       |\n",
            "|    value_loss           | 8.49e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-50201.02 +/- 12.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 20000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005139638 |\n",
            "|    clip_fraction        | 0.039       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.21       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.11e+05    |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00214    |\n",
            "|    std                  | 0.985       |\n",
            "|    value_loss           | 8.44e+05    |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 10        |\n",
            "|    time_elapsed    | 625       |\n",
            "|    total_timesteps | 20480     |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 33           |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 666          |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053378707 |\n",
            "|    clip_fraction        | 0.0519       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.19        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 4.44e+05     |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.00325     |\n",
            "|    std                  | 0.974        |\n",
            "|    value_loss           | 8.41e+05     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 34           |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 706          |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020102104 |\n",
            "|    clip_fraction        | 0.00435      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.17        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 5.03e+05     |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | 0.00102      |\n",
            "|    std                  | 0.972        |\n",
            "|    value_loss           | 8.36e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=25000, episode_reward=-50212.16 +/- 12.61\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 25000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004693583 |\n",
            "|    clip_fraction        | 0.0378      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.17       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.6e+05     |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.00251    |\n",
            "|    std                  | 0.973       |\n",
            "|    value_loss           | 8.32e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 33        |\n",
            "|    iterations      | 13        |\n",
            "|    time_elapsed    | 801       |\n",
            "|    total_timesteps | 26624     |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 34           |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 842          |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035684058 |\n",
            "|    clip_fraction        | 0.0347       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.17        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 5.17e+05     |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.000967    |\n",
            "|    std                  | 0.971        |\n",
            "|    value_loss           | 8.29e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-50205.93 +/- 10.90\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 30000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051206285 |\n",
            "|    clip_fraction        | 0.0476       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.18        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 4.49e+05     |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.00327     |\n",
            "|    std                  | 0.975        |\n",
            "|    value_loss           | 8.25e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 15        |\n",
            "|    time_elapsed    | 936       |\n",
            "|    total_timesteps | 30720     |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 33          |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 977         |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004766228 |\n",
            "|    clip_fraction        | 0.0184      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.17       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.82e+05    |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.000203   |\n",
            "|    std                  | 0.969       |\n",
            "|    value_loss           | 8.22e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 34          |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 1018        |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004825528 |\n",
            "|    clip_fraction        | 0.0423      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.14       |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.59e+05    |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.00177    |\n",
            "|    std                  | 0.957       |\n",
            "|    value_loss           | 8.19e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=35000, episode_reward=-50216.07 +/- 13.70\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 35000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004123984 |\n",
            "|    clip_fraction        | 0.0331      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.11       |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.42e+05    |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0013     |\n",
            "|    std                  | 0.953       |\n",
            "|    value_loss           | 8.16e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 33        |\n",
            "|    iterations      | 18        |\n",
            "|    time_elapsed    | 1114      |\n",
            "|    total_timesteps | 36864     |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 33           |\n",
            "|    iterations           | 19           |\n",
            "|    time_elapsed         | 1154         |\n",
            "|    total_timesteps      | 38912        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040102806 |\n",
            "|    clip_fraction        | 0.0146       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.11        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.19e+05     |\n",
            "|    n_updates            | 180          |\n",
            "|    policy_gradient_loss | -0.000293    |\n",
            "|    std                  | 0.953        |\n",
            "|    value_loss           | 8.13e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-50210.18 +/- 21.48\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 40000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005955025 |\n",
            "|    clip_fraction        | 0.0626      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.11       |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.51e+05    |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.00444    |\n",
            "|    std                  | 0.952       |\n",
            "|    value_loss           | 8.11e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 20        |\n",
            "|    time_elapsed    | 1249      |\n",
            "|    total_timesteps | 40960     |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 33           |\n",
            "|    iterations           | 21           |\n",
            "|    time_elapsed         | 1290         |\n",
            "|    total_timesteps      | 43008        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058530797 |\n",
            "|    clip_fraction        | 0.0484       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.11        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.93e+05     |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | -0.00292     |\n",
            "|    std                  | 0.951        |\n",
            "|    value_loss           | 8.08e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=45000, episode_reward=-50199.35 +/- 13.13\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 45000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060578343 |\n",
            "|    clip_fraction        | 0.0574       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.1         |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 4.09e+05     |\n",
            "|    n_updates            | 210          |\n",
            "|    policy_gradient_loss | -0.00377     |\n",
            "|    std                  | 0.947        |\n",
            "|    value_loss           | 7.98e+05     |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 22        |\n",
            "|    time_elapsed    | 1386      |\n",
            "|    total_timesteps | 45056     |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 1427         |\n",
            "|    total_timesteps      | 47104        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044201384 |\n",
            "|    clip_fraction        | 0.0347       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.08        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.96e+05     |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | -0.00147     |\n",
            "|    std                  | 0.941        |\n",
            "|    value_loss           | 7.71e+05     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 33          |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 1469        |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004622897 |\n",
            "|    clip_fraction        | 0.0478      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.07       |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.11e+05    |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.00271    |\n",
            "|    std                  | 0.938       |\n",
            "|    value_loss           | 7.67e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-50211.83 +/- 9.06\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 50000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0056048874 |\n",
            "|    clip_fraction        | 0.0639       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.03        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.76e+05     |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | -0.00491     |\n",
            "|    std                  | 0.919        |\n",
            "|    value_loss           | 7.61e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 25        |\n",
            "|    time_elapsed    | 1564      |\n",
            "|    total_timesteps | 51200     |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 33           |\n",
            "|    iterations           | 26           |\n",
            "|    time_elapsed         | 1604         |\n",
            "|    total_timesteps      | 53248        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0057532904 |\n",
            "|    clip_fraction        | 0.0624       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4           |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 4.23e+05     |\n",
            "|    n_updates            | 250          |\n",
            "|    policy_gradient_loss | -0.00395     |\n",
            "|    std                  | 0.917        |\n",
            "|    value_loss           | 7.57e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=55000, episode_reward=-50210.49 +/- 12.14\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 55000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014560053 |\n",
            "|    clip_fraction        | 0.0152       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -4.02        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.49e+05     |\n",
            "|    n_updates            | 260          |\n",
            "|    policy_gradient_loss | -9.29e-06    |\n",
            "|    std                  | 0.929        |\n",
            "|    value_loss           | 7.52e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 27        |\n",
            "|    time_elapsed    | 1700      |\n",
            "|    total_timesteps | 55296     |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 1740        |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006216801 |\n",
            "|    clip_fraction        | 0.0321      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.02       |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.84e+05    |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.00226    |\n",
            "|    std                  | 0.926       |\n",
            "|    value_loss           | 7.48e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 33           |\n",
            "|    iterations           | 29           |\n",
            "|    time_elapsed         | 1781         |\n",
            "|    total_timesteps      | 59392        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044402946 |\n",
            "|    clip_fraction        | 0.0494       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.97        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 4.76e+05     |\n",
            "|    n_updates            | 280          |\n",
            "|    policy_gradient_loss | -0.00347     |\n",
            "|    std                  | 0.903        |\n",
            "|    value_loss           | 7.44e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-50210.22 +/- 19.05\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 60000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00445793 |\n",
            "|    clip_fraction        | 0.0231     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.92      |\n",
            "|    explained_variance   | 1.19e-07   |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 2.85e+05   |\n",
            "|    n_updates            | 290        |\n",
            "|    policy_gradient_loss | -0.00131   |\n",
            "|    std                  | 0.887      |\n",
            "|    value_loss           | 7.39e+05   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 30        |\n",
            "|    time_elapsed    | 1876      |\n",
            "|    total_timesteps | 61440     |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 33           |\n",
            "|    iterations           | 31           |\n",
            "|    time_elapsed         | 1917         |\n",
            "|    total_timesteps      | 63488        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050120624 |\n",
            "|    clip_fraction        | 0.0367       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.87        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 4.01e+05     |\n",
            "|    n_updates            | 300          |\n",
            "|    policy_gradient_loss | -0.00156     |\n",
            "|    std                  | 0.877        |\n",
            "|    value_loss           | 7.35e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=65000, episode_reward=-50209.90 +/- 10.90\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 65000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003344505 |\n",
            "|    clip_fraction        | 0.0294      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.89       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.96e+05    |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | -0.00123    |\n",
            "|    std                  | 0.892       |\n",
            "|    value_loss           | 7.31e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 32        |\n",
            "|    time_elapsed    | 2010      |\n",
            "|    total_timesteps | 65536     |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 2051        |\n",
            "|    total_timesteps      | 67584       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005554796 |\n",
            "|    clip_fraction        | 0.0383      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.92       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.37e+05    |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.00167    |\n",
            "|    std                  | 0.895       |\n",
            "|    value_loss           | 7.28e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 33           |\n",
            "|    iterations           | 34           |\n",
            "|    time_elapsed         | 2091         |\n",
            "|    total_timesteps      | 69632        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063190586 |\n",
            "|    clip_fraction        | 0.0379       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.91        |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.09e+05     |\n",
            "|    n_updates            | 330          |\n",
            "|    policy_gradient_loss | -0.0021      |\n",
            "|    std                  | 0.889        |\n",
            "|    value_loss           | 7.25e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-50211.83 +/- 12.82\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 70000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031381242 |\n",
            "|    clip_fraction        | 0.0193       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.89        |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 4.83e+05     |\n",
            "|    n_updates            | 340          |\n",
            "|    policy_gradient_loss | -0.000404    |\n",
            "|    std                  | 0.884        |\n",
            "|    value_loss           | 7.21e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 35        |\n",
            "|    time_elapsed    | 2186      |\n",
            "|    total_timesteps | 71680     |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 33           |\n",
            "|    iterations           | 36           |\n",
            "|    time_elapsed         | 2227         |\n",
            "|    total_timesteps      | 73728        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036438962 |\n",
            "|    clip_fraction        | 0.0177       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.89        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.31e+05     |\n",
            "|    n_updates            | 350          |\n",
            "|    policy_gradient_loss | 0.000471     |\n",
            "|    std                  | 0.886        |\n",
            "|    value_loss           | 7.18e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=75000, episode_reward=-50208.16 +/- 9.93\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 75000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038138516 |\n",
            "|    clip_fraction        | 0.0258       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.88        |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.67e+05     |\n",
            "|    n_updates            | 360          |\n",
            "|    policy_gradient_loss | -0.000555    |\n",
            "|    std                  | 0.882        |\n",
            "|    value_loss           | 7.16e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 37        |\n",
            "|    time_elapsed    | 2322      |\n",
            "|    total_timesteps | 75776     |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 38           |\n",
            "|    time_elapsed         | 2364         |\n",
            "|    total_timesteps      | 77824        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041654394 |\n",
            "|    clip_fraction        | 0.0189       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.89        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.26e+05     |\n",
            "|    n_updates            | 370          |\n",
            "|    policy_gradient_loss | -2.8e-05     |\n",
            "|    std                  | 0.89         |\n",
            "|    value_loss           | 7.13e+05     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 33           |\n",
            "|    iterations           | 39           |\n",
            "|    time_elapsed         | 2404         |\n",
            "|    total_timesteps      | 79872        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050997892 |\n",
            "|    clip_fraction        | 0.0304       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.92        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.38e+05     |\n",
            "|    n_updates            | 380          |\n",
            "|    policy_gradient_loss | -0.00168     |\n",
            "|    std                  | 0.9          |\n",
            "|    value_loss           | 7.1e+05      |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-50212.20 +/- 14.79\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 80000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051898835 |\n",
            "|    clip_fraction        | 0.0417       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.91        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 4.4e+05      |\n",
            "|    n_updates            | 390          |\n",
            "|    policy_gradient_loss | -0.0028      |\n",
            "|    std                  | 0.886        |\n",
            "|    value_loss           | 7.08e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 40        |\n",
            "|    time_elapsed    | 2500      |\n",
            "|    total_timesteps | 81920     |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 33           |\n",
            "|    iterations           | 41           |\n",
            "|    time_elapsed         | 2541         |\n",
            "|    total_timesteps      | 83968        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044674054 |\n",
            "|    clip_fraction        | 0.0285       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.88        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.56e+05     |\n",
            "|    n_updates            | 400          |\n",
            "|    policy_gradient_loss | -0.000351    |\n",
            "|    std                  | 0.885        |\n",
            "|    value_loss           | 7.05e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=85000, episode_reward=-50199.13 +/- 18.17\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 85000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042907177 |\n",
            "|    clip_fraction        | 0.0327       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.87        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.28e+05     |\n",
            "|    n_updates            | 410          |\n",
            "|    policy_gradient_loss | -0.00139     |\n",
            "|    std                  | 0.877        |\n",
            "|    value_loss           | 7.03e+05     |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 42        |\n",
            "|    time_elapsed    | 2636      |\n",
            "|    total_timesteps | 86016     |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 2677        |\n",
            "|    total_timesteps      | 88064       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005490194 |\n",
            "|    clip_fraction        | 0.0419      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.84       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.1e+05     |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | -0.0019     |\n",
            "|    std                  | 0.869       |\n",
            "|    value_loss           | 6.88e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-50210.92 +/- 10.63\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 90000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006035866 |\n",
            "|    clip_fraction        | 0.0537      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.84       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.9e+05     |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.003      |\n",
            "|    std                  | 0.875       |\n",
            "|    value_loss           | 6.69e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 44        |\n",
            "|    time_elapsed    | 2774      |\n",
            "|    total_timesteps | 90112     |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 2815        |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005123888 |\n",
            "|    clip_fraction        | 0.0439      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.86       |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.61e+05    |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.00281    |\n",
            "|    std                  | 0.881       |\n",
            "|    value_loss           | 6.65e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 46           |\n",
            "|    time_elapsed         | 2856         |\n",
            "|    total_timesteps      | 94208        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0055309064 |\n",
            "|    clip_fraction        | 0.0382       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.87        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.17e+05     |\n",
            "|    n_updates            | 450          |\n",
            "|    policy_gradient_loss | -0.00285     |\n",
            "|    std                  | 0.878        |\n",
            "|    value_loss           | 6.61e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=95000, episode_reward=-50206.47 +/- 9.09\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 95000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042765243 |\n",
            "|    clip_fraction        | 0.02         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.85        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.35e+05     |\n",
            "|    n_updates            | 460          |\n",
            "|    policy_gradient_loss | -0.000469    |\n",
            "|    std                  | 0.873        |\n",
            "|    value_loss           | 6.56e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 47        |\n",
            "|    time_elapsed    | 2951      |\n",
            "|    total_timesteps | 96256     |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 48           |\n",
            "|    time_elapsed         | 2991         |\n",
            "|    total_timesteps      | 98304        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046468657 |\n",
            "|    clip_fraction        | 0.0159       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.83        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.28e+05     |\n",
            "|    n_updates            | 470          |\n",
            "|    policy_gradient_loss | 0.000607     |\n",
            "|    std                  | 0.864        |\n",
            "|    value_loss           | 6.52e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-50206.77 +/- 10.92\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 100000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035653713 |\n",
            "|    clip_fraction        | 0.0232       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.82        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.65e+05     |\n",
            "|    n_updates            | 480          |\n",
            "|    policy_gradient_loss | -0.000496    |\n",
            "|    std                  | 0.869        |\n",
            "|    value_loss           | 6.48e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 49        |\n",
            "|    time_elapsed    | 3087      |\n",
            "|    total_timesteps | 100352    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 50           |\n",
            "|    time_elapsed         | 3129         |\n",
            "|    total_timesteps      | 102400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045063403 |\n",
            "|    clip_fraction        | 0.0389       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.81        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.43e+05     |\n",
            "|    n_updates            | 490          |\n",
            "|    policy_gradient_loss | -0.00261     |\n",
            "|    std                  | 0.861        |\n",
            "|    value_loss           | 6.44e+05     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 3169        |\n",
            "|    total_timesteps      | 104448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005472139 |\n",
            "|    clip_fraction        | 0.0275      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.8        |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.06e+05    |\n",
            "|    n_updates            | 500         |\n",
            "|    policy_gradient_loss | -0.000807   |\n",
            "|    std                  | 0.86        |\n",
            "|    value_loss           | 6.4e+05     |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=105000, episode_reward=-50213.64 +/- 9.38\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 105000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048526945 |\n",
            "|    clip_fraction        | 0.0273       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.78        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.12e+05     |\n",
            "|    n_updates            | 510          |\n",
            "|    policy_gradient_loss | -0.000766    |\n",
            "|    std                  | 0.854        |\n",
            "|    value_loss           | 6.37e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 52        |\n",
            "|    time_elapsed    | 3265      |\n",
            "|    total_timesteps | 106496    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 53          |\n",
            "|    time_elapsed         | 3305        |\n",
            "|    total_timesteps      | 108544      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002996068 |\n",
            "|    clip_fraction        | 0.0253      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.74       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.63e+05    |\n",
            "|    n_updates            | 520         |\n",
            "|    policy_gradient_loss | -0.000826   |\n",
            "|    std                  | 0.839       |\n",
            "|    value_loss           | 6.34e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=110000, episode_reward=-50206.50 +/- 10.93\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 110000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005035983 |\n",
            "|    clip_fraction        | 0.0235      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.71       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.11e+05    |\n",
            "|    n_updates            | 530         |\n",
            "|    policy_gradient_loss | -0.000502   |\n",
            "|    std                  | 0.833       |\n",
            "|    value_loss           | 6.3e+05     |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 54        |\n",
            "|    time_elapsed    | 3401      |\n",
            "|    total_timesteps | 110592    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 55          |\n",
            "|    time_elapsed         | 3442        |\n",
            "|    total_timesteps      | 112640      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003454103 |\n",
            "|    clip_fraction        | 0.0312      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.71       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.65e+05    |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | -0.00137    |\n",
            "|    std                  | 0.838       |\n",
            "|    value_loss           | 6.27e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 56           |\n",
            "|    time_elapsed         | 3482         |\n",
            "|    total_timesteps      | 114688       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036001643 |\n",
            "|    clip_fraction        | 0.0241       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.73        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.83e+05     |\n",
            "|    n_updates            | 550          |\n",
            "|    policy_gradient_loss | -0.00075     |\n",
            "|    std                  | 0.845        |\n",
            "|    value_loss           | 6.24e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=115000, episode_reward=-50205.07 +/- 15.51\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 115000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030244382 |\n",
            "|    clip_fraction        | 0.00786      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.75        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.98e+05     |\n",
            "|    n_updates            | 560          |\n",
            "|    policy_gradient_loss | 0.000711     |\n",
            "|    std                  | 0.847        |\n",
            "|    value_loss           | 6.21e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 57        |\n",
            "|    time_elapsed    | 3578      |\n",
            "|    total_timesteps | 116736    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 58           |\n",
            "|    time_elapsed         | 3619         |\n",
            "|    total_timesteps      | 118784       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053925673 |\n",
            "|    clip_fraction        | 0.0393       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.76        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.33e+05     |\n",
            "|    n_updates            | 570          |\n",
            "|    policy_gradient_loss | -0.00162     |\n",
            "|    std                  | 0.851        |\n",
            "|    value_loss           | 6.19e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=-50202.83 +/- 7.01\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 120000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002308815 |\n",
            "|    clip_fraction        | 0.0237      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.77       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.68e+05    |\n",
            "|    n_updates            | 580         |\n",
            "|    policy_gradient_loss | -0.00135    |\n",
            "|    std                  | 0.853       |\n",
            "|    value_loss           | 6.17e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 59        |\n",
            "|    time_elapsed    | 3713      |\n",
            "|    total_timesteps | 120832    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 60           |\n",
            "|    time_elapsed         | 3754         |\n",
            "|    total_timesteps      | 122880       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040922947 |\n",
            "|    clip_fraction        | 0.034        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.77        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.24e+05     |\n",
            "|    n_updates            | 590          |\n",
            "|    policy_gradient_loss | -0.00109     |\n",
            "|    std                  | 0.854        |\n",
            "|    value_loss           | 6.15e+05     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 61           |\n",
            "|    time_elapsed         | 3795         |\n",
            "|    total_timesteps      | 124928       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041097053 |\n",
            "|    clip_fraction        | 0.0258       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.75        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.64e+05     |\n",
            "|    n_updates            | 600          |\n",
            "|    policy_gradient_loss | -0.000926    |\n",
            "|    std                  | 0.841        |\n",
            "|    value_loss           | 6.13e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=125000, episode_reward=-50208.80 +/- 10.53\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 125000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067175375 |\n",
            "|    clip_fraction        | 0.0445       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.7         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.05e+05     |\n",
            "|    n_updates            | 610          |\n",
            "|    policy_gradient_loss | -0.00283     |\n",
            "|    std                  | 0.828        |\n",
            "|    value_loss           | 6.11e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 62        |\n",
            "|    time_elapsed    | 3892      |\n",
            "|    total_timesteps | 126976    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 63           |\n",
            "|    time_elapsed         | 3933         |\n",
            "|    total_timesteps      | 129024       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044888062 |\n",
            "|    clip_fraction        | 0.0318       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.68        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.33e+05     |\n",
            "|    n_updates            | 620          |\n",
            "|    policy_gradient_loss | -0.00153     |\n",
            "|    std                  | 0.828        |\n",
            "|    value_loss           | 6.09e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=130000, episode_reward=-50206.43 +/- 14.43\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 130000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038238412 |\n",
            "|    clip_fraction        | 0.0257       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.68        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.45e+05     |\n",
            "|    n_updates            | 630          |\n",
            "|    policy_gradient_loss | -0.000609    |\n",
            "|    std                  | 0.829        |\n",
            "|    value_loss           | 5.89e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 64        |\n",
            "|    time_elapsed    | 4027      |\n",
            "|    total_timesteps | 131072    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 65          |\n",
            "|    time_elapsed         | 4068        |\n",
            "|    total_timesteps      | 133120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004211833 |\n",
            "|    clip_fraction        | 0.0498      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.67       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.44e+05    |\n",
            "|    n_updates            | 640         |\n",
            "|    policy_gradient_loss | -0.00225    |\n",
            "|    std                  | 0.827       |\n",
            "|    value_loss           | 5.79e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=135000, episode_reward=-50210.10 +/- 10.84\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 135000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005115783 |\n",
            "|    clip_fraction        | 0.039       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.65       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.57e+05    |\n",
            "|    n_updates            | 650         |\n",
            "|    policy_gradient_loss | -0.00212    |\n",
            "|    std                  | 0.819       |\n",
            "|    value_loss           | 5.75e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 66        |\n",
            "|    time_elapsed    | 4162      |\n",
            "|    total_timesteps | 135168    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 67           |\n",
            "|    time_elapsed         | 4203         |\n",
            "|    total_timesteps      | 137216       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046936455 |\n",
            "|    clip_fraction        | 0.043        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.63        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.87e+05     |\n",
            "|    n_updates            | 660          |\n",
            "|    policy_gradient_loss | -0.0019      |\n",
            "|    std                  | 0.819        |\n",
            "|    value_loss           | 5.7e+05      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 68           |\n",
            "|    time_elapsed         | 4243         |\n",
            "|    total_timesteps      | 139264       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064134086 |\n",
            "|    clip_fraction        | 0.0572       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.62        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.51e+05     |\n",
            "|    n_updates            | 670          |\n",
            "|    policy_gradient_loss | -0.00337     |\n",
            "|    std                  | 0.815        |\n",
            "|    value_loss           | 5.66e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=140000, episode_reward=-50202.34 +/- 9.14\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 140000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062732855 |\n",
            "|    clip_fraction        | 0.0502       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.6         |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.03e+05     |\n",
            "|    n_updates            | 680          |\n",
            "|    policy_gradient_loss | -0.00285     |\n",
            "|    std                  | 0.805        |\n",
            "|    value_loss           | 5.62e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 69        |\n",
            "|    time_elapsed    | 4338      |\n",
            "|    total_timesteps | 141312    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 70           |\n",
            "|    time_elapsed         | 4378         |\n",
            "|    total_timesteps      | 143360       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039853584 |\n",
            "|    clip_fraction        | 0.0189       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.61        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.87e+05     |\n",
            "|    n_updates            | 690          |\n",
            "|    policy_gradient_loss | -2.39e-06    |\n",
            "|    std                  | 0.819        |\n",
            "|    value_loss           | 5.59e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=145000, episode_reward=-50197.13 +/- 12.35\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 145000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00492079 |\n",
            "|    clip_fraction        | 0.0525     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.64      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 2.84e+05   |\n",
            "|    n_updates            | 700        |\n",
            "|    policy_gradient_loss | -0.00267   |\n",
            "|    std                  | 0.822      |\n",
            "|    value_loss           | 5.55e+05   |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 71        |\n",
            "|    time_elapsed    | 4474      |\n",
            "|    total_timesteps | 145408    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 72           |\n",
            "|    time_elapsed         | 4516         |\n",
            "|    total_timesteps      | 147456       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058230264 |\n",
            "|    clip_fraction        | 0.0516       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.63        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2e+05        |\n",
            "|    n_updates            | 710          |\n",
            "|    policy_gradient_loss | -0.00205     |\n",
            "|    std                  | 0.817        |\n",
            "|    value_loss           | 5.52e+05     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 73          |\n",
            "|    time_elapsed         | 4556        |\n",
            "|    total_timesteps      | 149504      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004325344 |\n",
            "|    clip_fraction        | 0.0279      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.6        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.95e+05    |\n",
            "|    n_updates            | 720         |\n",
            "|    policy_gradient_loss | -0.000215   |\n",
            "|    std                  | 0.809       |\n",
            "|    value_loss           | 5.49e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=150000, episode_reward=-50204.92 +/- 14.21\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 150000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004037902 |\n",
            "|    clip_fraction        | 0.0279      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.56       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.61e+05    |\n",
            "|    n_updates            | 730         |\n",
            "|    policy_gradient_loss | -0.000934   |\n",
            "|    std                  | 0.796       |\n",
            "|    value_loss           | 5.46e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 74        |\n",
            "|    time_elapsed    | 4652      |\n",
            "|    total_timesteps | 151552    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 75           |\n",
            "|    time_elapsed         | 4694         |\n",
            "|    total_timesteps      | 153600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036211885 |\n",
            "|    clip_fraction        | 0.0215       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.52        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.76e+05     |\n",
            "|    n_updates            | 740          |\n",
            "|    policy_gradient_loss | -0.000291    |\n",
            "|    std                  | 0.786        |\n",
            "|    value_loss           | 5.43e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=155000, episode_reward=-50212.14 +/- 7.68\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 155000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042840363 |\n",
            "|    clip_fraction        | 0.0366       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.49        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.06e+05     |\n",
            "|    n_updates            | 750          |\n",
            "|    policy_gradient_loss | -0.00175     |\n",
            "|    std                  | 0.779        |\n",
            "|    value_loss           | 5.4e+05      |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 76        |\n",
            "|    time_elapsed    | 4789      |\n",
            "|    total_timesteps | 155648    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 77          |\n",
            "|    time_elapsed         | 4830        |\n",
            "|    total_timesteps      | 157696      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007373413 |\n",
            "|    clip_fraction        | 0.0507      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.43       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.68e+05    |\n",
            "|    n_updates            | 760         |\n",
            "|    policy_gradient_loss | -0.00387    |\n",
            "|    std                  | 0.76        |\n",
            "|    value_loss           | 5.37e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 78           |\n",
            "|    time_elapsed         | 4871         |\n",
            "|    total_timesteps      | 159744       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037197233 |\n",
            "|    clip_fraction        | 0.0313       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.38e+05     |\n",
            "|    n_updates            | 770          |\n",
            "|    policy_gradient_loss | -0.000328    |\n",
            "|    std                  | 0.763        |\n",
            "|    value_loss           | 5.35e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=-50210.45 +/- 3.87\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 160000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038356609 |\n",
            "|    clip_fraction        | 0.0261       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.43        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3e+05        |\n",
            "|    n_updates            | 780          |\n",
            "|    policy_gradient_loss | -0.000609    |\n",
            "|    std                  | 0.769        |\n",
            "|    value_loss           | 5.33e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 79        |\n",
            "|    time_elapsed    | 4967      |\n",
            "|    total_timesteps | 161792    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 80           |\n",
            "|    time_elapsed         | 5008         |\n",
            "|    total_timesteps      | 163840       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0059884824 |\n",
            "|    clip_fraction        | 0.0423       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.45        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.36e+05     |\n",
            "|    n_updates            | 790          |\n",
            "|    policy_gradient_loss | -0.00116     |\n",
            "|    std                  | 0.776        |\n",
            "|    value_loss           | 5.31e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=165000, episode_reward=-50202.98 +/- 12.73\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 165000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0072309296 |\n",
            "|    clip_fraction        | 0.0542       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.47        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.1e+05      |\n",
            "|    n_updates            | 800          |\n",
            "|    policy_gradient_loss | -0.00265     |\n",
            "|    std                  | 0.78         |\n",
            "|    value_loss           | 5.29e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 81        |\n",
            "|    time_elapsed    | 5104      |\n",
            "|    total_timesteps | 165888    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 82           |\n",
            "|    time_elapsed         | 5145         |\n",
            "|    total_timesteps      | 167936       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030593337 |\n",
            "|    clip_fraction        | 0.023        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.46        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.52e+05     |\n",
            "|    n_updates            | 810          |\n",
            "|    policy_gradient_loss | -0.00145     |\n",
            "|    std                  | 0.776        |\n",
            "|    value_loss           | 5.28e+05     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 83           |\n",
            "|    time_elapsed         | 5186         |\n",
            "|    total_timesteps      | 169984       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025094047 |\n",
            "|    clip_fraction        | 0.0341       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.45        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.87e+05     |\n",
            "|    n_updates            | 820          |\n",
            "|    policy_gradient_loss | -0.00153     |\n",
            "|    std                  | 0.778        |\n",
            "|    value_loss           | 5.26e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=170000, episode_reward=-50206.76 +/- 3.60\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 170000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038181609 |\n",
            "|    clip_fraction        | 0.0444       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.45        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.98e+05     |\n",
            "|    n_updates            | 830          |\n",
            "|    policy_gradient_loss | -0.00199     |\n",
            "|    std                  | 0.778        |\n",
            "|    value_loss           | 5.25e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 84        |\n",
            "|    time_elapsed    | 5282      |\n",
            "|    total_timesteps | 172032    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 85          |\n",
            "|    time_elapsed         | 5323        |\n",
            "|    total_timesteps      | 174080      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002391261 |\n",
            "|    clip_fraction        | 0.0257      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.43       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.16e+05    |\n",
            "|    n_updates            | 840         |\n",
            "|    policy_gradient_loss | -0.00132    |\n",
            "|    std                  | 0.765       |\n",
            "|    value_loss           | 5.04e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=175000, episode_reward=-50208.03 +/- 13.20\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 175000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005420194 |\n",
            "|    clip_fraction        | 0.0522      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.37       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.33e+05    |\n",
            "|    n_updates            | 850         |\n",
            "|    policy_gradient_loss | -0.00341    |\n",
            "|    std                  | 0.754       |\n",
            "|    value_loss           | 4.97e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 86        |\n",
            "|    time_elapsed    | 5418      |\n",
            "|    total_timesteps | 176128    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 87           |\n",
            "|    time_elapsed         | 5459         |\n",
            "|    total_timesteps      | 178176       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019267958 |\n",
            "|    clip_fraction        | 0.0127       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.34        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.5e+05      |\n",
            "|    n_updates            | 860          |\n",
            "|    policy_gradient_loss | 0.00107      |\n",
            "|    std                  | 0.751        |\n",
            "|    value_loss           | 4.93e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=180000, episode_reward=-50202.70 +/- 12.62\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 180000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006234116 |\n",
            "|    clip_fraction        | 0.0513      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.35       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.17e+05    |\n",
            "|    n_updates            | 870         |\n",
            "|    policy_gradient_loss | -0.00291    |\n",
            "|    std                  | 0.754       |\n",
            "|    value_loss           | 4.9e+05     |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 88        |\n",
            "|    time_elapsed    | 5554      |\n",
            "|    total_timesteps | 180224    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 89           |\n",
            "|    time_elapsed         | 5596         |\n",
            "|    total_timesteps      | 182272       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046365885 |\n",
            "|    clip_fraction        | 0.034        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.34        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.99e+05     |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | -0.00156     |\n",
            "|    std                  | 0.744        |\n",
            "|    value_loss           | 4.86e+05     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 90           |\n",
            "|    time_elapsed         | 5637         |\n",
            "|    total_timesteps      | 184320       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0061358544 |\n",
            "|    clip_fraction        | 0.0399       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.28        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.11e+05     |\n",
            "|    n_updates            | 890          |\n",
            "|    policy_gradient_loss | -0.00222     |\n",
            "|    std                  | 0.726        |\n",
            "|    value_loss           | 4.82e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=185000, episode_reward=-50204.68 +/- 12.87\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 185000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004505831 |\n",
            "|    clip_fraction        | 0.0268      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.26       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.38e+05    |\n",
            "|    n_updates            | 900         |\n",
            "|    policy_gradient_loss | -0.00142    |\n",
            "|    std                  | 0.728       |\n",
            "|    value_loss           | 4.79e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 91        |\n",
            "|    time_elapsed    | 5733      |\n",
            "|    total_timesteps | 186368    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 92          |\n",
            "|    time_elapsed         | 5773        |\n",
            "|    total_timesteps      | 188416      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004065321 |\n",
            "|    clip_fraction        | 0.031       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.29       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.32e+05    |\n",
            "|    n_updates            | 910         |\n",
            "|    policy_gradient_loss | -0.000782   |\n",
            "|    std                  | 0.738       |\n",
            "|    value_loss           | 4.76e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=190000, episode_reward=-50212.09 +/- 10.95\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 190000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004944363 |\n",
            "|    clip_fraction        | 0.0296      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.29       |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.08e+05    |\n",
            "|    n_updates            | 920         |\n",
            "|    policy_gradient_loss | -4.08e-05   |\n",
            "|    std                  | 0.731       |\n",
            "|    value_loss           | 4.74e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 93        |\n",
            "|    time_elapsed    | 5868      |\n",
            "|    total_timesteps | 190464    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 94          |\n",
            "|    time_elapsed         | 5910        |\n",
            "|    total_timesteps      | 192512      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004985301 |\n",
            "|    clip_fraction        | 0.0343      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.27       |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.13e+05    |\n",
            "|    n_updates            | 930         |\n",
            "|    policy_gradient_loss | -0.00197    |\n",
            "|    std                  | 0.728       |\n",
            "|    value_loss           | 4.71e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 95           |\n",
            "|    time_elapsed         | 5951         |\n",
            "|    total_timesteps      | 194560       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041493555 |\n",
            "|    clip_fraction        | 0.0483       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.26        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.03e+05     |\n",
            "|    n_updates            | 940          |\n",
            "|    policy_gradient_loss | -0.00294     |\n",
            "|    std                  | 0.729        |\n",
            "|    value_loss           | 4.68e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=195000, episode_reward=-50204.81 +/- 7.82\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 195000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063999826 |\n",
            "|    clip_fraction        | 0.0325       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.27        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.87e+05     |\n",
            "|    n_updates            | 950          |\n",
            "|    policy_gradient_loss | -0.000861    |\n",
            "|    std                  | 0.729        |\n",
            "|    value_loss           | 4.65e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 96        |\n",
            "|    time_elapsed    | 6047      |\n",
            "|    total_timesteps | 196608    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 97           |\n",
            "|    time_elapsed         | 6089         |\n",
            "|    total_timesteps      | 198656       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0055314964 |\n",
            "|    clip_fraction        | 0.0296       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.25        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.36e+05     |\n",
            "|    n_updates            | 960          |\n",
            "|    policy_gradient_loss | -0.000739    |\n",
            "|    std                  | 0.72         |\n",
            "|    value_loss           | 4.63e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=200000, episode_reward=-50208.52 +/- 11.90\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 200000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043704547 |\n",
            "|    clip_fraction        | 0.0265       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.82e+05     |\n",
            "|    n_updates            | 970          |\n",
            "|    policy_gradient_loss | -0.000725    |\n",
            "|    std                  | 0.715        |\n",
            "|    value_loss           | 4.61e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 98        |\n",
            "|    time_elapsed    | 6185      |\n",
            "|    total_timesteps | 200704    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 99          |\n",
            "|    time_elapsed         | 6227        |\n",
            "|    total_timesteps      | 202752      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005623165 |\n",
            "|    clip_fraction        | 0.0413      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.21       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.71e+05    |\n",
            "|    n_updates            | 980         |\n",
            "|    policy_gradient_loss | -0.00182    |\n",
            "|    std                  | 0.717       |\n",
            "|    value_loss           | 4.59e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 100          |\n",
            "|    time_elapsed         | 6268         |\n",
            "|    total_timesteps      | 204800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062211957 |\n",
            "|    clip_fraction        | 0.0485       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.2         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.38e+05     |\n",
            "|    n_updates            | 990          |\n",
            "|    policy_gradient_loss | -0.00175     |\n",
            "|    std                  | 0.712        |\n",
            "|    value_loss           | 4.57e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=205000, episode_reward=-50203.08 +/- 12.47\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 205000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036938689 |\n",
            "|    clip_fraction        | 0.0309       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.24        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.17e+05     |\n",
            "|    n_updates            | 1000         |\n",
            "|    policy_gradient_loss | -0.00132     |\n",
            "|    std                  | 0.728        |\n",
            "|    value_loss           | 4.56e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 101       |\n",
            "|    time_elapsed    | 6366      |\n",
            "|    total_timesteps | 206848    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 102         |\n",
            "|    time_elapsed         | 6408        |\n",
            "|    total_timesteps      | 208896      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006268311 |\n",
            "|    clip_fraction        | 0.042       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.23       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.37e+05    |\n",
            "|    n_updates            | 1010        |\n",
            "|    policy_gradient_loss | -0.00117    |\n",
            "|    std                  | 0.718       |\n",
            "|    value_loss           | 4.54e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=210000, episode_reward=-50197.36 +/- 13.70\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 210000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0059842886 |\n",
            "|    clip_fraction        | 0.0589       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.23        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.33e+05     |\n",
            "|    n_updates            | 1020         |\n",
            "|    policy_gradient_loss | -0.00356     |\n",
            "|    std                  | 0.724        |\n",
            "|    value_loss           | 4.53e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 103       |\n",
            "|    time_elapsed    | 6504      |\n",
            "|    total_timesteps | 210944    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 104          |\n",
            "|    time_elapsed         | 6546         |\n",
            "|    total_timesteps      | 212992       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039617494 |\n",
            "|    clip_fraction        | 0.0314       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.24        |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.65e+05     |\n",
            "|    n_updates            | 1030         |\n",
            "|    policy_gradient_loss | -0.000802    |\n",
            "|    std                  | 0.723        |\n",
            "|    value_loss           | 4.52e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=215000, episode_reward=-50191.65 +/- 17.86\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 215000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0055840327 |\n",
            "|    clip_fraction        | 0.0338       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.24        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.6e+05      |\n",
            "|    n_updates            | 1040         |\n",
            "|    policy_gradient_loss | 1.69e-05     |\n",
            "|    std                  | 0.721        |\n",
            "|    value_loss           | 4.52e+05     |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 105       |\n",
            "|    time_elapsed    | 6643      |\n",
            "|    total_timesteps | 215040    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 106         |\n",
            "|    time_elapsed         | 6684        |\n",
            "|    total_timesteps      | 217088      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009357236 |\n",
            "|    clip_fraction        | 0.129       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.23       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.79e+05    |\n",
            "|    n_updates            | 1050        |\n",
            "|    policy_gradient_loss | 0.00421     |\n",
            "|    std                  | 0.716       |\n",
            "|    value_loss           | 4.27e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 107          |\n",
            "|    time_elapsed         | 6727         |\n",
            "|    total_timesteps      | 219136       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0066315723 |\n",
            "|    clip_fraction        | 0.0929       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -3.21        |\n",
            "|    explained_variance   | 0.0869       |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.96e+05     |\n",
            "|    n_updates            | 1060         |\n",
            "|    policy_gradient_loss | 4.16e-05     |\n",
            "|    std                  | 0.711        |\n",
            "|    value_loss           | 4.23e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=220000, episode_reward=-50210.84 +/- 12.26\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 220000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00854824 |\n",
            "|    clip_fraction        | 0.0917     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.14      |\n",
            "|    explained_variance   | 0.122      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.48e+05   |\n",
            "|    n_updates            | 1070       |\n",
            "|    policy_gradient_loss | -0.00128   |\n",
            "|    std                  | 0.69       |\n",
            "|    value_loss           | 3.43e+05   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 108       |\n",
            "|    time_elapsed    | 6824      |\n",
            "|    total_timesteps | 221184    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 109         |\n",
            "|    time_elapsed         | 6866        |\n",
            "|    total_timesteps      | 223232      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005092926 |\n",
            "|    clip_fraction        | 0.0465      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.08       |\n",
            "|    explained_variance   | 0.155       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.01e+05    |\n",
            "|    n_updates            | 1080        |\n",
            "|    policy_gradient_loss | 0.00107     |\n",
            "|    std                  | 0.677       |\n",
            "|    value_loss           | 3.19e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=225000, episode_reward=-50206.28 +/- 7.26\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 225000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004815542 |\n",
            "|    clip_fraction        | 0.0631      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.02       |\n",
            "|    explained_variance   | 0.214       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.88e+05    |\n",
            "|    n_updates            | 1090        |\n",
            "|    policy_gradient_loss | -0.00136    |\n",
            "|    std                  | 0.665       |\n",
            "|    value_loss           | 3.86e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 110       |\n",
            "|    time_elapsed    | 6962      |\n",
            "|    total_timesteps | 225280    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 32         |\n",
            "|    iterations           | 111        |\n",
            "|    time_elapsed         | 7005       |\n",
            "|    total_timesteps      | 227328     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00350831 |\n",
            "|    clip_fraction        | 0.0292     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.96      |\n",
            "|    explained_variance   | 0.284      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.69e+05   |\n",
            "|    n_updates            | 1100       |\n",
            "|    policy_gradient_loss | -0.000766  |\n",
            "|    std                  | 0.652      |\n",
            "|    value_loss           | 4.08e+05   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 112          |\n",
            "|    time_elapsed         | 7053         |\n",
            "|    total_timesteps      | 229376       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050846767 |\n",
            "|    clip_fraction        | 0.0594       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.94        |\n",
            "|    explained_variance   | 0.315        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.09e+05     |\n",
            "|    n_updates            | 1110         |\n",
            "|    policy_gradient_loss | -0.00331     |\n",
            "|    std                  | 0.652        |\n",
            "|    value_loss           | 4.05e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=230000, episode_reward=-50211.89 +/- 9.80\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 230000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0057523362 |\n",
            "|    clip_fraction        | 0.0474       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.93        |\n",
            "|    explained_variance   | 0.351        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.66e+05     |\n",
            "|    n_updates            | 1120         |\n",
            "|    policy_gradient_loss | 0.0012       |\n",
            "|    std                  | 0.648        |\n",
            "|    value_loss           | 4.03e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 113       |\n",
            "|    time_elapsed    | 7148      |\n",
            "|    total_timesteps | 231424    |\n",
            "----------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1e+03     |\n",
            "|    ep_rew_mean          | -5.02e+04 |\n",
            "| time/                   |           |\n",
            "|    fps                  | 32        |\n",
            "|    iterations           | 114       |\n",
            "|    time_elapsed         | 7190      |\n",
            "|    total_timesteps      | 233472    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0065782 |\n",
            "|    clip_fraction        | 0.0634    |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.93     |\n",
            "|    explained_variance   | 0.362     |\n",
            "|    learning_rate        | 0.0005    |\n",
            "|    loss                 | 1.91e+05  |\n",
            "|    n_updates            | 1130      |\n",
            "|    policy_gradient_loss | -0.00107  |\n",
            "|    std                  | 0.649     |\n",
            "|    value_loss           | 4e+05     |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=235000, episode_reward=-50200.80 +/- 10.63\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 235000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062017827 |\n",
            "|    clip_fraction        | 0.0679       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.9         |\n",
            "|    explained_variance   | 0.379        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.79e+05     |\n",
            "|    n_updates            | 1140         |\n",
            "|    policy_gradient_loss | -0.00237     |\n",
            "|    std                  | 0.637        |\n",
            "|    value_loss           | 3.97e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 115       |\n",
            "|    time_elapsed    | 7286      |\n",
            "|    total_timesteps | 235520    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 116         |\n",
            "|    time_elapsed         | 7329        |\n",
            "|    total_timesteps      | 237568      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004602245 |\n",
            "|    clip_fraction        | 0.0532      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.85       |\n",
            "|    explained_variance   | 0.368       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.8e+05     |\n",
            "|    n_updates            | 1150        |\n",
            "|    policy_gradient_loss | -0.000686   |\n",
            "|    std                  | 0.626       |\n",
            "|    value_loss           | 3.95e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 117          |\n",
            "|    time_elapsed         | 7371         |\n",
            "|    total_timesteps      | 239616       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051150788 |\n",
            "|    clip_fraction        | 0.0588       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.82        |\n",
            "|    explained_variance   | 0.369        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.77e+05     |\n",
            "|    n_updates            | 1160         |\n",
            "|    policy_gradient_loss | 0.000324     |\n",
            "|    std                  | 0.623        |\n",
            "|    value_loss           | 3.92e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=240000, episode_reward=-50202.83 +/- 12.82\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 240000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00984245 |\n",
            "|    clip_fraction        | 0.0688     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.8       |\n",
            "|    explained_variance   | 0.38       |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.94e+05   |\n",
            "|    n_updates            | 1170       |\n",
            "|    policy_gradient_loss | -0.00157   |\n",
            "|    std                  | 0.618      |\n",
            "|    value_loss           | 3.9e+05    |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 118       |\n",
            "|    time_elapsed    | 7467      |\n",
            "|    total_timesteps | 241664    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 32         |\n",
            "|    iterations           | 119        |\n",
            "|    time_elapsed         | 7508       |\n",
            "|    total_timesteps      | 243712     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00559403 |\n",
            "|    clip_fraction        | 0.0639     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.81      |\n",
            "|    explained_variance   | 0.397      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.68e+05   |\n",
            "|    n_updates            | 1180       |\n",
            "|    policy_gradient_loss | -0.00171   |\n",
            "|    std                  | 0.621      |\n",
            "|    value_loss           | 3.88e+05   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=245000, episode_reward=-50200.85 +/- 7.07\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 245000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004331373 |\n",
            "|    clip_fraction        | 0.0687      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.8        |\n",
            "|    explained_variance   | 0.423       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.33e+05    |\n",
            "|    n_updates            | 1190        |\n",
            "|    policy_gradient_loss | -0.00252    |\n",
            "|    std                  | 0.615       |\n",
            "|    value_loss           | 3.87e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 120       |\n",
            "|    time_elapsed    | 7605      |\n",
            "|    total_timesteps | 245760    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 121         |\n",
            "|    time_elapsed         | 7646        |\n",
            "|    total_timesteps      | 247808      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004705649 |\n",
            "|    clip_fraction        | 0.0542      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.8        |\n",
            "|    explained_variance   | 0.394       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.8e+05     |\n",
            "|    n_updates            | 1200        |\n",
            "|    policy_gradient_loss | 1.54e-06    |\n",
            "|    std                  | 0.62        |\n",
            "|    value_loss           | 3.85e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 122         |\n",
            "|    time_elapsed         | 7688        |\n",
            "|    total_timesteps      | 249856      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010615952 |\n",
            "|    clip_fraction        | 0.106       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.82       |\n",
            "|    explained_variance   | 0.497       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.86e+05    |\n",
            "|    n_updates            | 1210        |\n",
            "|    policy_gradient_loss | 0.00243     |\n",
            "|    std                  | 0.623       |\n",
            "|    value_loss           | 3.85e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=250000, episode_reward=-50208.11 +/- 8.10\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 250000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00461411 |\n",
            "|    clip_fraction        | 0.033      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.82      |\n",
            "|    explained_variance   | 0.521      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.8e+05    |\n",
            "|    n_updates            | 1220       |\n",
            "|    policy_gradient_loss | -0.00102   |\n",
            "|    std                  | 0.623      |\n",
            "|    value_loss           | 3.85e+05   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 123       |\n",
            "|    time_elapsed    | 7783      |\n",
            "|    total_timesteps | 251904    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 124          |\n",
            "|    time_elapsed         | 7824         |\n",
            "|    total_timesteps      | 253952       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043495195 |\n",
            "|    clip_fraction        | 0.0311       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.83        |\n",
            "|    explained_variance   | 0.507        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.16e+05     |\n",
            "|    n_updates            | 1230         |\n",
            "|    policy_gradient_loss | 0.00179      |\n",
            "|    std                  | 0.626        |\n",
            "|    value_loss           | 3.84e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=255000, episode_reward=-50208.44 +/- 11.68\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 255000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0075945864 |\n",
            "|    clip_fraction        | 0.069        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.83        |\n",
            "|    explained_variance   | 0.539        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.47e+05     |\n",
            "|    n_updates            | 1240         |\n",
            "|    policy_gradient_loss | -0.00342     |\n",
            "|    std                  | 0.625        |\n",
            "|    value_loss           | 3.81e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 125       |\n",
            "|    time_elapsed    | 7920      |\n",
            "|    total_timesteps | 256000    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 32         |\n",
            "|    iterations           | 126        |\n",
            "|    time_elapsed         | 7961       |\n",
            "|    total_timesteps      | 258048     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00498654 |\n",
            "|    clip_fraction        | 0.0262     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.83      |\n",
            "|    explained_variance   | 0.58       |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.92e+05   |\n",
            "|    n_updates            | 1250       |\n",
            "|    policy_gradient_loss | -0.00111   |\n",
            "|    std                  | 0.625      |\n",
            "|    value_loss           | 3.84e+05   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=260000, episode_reward=-50210.33 +/- 12.09\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 260000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005540885 |\n",
            "|    clip_fraction        | 0.0413      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.85       |\n",
            "|    explained_variance   | 0.65        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.15e+05    |\n",
            "|    n_updates            | 1260        |\n",
            "|    policy_gradient_loss | -0.00251    |\n",
            "|    std                  | 0.631       |\n",
            "|    value_loss           | 3.62e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 127       |\n",
            "|    time_elapsed    | 8058      |\n",
            "|    total_timesteps | 260096    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 128         |\n",
            "|    time_elapsed         | 8099        |\n",
            "|    total_timesteps      | 262144      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003160643 |\n",
            "|    clip_fraction        | 0.0383      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.86       |\n",
            "|    explained_variance   | 0.627       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.51e+05    |\n",
            "|    n_updates            | 1270        |\n",
            "|    policy_gradient_loss | -0.00254    |\n",
            "|    std                  | 0.633       |\n",
            "|    value_loss           | 3.6e+05     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 32         |\n",
            "|    iterations           | 129        |\n",
            "|    time_elapsed         | 8141       |\n",
            "|    total_timesteps      | 264192     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00849997 |\n",
            "|    clip_fraction        | 0.0757     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.87      |\n",
            "|    explained_variance   | 0.607      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.2e+05    |\n",
            "|    n_updates            | 1280       |\n",
            "|    policy_gradient_loss | -0.00625   |\n",
            "|    std                  | 0.633      |\n",
            "|    value_loss           | 3.59e+05   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=265000, episode_reward=-50198.87 +/- 11.63\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 265000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060526864 |\n",
            "|    clip_fraction        | 0.0421       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.84        |\n",
            "|    explained_variance   | 0.636        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.43e+05     |\n",
            "|    n_updates            | 1290         |\n",
            "|    policy_gradient_loss | -0.00207     |\n",
            "|    std                  | 0.625        |\n",
            "|    value_loss           | 3.54e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 130       |\n",
            "|    time_elapsed    | 8235      |\n",
            "|    total_timesteps | 266240    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 131         |\n",
            "|    time_elapsed         | 8276        |\n",
            "|    total_timesteps      | 268288      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007934116 |\n",
            "|    clip_fraction        | 0.089       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.81       |\n",
            "|    explained_variance   | 0.633       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.5e+05     |\n",
            "|    n_updates            | 1300        |\n",
            "|    policy_gradient_loss | -0.00478    |\n",
            "|    std                  | 0.621       |\n",
            "|    value_loss           | 3.52e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=270000, episode_reward=-50208.77 +/- 6.04\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 270000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004657656 |\n",
            "|    clip_fraction        | 0.0355      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.77       |\n",
            "|    explained_variance   | 0.621       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.22e+05    |\n",
            "|    n_updates            | 1310        |\n",
            "|    policy_gradient_loss | -0.00146    |\n",
            "|    std                  | 0.608       |\n",
            "|    value_loss           | 3.49e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 132       |\n",
            "|    time_elapsed    | 8373      |\n",
            "|    total_timesteps | 270336    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 133          |\n",
            "|    time_elapsed         | 8414         |\n",
            "|    total_timesteps      | 272384       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041723573 |\n",
            "|    clip_fraction        | 0.0311       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.71        |\n",
            "|    explained_variance   | 0.655        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.63e+05     |\n",
            "|    n_updates            | 1320         |\n",
            "|    policy_gradient_loss | -0.000131    |\n",
            "|    std                  | 0.596        |\n",
            "|    value_loss           | 3.46e+05     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 134          |\n",
            "|    time_elapsed         | 8455         |\n",
            "|    total_timesteps      | 274432       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0059942165 |\n",
            "|    clip_fraction        | 0.0473       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.67        |\n",
            "|    explained_variance   | 0.666        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.3e+05      |\n",
            "|    n_updates            | 1330         |\n",
            "|    policy_gradient_loss | -0.00142     |\n",
            "|    std                  | 0.589        |\n",
            "|    value_loss           | 3.44e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=275000, episode_reward=-50206.31 +/- 9.07\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 275000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033388578 |\n",
            "|    clip_fraction        | 0.044        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.64        |\n",
            "|    explained_variance   | 0.671        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.57e+05     |\n",
            "|    n_updates            | 1340         |\n",
            "|    policy_gradient_loss | 0.000283     |\n",
            "|    std                  | 0.585        |\n",
            "|    value_loss           | 3.43e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 135       |\n",
            "|    time_elapsed    | 8552      |\n",
            "|    total_timesteps | 276480    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 136          |\n",
            "|    time_elapsed         | 8593         |\n",
            "|    total_timesteps      | 278528       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067918543 |\n",
            "|    clip_fraction        | 0.0701       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.63        |\n",
            "|    explained_variance   | 0.66         |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.21e+05     |\n",
            "|    n_updates            | 1350         |\n",
            "|    policy_gradient_loss | -0.0043      |\n",
            "|    std                  | 0.585        |\n",
            "|    value_loss           | 3.39e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=280000, episode_reward=-50201.32 +/- 12.71\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 280000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043280907 |\n",
            "|    clip_fraction        | 0.0501       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.62        |\n",
            "|    explained_variance   | 0.657        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.11e+05     |\n",
            "|    n_updates            | 1360         |\n",
            "|    policy_gradient_loss | -0.0005      |\n",
            "|    std                  | 0.581        |\n",
            "|    value_loss           | 3.36e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 137       |\n",
            "|    time_elapsed    | 8689      |\n",
            "|    total_timesteps | 280576    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 138          |\n",
            "|    time_elapsed         | 8730         |\n",
            "|    total_timesteps      | 282624       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053574406 |\n",
            "|    clip_fraction        | 0.0528       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.64        |\n",
            "|    explained_variance   | 0.675        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.42e+05     |\n",
            "|    n_updates            | 1370         |\n",
            "|    policy_gradient_loss | -0.00263     |\n",
            "|    std                  | 0.592        |\n",
            "|    value_loss           | 3.34e+05     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 139         |\n",
            "|    time_elapsed         | 8771        |\n",
            "|    total_timesteps      | 284672      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006001947 |\n",
            "|    clip_fraction        | 0.0572      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.65       |\n",
            "|    explained_variance   | 0.672       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.92e+05    |\n",
            "|    n_updates            | 1380        |\n",
            "|    policy_gradient_loss | -0.0026     |\n",
            "|    std                  | 0.588       |\n",
            "|    value_loss           | 3.32e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=285000, episode_reward=-50195.32 +/- 14.18\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 285000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005510697 |\n",
            "|    clip_fraction        | 0.0552      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.64       |\n",
            "|    explained_variance   | 0.683       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.37e+05    |\n",
            "|    n_updates            | 1390        |\n",
            "|    policy_gradient_loss | -0.00154    |\n",
            "|    std                  | 0.586       |\n",
            "|    value_loss           | 3.3e+05     |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 140       |\n",
            "|    time_elapsed    | 8867      |\n",
            "|    total_timesteps | 286720    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 141          |\n",
            "|    time_elapsed         | 8907         |\n",
            "|    total_timesteps      | 288768       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0066848043 |\n",
            "|    clip_fraction        | 0.0498       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.61        |\n",
            "|    explained_variance   | 0.692        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.86e+05     |\n",
            "|    n_updates            | 1400         |\n",
            "|    policy_gradient_loss | -0.000716    |\n",
            "|    std                  | 0.58         |\n",
            "|    value_loss           | 3.32e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=290000, episode_reward=-50215.80 +/- 7.03\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 290000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037071826 |\n",
            "|    clip_fraction        | 0.0473       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.6         |\n",
            "|    explained_variance   | 0.706        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.8e+05      |\n",
            "|    n_updates            | 1410         |\n",
            "|    policy_gradient_loss | 0.000687     |\n",
            "|    std                  | 0.581        |\n",
            "|    value_loss           | 3.3e+05      |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 142       |\n",
            "|    time_elapsed    | 9003      |\n",
            "|    total_timesteps | 290816    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 143         |\n",
            "|    time_elapsed         | 9045        |\n",
            "|    total_timesteps      | 292864      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004993193 |\n",
            "|    clip_fraction        | 0.0747      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.61       |\n",
            "|    explained_variance   | 0.684       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.71e+05    |\n",
            "|    n_updates            | 1420        |\n",
            "|    policy_gradient_loss | -0.00221    |\n",
            "|    std                  | 0.585       |\n",
            "|    value_loss           | 3.27e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 144          |\n",
            "|    time_elapsed         | 9086         |\n",
            "|    total_timesteps      | 294912       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067262743 |\n",
            "|    clip_fraction        | 0.0784       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.62        |\n",
            "|    explained_variance   | 0.686        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.35e+05     |\n",
            "|    n_updates            | 1430         |\n",
            "|    policy_gradient_loss | -0.00193     |\n",
            "|    std                  | 0.587        |\n",
            "|    value_loss           | 3.27e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=295000, episode_reward=-50203.18 +/- 13.70\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 295000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063389987 |\n",
            "|    clip_fraction        | 0.0853       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.62        |\n",
            "|    explained_variance   | 0.702        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.83e+05     |\n",
            "|    n_updates            | 1440         |\n",
            "|    policy_gradient_loss | -0.00103     |\n",
            "|    std                  | 0.585        |\n",
            "|    value_loss           | 3.26e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 145       |\n",
            "|    time_elapsed    | 9182      |\n",
            "|    total_timesteps | 296960    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 146          |\n",
            "|    time_elapsed         | 9224         |\n",
            "|    total_timesteps      | 299008       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063799047 |\n",
            "|    clip_fraction        | 0.0819       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.6         |\n",
            "|    explained_variance   | 0.7          |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.8e+05      |\n",
            "|    n_updates            | 1450         |\n",
            "|    policy_gradient_loss | -0.00313     |\n",
            "|    std                  | 0.58         |\n",
            "|    value_loss           | 3.25e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=300000, episode_reward=-50215.81 +/- 18.03\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 300000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012858777 |\n",
            "|    clip_fraction        | 0.221       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.61       |\n",
            "|    explained_variance   | 0.702       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.17e+05    |\n",
            "|    n_updates            | 1460        |\n",
            "|    policy_gradient_loss | 0.0117      |\n",
            "|    std                  | 0.586       |\n",
            "|    value_loss           | 2.7e+05     |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 147       |\n",
            "|    time_elapsed    | 9321      |\n",
            "|    total_timesteps | 301056    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 148         |\n",
            "|    time_elapsed         | 9362        |\n",
            "|    total_timesteps      | 303104      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006204268 |\n",
            "|    clip_fraction        | 0.0866      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.61       |\n",
            "|    explained_variance   | 0.732       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.9e+05     |\n",
            "|    n_updates            | 1470        |\n",
            "|    policy_gradient_loss | -0.00458    |\n",
            "|    std                  | 0.582       |\n",
            "|    value_loss           | 3.07e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=305000, episode_reward=-50210.57 +/- 9.47\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 305000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005102874 |\n",
            "|    clip_fraction        | 0.0428      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.63       |\n",
            "|    explained_variance   | 0.734       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.23e+05    |\n",
            "|    n_updates            | 1480        |\n",
            "|    policy_gradient_loss | -0.000196   |\n",
            "|    std                  | 0.589       |\n",
            "|    value_loss           | 3.04e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 149       |\n",
            "|    time_elapsed    | 9458      |\n",
            "|    total_timesteps | 305152    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 150          |\n",
            "|    time_elapsed         | 9500         |\n",
            "|    total_timesteps      | 307200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063467836 |\n",
            "|    clip_fraction        | 0.0709       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.63        |\n",
            "|    explained_variance   | 0.715        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.57e+05     |\n",
            "|    n_updates            | 1490         |\n",
            "|    policy_gradient_loss | -0.00352     |\n",
            "|    std                  | 0.589        |\n",
            "|    value_loss           | 3.01e+05     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 151         |\n",
            "|    time_elapsed         | 9542        |\n",
            "|    total_timesteps      | 309248      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006293649 |\n",
            "|    clip_fraction        | 0.0767      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.63       |\n",
            "|    explained_variance   | 0.706       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.01e+05    |\n",
            "|    n_updates            | 1500        |\n",
            "|    policy_gradient_loss | -0.00173    |\n",
            "|    std                  | 0.589       |\n",
            "|    value_loss           | 3e+05       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=310000, episode_reward=-50215.47 +/- 3.36\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 310000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0083584385 |\n",
            "|    clip_fraction        | 0.0846       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.61        |\n",
            "|    explained_variance   | 0.696        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.47e+05     |\n",
            "|    n_updates            | 1510         |\n",
            "|    policy_gradient_loss | -0.0035      |\n",
            "|    std                  | 0.585        |\n",
            "|    value_loss           | 2.97e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 152       |\n",
            "|    time_elapsed    | 9638      |\n",
            "|    total_timesteps | 311296    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 153         |\n",
            "|    time_elapsed         | 9679        |\n",
            "|    total_timesteps      | 313344      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006562836 |\n",
            "|    clip_fraction        | 0.07        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.6        |\n",
            "|    explained_variance   | 0.696       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.47e+05    |\n",
            "|    n_updates            | 1520        |\n",
            "|    policy_gradient_loss | 0.000359    |\n",
            "|    std                  | 0.582       |\n",
            "|    value_loss           | 3.02e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=315000, episode_reward=-50200.97 +/- 17.80\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 315000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010987399 |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.62       |\n",
            "|    explained_variance   | 0.613       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 9.14e+04    |\n",
            "|    n_updates            | 1530        |\n",
            "|    policy_gradient_loss | -0.000529   |\n",
            "|    std                  | 0.59        |\n",
            "|    value_loss           | 1.96e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 154       |\n",
            "|    time_elapsed    | 9775      |\n",
            "|    total_timesteps | 315392    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 155         |\n",
            "|    time_elapsed         | 9816        |\n",
            "|    total_timesteps      | 317440      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007273874 |\n",
            "|    clip_fraction        | 0.0916      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.64       |\n",
            "|    explained_variance   | 0.696       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.24e+05    |\n",
            "|    n_updates            | 1540        |\n",
            "|    policy_gradient_loss | -0.00504    |\n",
            "|    std                  | 0.592       |\n",
            "|    value_loss           | 2.89e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 32         |\n",
            "|    iterations           | 156        |\n",
            "|    time_elapsed         | 9858       |\n",
            "|    total_timesteps      | 319488     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00513783 |\n",
            "|    clip_fraction        | 0.0513     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.64      |\n",
            "|    explained_variance   | 0.717      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.2e+05    |\n",
            "|    n_updates            | 1550       |\n",
            "|    policy_gradient_loss | -0.00145   |\n",
            "|    std                  | 0.591      |\n",
            "|    value_loss           | 2.87e+05   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=320000, episode_reward=-50206.51 +/- 7.04\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 320000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0059791002 |\n",
            "|    clip_fraction        | 0.0487       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.64        |\n",
            "|    explained_variance   | 0.773        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.4e+05      |\n",
            "|    n_updates            | 1560         |\n",
            "|    policy_gradient_loss | -0.00121     |\n",
            "|    std                  | 0.592        |\n",
            "|    value_loss           | 2.87e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 157       |\n",
            "|    time_elapsed    | 9954      |\n",
            "|    total_timesteps | 321536    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 158         |\n",
            "|    time_elapsed         | 9995        |\n",
            "|    total_timesteps      | 323584      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007501998 |\n",
            "|    clip_fraction        | 0.0678      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.61       |\n",
            "|    explained_variance   | 0.739       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.59e+05    |\n",
            "|    n_updates            | 1570        |\n",
            "|    policy_gradient_loss | -0.00492    |\n",
            "|    std                  | 0.584       |\n",
            "|    value_loss           | 2.84e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=325000, episode_reward=-50213.60 +/- 11.01\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 325000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063429954 |\n",
            "|    clip_fraction        | 0.0692       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.58        |\n",
            "|    explained_variance   | 0.722        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.45e+05     |\n",
            "|    n_updates            | 1580         |\n",
            "|    policy_gradient_loss | -0.00264     |\n",
            "|    std                  | 0.581        |\n",
            "|    value_loss           | 2.82e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 159       |\n",
            "|    time_elapsed    | 10091     |\n",
            "|    total_timesteps | 325632    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 160          |\n",
            "|    time_elapsed         | 10132        |\n",
            "|    total_timesteps      | 327680       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063921763 |\n",
            "|    clip_fraction        | 0.0686       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.55        |\n",
            "|    explained_variance   | 0.736        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.19e+05     |\n",
            "|    n_updates            | 1590         |\n",
            "|    policy_gradient_loss | -0.00235     |\n",
            "|    std                  | 0.577        |\n",
            "|    value_loss           | 2.8e+05      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 161         |\n",
            "|    time_elapsed         | 10173       |\n",
            "|    total_timesteps      | 329728      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008145827 |\n",
            "|    clip_fraction        | 0.0707      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.57       |\n",
            "|    explained_variance   | 0.731       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.07e+05    |\n",
            "|    n_updates            | 1600        |\n",
            "|    policy_gradient_loss | -0.0046     |\n",
            "|    std                  | 0.583       |\n",
            "|    value_loss           | 2.79e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=330000, episode_reward=-50212.08 +/- 7.43\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 330000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00844099 |\n",
            "|    clip_fraction        | 0.0903     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.6       |\n",
            "|    explained_variance   | 0.754      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.63e+05   |\n",
            "|    n_updates            | 1610       |\n",
            "|    policy_gradient_loss | -0.00401   |\n",
            "|    std                  | 0.59       |\n",
            "|    value_loss           | 2.77e+05   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 162       |\n",
            "|    time_elapsed    | 10269     |\n",
            "|    total_timesteps | 331776    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 163         |\n",
            "|    time_elapsed         | 10310       |\n",
            "|    total_timesteps      | 333824      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012409475 |\n",
            "|    clip_fraction        | 0.089       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.6        |\n",
            "|    explained_variance   | 0.751       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.18e+05    |\n",
            "|    n_updates            | 1620        |\n",
            "|    policy_gradient_loss | -0.00228    |\n",
            "|    std                  | 0.586       |\n",
            "|    value_loss           | 2.76e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=335000, episode_reward=-50210.24 +/- 10.67\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 335000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062111923 |\n",
            "|    clip_fraction        | 0.0596       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.58        |\n",
            "|    explained_variance   | 0.752        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.02e+05     |\n",
            "|    n_updates            | 1630         |\n",
            "|    policy_gradient_loss | -0.0025      |\n",
            "|    std                  | 0.579        |\n",
            "|    value_loss           | 2.75e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 164       |\n",
            "|    time_elapsed    | 10406     |\n",
            "|    total_timesteps | 335872    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 165          |\n",
            "|    time_elapsed         | 10447        |\n",
            "|    total_timesteps      | 337920       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064789946 |\n",
            "|    clip_fraction        | 0.0585       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.55        |\n",
            "|    explained_variance   | 0.755        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 9.53e+04     |\n",
            "|    n_updates            | 1640         |\n",
            "|    policy_gradient_loss | -0.000373    |\n",
            "|    std                  | 0.575        |\n",
            "|    value_loss           | 2.74e+05     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 166          |\n",
            "|    time_elapsed         | 10488        |\n",
            "|    total_timesteps      | 339968       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0057013235 |\n",
            "|    clip_fraction        | 0.0791       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.52        |\n",
            "|    explained_variance   | 0.76         |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 8.75e+04     |\n",
            "|    n_updates            | 1650         |\n",
            "|    policy_gradient_loss | -0.00496     |\n",
            "|    std                  | 0.571        |\n",
            "|    value_loss           | 2.73e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=340000, episode_reward=-50195.27 +/- 9.56\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 340000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0061255116 |\n",
            "|    clip_fraction        | 0.0768       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.5         |\n",
            "|    explained_variance   | 0.764        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.72e+05     |\n",
            "|    n_updates            | 1660         |\n",
            "|    policy_gradient_loss | -0.00334     |\n",
            "|    std                  | 0.569        |\n",
            "|    value_loss           | 2.73e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 167       |\n",
            "|    time_elapsed    | 10583     |\n",
            "|    total_timesteps | 342016    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 168          |\n",
            "|    time_elapsed         | 10625        |\n",
            "|    total_timesteps      | 344064       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0068314653 |\n",
            "|    clip_fraction        | 0.0757       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.46        |\n",
            "|    explained_variance   | 0.798        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.09e+05     |\n",
            "|    n_updates            | 1670         |\n",
            "|    policy_gradient_loss | -0.0037      |\n",
            "|    std                  | 0.56         |\n",
            "|    value_loss           | 2.66e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=345000, episode_reward=-50199.23 +/- 11.47\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 345000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009043653 |\n",
            "|    clip_fraction        | 0.088       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.45       |\n",
            "|    explained_variance   | 0.793       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.26e+05    |\n",
            "|    n_updates            | 1680        |\n",
            "|    policy_gradient_loss | -0.00336    |\n",
            "|    std                  | 0.559       |\n",
            "|    value_loss           | 2.59e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 169       |\n",
            "|    time_elapsed    | 10721     |\n",
            "|    total_timesteps | 346112    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 170         |\n",
            "|    time_elapsed         | 10763       |\n",
            "|    total_timesteps      | 348160      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009980445 |\n",
            "|    clip_fraction        | 0.109       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.43       |\n",
            "|    explained_variance   | 0.772       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.82e+05    |\n",
            "|    n_updates            | 1690        |\n",
            "|    policy_gradient_loss | -0.00322    |\n",
            "|    std                  | 0.558       |\n",
            "|    value_loss           | 2.55e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=350000, episode_reward=-50204.84 +/- 7.61\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 350000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008149625 |\n",
            "|    clip_fraction        | 0.0911      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.43       |\n",
            "|    explained_variance   | 0.768       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.81e+04    |\n",
            "|    n_updates            | 1700        |\n",
            "|    policy_gradient_loss | -0.00354    |\n",
            "|    std                  | 0.56        |\n",
            "|    value_loss           | 2.52e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 171       |\n",
            "|    time_elapsed    | 10859     |\n",
            "|    total_timesteps | 350208    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 172         |\n",
            "|    time_elapsed         | 10900       |\n",
            "|    total_timesteps      | 352256      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008313205 |\n",
            "|    clip_fraction        | 0.0961      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.39       |\n",
            "|    explained_variance   | 0.77        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.13e+05    |\n",
            "|    n_updates            | 1710        |\n",
            "|    policy_gradient_loss | -0.00273    |\n",
            "|    std                  | 0.55        |\n",
            "|    value_loss           | 2.52e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 173         |\n",
            "|    time_elapsed         | 10942       |\n",
            "|    total_timesteps      | 354304      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007951643 |\n",
            "|    clip_fraction        | 0.0794      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.35       |\n",
            "|    explained_variance   | 0.765       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.04e+05    |\n",
            "|    n_updates            | 1720        |\n",
            "|    policy_gradient_loss | -0.00264    |\n",
            "|    std                  | 0.542       |\n",
            "|    value_loss           | 2.49e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=355000, episode_reward=-50208.27 +/- 6.11\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 355000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005977057 |\n",
            "|    clip_fraction        | 0.0627      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.34       |\n",
            "|    explained_variance   | 0.794       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.27e+05    |\n",
            "|    n_updates            | 1730        |\n",
            "|    policy_gradient_loss | -0.0026     |\n",
            "|    std                  | 0.541       |\n",
            "|    value_loss           | 2.54e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 174       |\n",
            "|    time_elapsed    | 11037     |\n",
            "|    total_timesteps | 356352    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 175         |\n",
            "|    time_elapsed         | 11078       |\n",
            "|    total_timesteps      | 358400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011524498 |\n",
            "|    clip_fraction        | 0.0963      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.32       |\n",
            "|    explained_variance   | 0.771       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.1e+05     |\n",
            "|    n_updates            | 1740        |\n",
            "|    policy_gradient_loss | -0.00387    |\n",
            "|    std                  | 0.539       |\n",
            "|    value_loss           | 2.44e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=360000, episode_reward=-50200.65 +/- 13.75\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 360000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007833243 |\n",
            "|    clip_fraction        | 0.0777      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.29       |\n",
            "|    explained_variance   | 0.782       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.57e+05    |\n",
            "|    n_updates            | 1750        |\n",
            "|    policy_gradient_loss | -0.00118    |\n",
            "|    std                  | 0.539       |\n",
            "|    value_loss           | 2.42e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 176       |\n",
            "|    time_elapsed    | 11173     |\n",
            "|    total_timesteps | 360448    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 177          |\n",
            "|    time_elapsed         | 11215        |\n",
            "|    total_timesteps      | 362496       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064682746 |\n",
            "|    clip_fraction        | 0.0826       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.27        |\n",
            "|    explained_variance   | 0.783        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 9.9e+04      |\n",
            "|    n_updates            | 1760         |\n",
            "|    policy_gradient_loss | -0.00121     |\n",
            "|    std                  | 0.532        |\n",
            "|    value_loss           | 2.4e+05      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 178         |\n",
            "|    time_elapsed         | 11256       |\n",
            "|    total_timesteps      | 364544      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009360736 |\n",
            "|    clip_fraction        | 0.117       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.26       |\n",
            "|    explained_variance   | 0.79        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.11e+05    |\n",
            "|    n_updates            | 1770        |\n",
            "|    policy_gradient_loss | -0.00616    |\n",
            "|    std                  | 0.535       |\n",
            "|    value_loss           | 2.38e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=365000, episode_reward=-50198.99 +/- 19.82\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 365000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005370749 |\n",
            "|    clip_fraction        | 0.0844      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.28       |\n",
            "|    explained_variance   | 0.793       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.17e+05    |\n",
            "|    n_updates            | 1780        |\n",
            "|    policy_gradient_loss | -0.00171    |\n",
            "|    std                  | 0.539       |\n",
            "|    value_loss           | 2.36e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 179       |\n",
            "|    time_elapsed    | 11352     |\n",
            "|    total_timesteps | 366592    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 180          |\n",
            "|    time_elapsed         | 11393        |\n",
            "|    total_timesteps      | 368640       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071651246 |\n",
            "|    clip_fraction        | 0.104        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.29        |\n",
            "|    explained_variance   | 0.782        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.1e+05      |\n",
            "|    n_updates            | 1790         |\n",
            "|    policy_gradient_loss | 0.000345     |\n",
            "|    std                  | 0.54         |\n",
            "|    value_loss           | 2.36e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=370000, episode_reward=-50215.65 +/- 3.51\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 370000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010053923 |\n",
            "|    clip_fraction        | 0.101       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.28       |\n",
            "|    explained_variance   | 0.806       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.76e+04    |\n",
            "|    n_updates            | 1800        |\n",
            "|    policy_gradient_loss | -0.00707    |\n",
            "|    std                  | 0.534       |\n",
            "|    value_loss           | 2.33e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 181       |\n",
            "|    time_elapsed    | 11488     |\n",
            "|    total_timesteps | 370688    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 182         |\n",
            "|    time_elapsed         | 11530       |\n",
            "|    total_timesteps      | 372736      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010779517 |\n",
            "|    clip_fraction        | 0.0935      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.21       |\n",
            "|    explained_variance   | 0.8         |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.27e+05    |\n",
            "|    n_updates            | 1810        |\n",
            "|    policy_gradient_loss | -0.00317    |\n",
            "|    std                  | 0.52        |\n",
            "|    value_loss           | 2.32e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 183          |\n",
            "|    time_elapsed         | 11571        |\n",
            "|    total_timesteps      | 374784       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0094077885 |\n",
            "|    clip_fraction        | 0.107        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.18        |\n",
            "|    explained_variance   | 0.799        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.42e+05     |\n",
            "|    n_updates            | 1820         |\n",
            "|    policy_gradient_loss | 0.000939     |\n",
            "|    std                  | 0.519        |\n",
            "|    value_loss           | 2.41e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=375000, episode_reward=-50208.32 +/- 17.72\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 375000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007993836 |\n",
            "|    clip_fraction        | 0.0958      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.16       |\n",
            "|    explained_variance   | 0.803       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.22e+05    |\n",
            "|    n_updates            | 1830        |\n",
            "|    policy_gradient_loss | -0.00409    |\n",
            "|    std                  | 0.516       |\n",
            "|    value_loss           | 2.3e+05     |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 184       |\n",
            "|    time_elapsed    | 11668     |\n",
            "|    total_timesteps | 376832    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 32         |\n",
            "|    iterations           | 185        |\n",
            "|    time_elapsed         | 11710      |\n",
            "|    total_timesteps      | 378880     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00867426 |\n",
            "|    clip_fraction        | 0.0757     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.16      |\n",
            "|    explained_variance   | 0.78       |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.02e+05   |\n",
            "|    n_updates            | 1840       |\n",
            "|    policy_gradient_loss | -0.00194   |\n",
            "|    std                  | 0.519      |\n",
            "|    value_loss           | 2.33e+05   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=380000, episode_reward=-50208.75 +/- 13.18\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 380000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008484349 |\n",
            "|    clip_fraction        | 0.105       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.17       |\n",
            "|    explained_variance   | 0.755       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.36e+05    |\n",
            "|    n_updates            | 1850        |\n",
            "|    policy_gradient_loss | -0.000183   |\n",
            "|    std                  | 0.519       |\n",
            "|    value_loss           | 2.31e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 186       |\n",
            "|    time_elapsed    | 11807     |\n",
            "|    total_timesteps | 380928    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 187         |\n",
            "|    time_elapsed         | 11848       |\n",
            "|    total_timesteps      | 382976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015272861 |\n",
            "|    clip_fraction        | 0.145       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.15       |\n",
            "|    explained_variance   | 0.782       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.62e+04    |\n",
            "|    n_updates            | 1860        |\n",
            "|    policy_gradient_loss | 0.0107      |\n",
            "|    std                  | 0.514       |\n",
            "|    value_loss           | 2.15e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=385000, episode_reward=-50208.55 +/- 10.70\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 385000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009946579 |\n",
            "|    clip_fraction        | 0.079       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.16       |\n",
            "|    explained_variance   | 0.808       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.06e+05    |\n",
            "|    n_updates            | 1870        |\n",
            "|    policy_gradient_loss | -0.00204    |\n",
            "|    std                  | 0.52        |\n",
            "|    value_loss           | 2.32e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 188       |\n",
            "|    time_elapsed    | 11945     |\n",
            "|    total_timesteps | 385024    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 189         |\n",
            "|    time_elapsed         | 11987       |\n",
            "|    total_timesteps      | 387072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008658767 |\n",
            "|    clip_fraction        | 0.0976      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.15       |\n",
            "|    explained_variance   | 0.816       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.84e+04    |\n",
            "|    n_updates            | 1880        |\n",
            "|    policy_gradient_loss | -0.00393    |\n",
            "|    std                  | 0.512       |\n",
            "|    value_loss           | 2.23e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 190          |\n",
            "|    time_elapsed         | 12029        |\n",
            "|    total_timesteps      | 389120       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073701655 |\n",
            "|    clip_fraction        | 0.116        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.13        |\n",
            "|    explained_variance   | 0.772        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.15e+05     |\n",
            "|    n_updates            | 1890         |\n",
            "|    policy_gradient_loss | 0.00191      |\n",
            "|    std                  | 0.511        |\n",
            "|    value_loss           | 2.18e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=390000, episode_reward=-50210.36 +/- 15.93\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 390000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008024133 |\n",
            "|    clip_fraction        | 0.0938      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.12       |\n",
            "|    explained_variance   | 0.812       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.04e+05    |\n",
            "|    n_updates            | 1900        |\n",
            "|    policy_gradient_loss | -0.0016     |\n",
            "|    std                  | 0.507       |\n",
            "|    value_loss           | 2.17e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 191       |\n",
            "|    time_elapsed    | 12125     |\n",
            "|    total_timesteps | 391168    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 192         |\n",
            "|    time_elapsed         | 12167       |\n",
            "|    total_timesteps      | 393216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006686468 |\n",
            "|    clip_fraction        | 0.0744      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.14       |\n",
            "|    explained_variance   | 0.806       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.22e+05    |\n",
            "|    n_updates            | 1910        |\n",
            "|    policy_gradient_loss | -0.00114    |\n",
            "|    std                  | 0.515       |\n",
            "|    value_loss           | 2.17e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=395000, episode_reward=-50204.94 +/- 17.10\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 395000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008234125 |\n",
            "|    clip_fraction        | 0.0866      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.14       |\n",
            "|    explained_variance   | 0.805       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.02e+05    |\n",
            "|    n_updates            | 1920        |\n",
            "|    policy_gradient_loss | -0.0014     |\n",
            "|    std                  | 0.513       |\n",
            "|    value_loss           | 2.16e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 193       |\n",
            "|    time_elapsed    | 12264     |\n",
            "|    total_timesteps | 395264    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 194          |\n",
            "|    time_elapsed         | 12306        |\n",
            "|    total_timesteps      | 397312       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073905527 |\n",
            "|    clip_fraction        | 0.0835       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.13        |\n",
            "|    explained_variance   | 0.814        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1e+05        |\n",
            "|    n_updates            | 1930         |\n",
            "|    policy_gradient_loss | -0.00321     |\n",
            "|    std                  | 0.508        |\n",
            "|    value_loss           | 2.14e+05     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 195         |\n",
            "|    time_elapsed         | 12347       |\n",
            "|    total_timesteps      | 399360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005575249 |\n",
            "|    clip_fraction        | 0.0534      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.12       |\n",
            "|    explained_variance   | 0.824       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.88e+04    |\n",
            "|    n_updates            | 1940        |\n",
            "|    policy_gradient_loss | -0.00244    |\n",
            "|    std                  | 0.51        |\n",
            "|    value_loss           | 2.21e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=400000, episode_reward=-50203.18 +/- 16.23\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 400000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0072118016 |\n",
            "|    clip_fraction        | 0.0936       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.11        |\n",
            "|    explained_variance   | 0.824        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 9.33e+04     |\n",
            "|    n_updates            | 1950         |\n",
            "|    policy_gradient_loss | -0.00255     |\n",
            "|    std                  | 0.505        |\n",
            "|    value_loss           | 2.09e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 196       |\n",
            "|    time_elapsed    | 12445     |\n",
            "|    total_timesteps | 401408    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 32         |\n",
            "|    iterations           | 197        |\n",
            "|    time_elapsed         | 12487      |\n",
            "|    total_timesteps      | 403456     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00908093 |\n",
            "|    clip_fraction        | 0.102      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.07      |\n",
            "|    explained_variance   | 0.821      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 7.51e+04   |\n",
            "|    n_updates            | 1960       |\n",
            "|    policy_gradient_loss | -0.00428   |\n",
            "|    std                  | 0.497      |\n",
            "|    value_loss           | 2.07e+05   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=405000, episode_reward=-50212.49 +/- 10.85\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 405000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009261074 |\n",
            "|    clip_fraction        | 0.106       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.06       |\n",
            "|    explained_variance   | 0.82        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.15e+05    |\n",
            "|    n_updates            | 1970        |\n",
            "|    policy_gradient_loss | -0.00385    |\n",
            "|    std                  | 0.498       |\n",
            "|    value_loss           | 2.06e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 198       |\n",
            "|    time_elapsed    | 12585     |\n",
            "|    total_timesteps | 405504    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 199         |\n",
            "|    time_elapsed         | 12628       |\n",
            "|    total_timesteps      | 407552      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009217624 |\n",
            "|    clip_fraction        | 0.099       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.06       |\n",
            "|    explained_variance   | 0.825       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.05e+05    |\n",
            "|    n_updates            | 1980        |\n",
            "|    policy_gradient_loss | -0.0038     |\n",
            "|    std                  | 0.494       |\n",
            "|    value_loss           | 2.05e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 200         |\n",
            "|    time_elapsed         | 12670       |\n",
            "|    total_timesteps      | 409600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008034512 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.04       |\n",
            "|    explained_variance   | 0.825       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.01e+05    |\n",
            "|    n_updates            | 1990        |\n",
            "|    policy_gradient_loss | -0.00325    |\n",
            "|    std                  | 0.491       |\n",
            "|    value_loss           | 2.03e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=410000, episode_reward=-50212.05 +/- 10.98\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 410000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010750765 |\n",
            "|    clip_fraction        | 0.108       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.04       |\n",
            "|    explained_variance   | 0.817       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.26e+05    |\n",
            "|    n_updates            | 2000        |\n",
            "|    policy_gradient_loss | -0.00119    |\n",
            "|    std                  | 0.491       |\n",
            "|    value_loss           | 2.06e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 201       |\n",
            "|    time_elapsed    | 12766     |\n",
            "|    total_timesteps | 411648    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 202         |\n",
            "|    time_elapsed         | 12808       |\n",
            "|    total_timesteps      | 413696      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022649229 |\n",
            "|    clip_fraction        | 0.175       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.04       |\n",
            "|    explained_variance   | 0.673       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.53e+04    |\n",
            "|    n_updates            | 2010        |\n",
            "|    policy_gradient_loss | 0.0116      |\n",
            "|    std                  | 0.491       |\n",
            "|    value_loss           | 1.64e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=415000, episode_reward=-50219.43 +/- 6.71\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 415000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013213655 |\n",
            "|    clip_fraction        | 0.131       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.01       |\n",
            "|    explained_variance   | 0.82        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.3e+04     |\n",
            "|    n_updates            | 2020        |\n",
            "|    policy_gradient_loss | -0.004      |\n",
            "|    std                  | 0.48        |\n",
            "|    value_loss           | 2.03e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 203       |\n",
            "|    time_elapsed    | 12905     |\n",
            "|    total_timesteps | 415744    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 204         |\n",
            "|    time_elapsed         | 12948       |\n",
            "|    total_timesteps      | 417792      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011575615 |\n",
            "|    clip_fraction        | 0.107       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.95       |\n",
            "|    explained_variance   | 0.834       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.88e+04    |\n",
            "|    n_updates            | 2030        |\n",
            "|    policy_gradient_loss | -0.00159    |\n",
            "|    std                  | 0.471       |\n",
            "|    value_loss           | 2.01e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 205          |\n",
            "|    time_elapsed         | 12990        |\n",
            "|    total_timesteps      | 419840       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0069216117 |\n",
            "|    clip_fraction        | 0.106        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.92        |\n",
            "|    explained_variance   | 0.824        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.01e+05     |\n",
            "|    n_updates            | 2040         |\n",
            "|    policy_gradient_loss | -0.00384     |\n",
            "|    std                  | 0.472        |\n",
            "|    value_loss           | 2.02e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=420000, episode_reward=-50204.97 +/- 7.61\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 420000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0090736635 |\n",
            "|    clip_fraction        | 0.085        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.93        |\n",
            "|    explained_variance   | 0.826        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 1.16e+05     |\n",
            "|    n_updates            | 2050         |\n",
            "|    policy_gradient_loss | -0.00165     |\n",
            "|    std                  | 0.472        |\n",
            "|    value_loss           | 2.04e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 206       |\n",
            "|    time_elapsed    | 13088     |\n",
            "|    total_timesteps | 421888    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 207         |\n",
            "|    time_elapsed         | 13131       |\n",
            "|    total_timesteps      | 423936      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007923761 |\n",
            "|    clip_fraction        | 0.0958      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.94       |\n",
            "|    explained_variance   | 0.834       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 9.27e+04    |\n",
            "|    n_updates            | 2060        |\n",
            "|    policy_gradient_loss | -0.00474    |\n",
            "|    std                  | 0.474       |\n",
            "|    value_loss           | 2.01e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=425000, episode_reward=-50209.74 +/- 7.23\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 425000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011150313 |\n",
            "|    clip_fraction        | 0.116       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.96       |\n",
            "|    explained_variance   | 0.832       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.06e+05    |\n",
            "|    n_updates            | 2070        |\n",
            "|    policy_gradient_loss | 0.00154     |\n",
            "|    std                  | 0.477       |\n",
            "|    value_loss           | 1.95e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 208       |\n",
            "|    time_elapsed    | 13227     |\n",
            "|    total_timesteps | 425984    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 209         |\n",
            "|    time_elapsed         | 13270       |\n",
            "|    total_timesteps      | 428032      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009407477 |\n",
            "|    clip_fraction        | 0.1         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.95       |\n",
            "|    explained_variance   | 0.838       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.09e+04    |\n",
            "|    n_updates            | 2080        |\n",
            "|    policy_gradient_loss | -0.0033     |\n",
            "|    std                  | 0.471       |\n",
            "|    value_loss           | 2.04e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=430000, episode_reward=-50213.88 +/- 7.88\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 430000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008933788 |\n",
            "|    clip_fraction        | 0.109       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.95       |\n",
            "|    explained_variance   | 0.858       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.44e+05    |\n",
            "|    n_updates            | 2090        |\n",
            "|    policy_gradient_loss | -0.00243    |\n",
            "|    std                  | 0.475       |\n",
            "|    value_loss           | 1.94e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 210       |\n",
            "|    time_elapsed    | 13368     |\n",
            "|    total_timesteps | 430080    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 211         |\n",
            "|    time_elapsed         | 13410       |\n",
            "|    total_timesteps      | 432128      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008081406 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.93       |\n",
            "|    explained_variance   | 0.856       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.16e+05    |\n",
            "|    n_updates            | 2100        |\n",
            "|    policy_gradient_loss | -0.00292    |\n",
            "|    std                  | 0.47        |\n",
            "|    value_loss           | 1.92e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 212         |\n",
            "|    time_elapsed         | 13453       |\n",
            "|    total_timesteps      | 434176      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007055591 |\n",
            "|    clip_fraction        | 0.0932      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.9        |\n",
            "|    explained_variance   | 0.866       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.98e+04    |\n",
            "|    n_updates            | 2110        |\n",
            "|    policy_gradient_loss | -0.0043     |\n",
            "|    std                  | 0.465       |\n",
            "|    value_loss           | 1.89e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=435000, episode_reward=-50213.71 +/- 12.60\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 435000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010480416 |\n",
            "|    clip_fraction        | 0.122       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.86       |\n",
            "|    explained_variance   | 0.85        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 9.25e+04    |\n",
            "|    n_updates            | 2120        |\n",
            "|    policy_gradient_loss | -0.00476    |\n",
            "|    std                  | 0.457       |\n",
            "|    value_loss           | 1.85e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 213       |\n",
            "|    time_elapsed    | 13549     |\n",
            "|    total_timesteps | 436224    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 214          |\n",
            "|    time_elapsed         | 13593        |\n",
            "|    total_timesteps      | 438272       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0079808645 |\n",
            "|    clip_fraction        | 0.0936       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.84        |\n",
            "|    explained_variance   | 0.844        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 8.58e+04     |\n",
            "|    n_updates            | 2130         |\n",
            "|    policy_gradient_loss | -2.2e-06     |\n",
            "|    std                  | 0.457        |\n",
            "|    value_loss           | 1.88e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=440000, episode_reward=-50215.90 +/- 6.74\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 440000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008706508 |\n",
            "|    clip_fraction        | 0.119       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.85       |\n",
            "|    explained_variance   | 0.841       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.93e+04    |\n",
            "|    n_updates            | 2140        |\n",
            "|    policy_gradient_loss | -0.00453    |\n",
            "|    std                  | 0.461       |\n",
            "|    value_loss           | 1.89e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 215       |\n",
            "|    time_elapsed    | 13690     |\n",
            "|    total_timesteps | 440320    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 216         |\n",
            "|    time_elapsed         | 13732       |\n",
            "|    total_timesteps      | 442368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007822694 |\n",
            "|    clip_fraction        | 0.124       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.84       |\n",
            "|    explained_variance   | 0.849       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 9.75e+04    |\n",
            "|    n_updates            | 2150        |\n",
            "|    policy_gradient_loss | -0.00474    |\n",
            "|    std                  | 0.455       |\n",
            "|    value_loss           | 1.84e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 217         |\n",
            "|    time_elapsed         | 13776       |\n",
            "|    total_timesteps      | 444416      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012095815 |\n",
            "|    clip_fraction        | 0.106       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.82       |\n",
            "|    explained_variance   | 0.855       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.76e+04    |\n",
            "|    n_updates            | 2160        |\n",
            "|    policy_gradient_loss | -0.00332    |\n",
            "|    std                  | 0.454       |\n",
            "|    value_loss           | 1.81e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=445000, episode_reward=-50210.56 +/- 12.33\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 445000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011038378 |\n",
            "|    clip_fraction        | 0.113       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.82       |\n",
            "|    explained_variance   | 0.856       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.87e+04    |\n",
            "|    n_updates            | 2170        |\n",
            "|    policy_gradient_loss | -0.00348    |\n",
            "|    std                  | 0.453       |\n",
            "|    value_loss           | 1.8e+05     |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 218       |\n",
            "|    time_elapsed    | 13873     |\n",
            "|    total_timesteps | 446464    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 219         |\n",
            "|    time_elapsed         | 13916       |\n",
            "|    total_timesteps      | 448512      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011824038 |\n",
            "|    clip_fraction        | 0.135       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.8        |\n",
            "|    explained_variance   | 0.859       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.98e+04    |\n",
            "|    n_updates            | 2180        |\n",
            "|    policy_gradient_loss | -0.0049     |\n",
            "|    std                  | 0.452       |\n",
            "|    value_loss           | 1.79e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=450000, episode_reward=-50217.88 +/- 14.44\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 450000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009688577 |\n",
            "|    clip_fraction        | 0.11        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.81       |\n",
            "|    explained_variance   | 0.854       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.09e+05    |\n",
            "|    n_updates            | 2190        |\n",
            "|    policy_gradient_loss | -0.00179    |\n",
            "|    std                  | 0.457       |\n",
            "|    value_loss           | 1.8e+05     |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 220       |\n",
            "|    time_elapsed    | 14013     |\n",
            "|    total_timesteps | 450560    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 221         |\n",
            "|    time_elapsed         | 14056       |\n",
            "|    total_timesteps      | 452608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009353217 |\n",
            "|    clip_fraction        | 0.117       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.84       |\n",
            "|    explained_variance   | 0.867       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.27e+04    |\n",
            "|    n_updates            | 2200        |\n",
            "|    policy_gradient_loss | -0.000886   |\n",
            "|    std                  | 0.461       |\n",
            "|    value_loss           | 1.77e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 222         |\n",
            "|    time_elapsed         | 14099       |\n",
            "|    total_timesteps      | 454656      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010054689 |\n",
            "|    clip_fraction        | 0.125       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.83       |\n",
            "|    explained_variance   | 0.859       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.59e+04    |\n",
            "|    n_updates            | 2210        |\n",
            "|    policy_gradient_loss | -0.00306    |\n",
            "|    std                  | 0.457       |\n",
            "|    value_loss           | 1.77e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=455000, episode_reward=-50206.61 +/- 3.55\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 455000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01264411 |\n",
            "|    clip_fraction        | 0.147      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.8       |\n",
            "|    explained_variance   | 0.864      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 9.74e+04   |\n",
            "|    n_updates            | 2220       |\n",
            "|    policy_gradient_loss | -0.005     |\n",
            "|    std                  | 0.451      |\n",
            "|    value_loss           | 1.76e+05   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 223       |\n",
            "|    time_elapsed    | 14196     |\n",
            "|    total_timesteps | 456704    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 224         |\n",
            "|    time_elapsed         | 14239       |\n",
            "|    total_timesteps      | 458752      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012256077 |\n",
            "|    clip_fraction        | 0.147       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.79       |\n",
            "|    explained_variance   | 0.859       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.58e+04    |\n",
            "|    n_updates            | 2230        |\n",
            "|    policy_gradient_loss | -0.00459    |\n",
            "|    std                  | 0.451       |\n",
            "|    value_loss           | 1.76e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=460000, episode_reward=-50210.04 +/- 10.57\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 460000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01027376 |\n",
            "|    clip_fraction        | 0.137      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.81      |\n",
            "|    explained_variance   | 0.867      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 8.43e+04   |\n",
            "|    n_updates            | 2240       |\n",
            "|    policy_gradient_loss | -0.00368   |\n",
            "|    std                  | 0.455      |\n",
            "|    value_loss           | 1.75e+05   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 225       |\n",
            "|    time_elapsed    | 14336     |\n",
            "|    total_timesteps | 460800    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 226         |\n",
            "|    time_elapsed         | 14379       |\n",
            "|    total_timesteps      | 462848      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009137537 |\n",
            "|    clip_fraction        | 0.11        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.8        |\n",
            "|    explained_variance   | 0.866       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.96e+04    |\n",
            "|    n_updates            | 2250        |\n",
            "|    policy_gradient_loss | -0.00371    |\n",
            "|    std                  | 0.451       |\n",
            "|    value_loss           | 1.75e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 227         |\n",
            "|    time_elapsed         | 14421       |\n",
            "|    total_timesteps      | 464896      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010555098 |\n",
            "|    clip_fraction        | 0.131       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.78       |\n",
            "|    explained_variance   | 0.866       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.37e+05    |\n",
            "|    n_updates            | 2260        |\n",
            "|    policy_gradient_loss | -0.00504    |\n",
            "|    std                  | 0.448       |\n",
            "|    value_loss           | 1.75e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=465000, episode_reward=-50199.03 +/- 13.13\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 465000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008847916 |\n",
            "|    clip_fraction        | 0.0976      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.75       |\n",
            "|    explained_variance   | 0.867       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 9.26e+04    |\n",
            "|    n_updates            | 2270        |\n",
            "|    policy_gradient_loss | -0.00146    |\n",
            "|    std                  | 0.443       |\n",
            "|    value_loss           | 1.76e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 228       |\n",
            "|    time_elapsed    | 14518     |\n",
            "|    total_timesteps | 466944    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 229         |\n",
            "|    time_elapsed         | 14561       |\n",
            "|    total_timesteps      | 468992      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009362459 |\n",
            "|    clip_fraction        | 0.122       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.74       |\n",
            "|    explained_variance   | 0.863       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.25e+04    |\n",
            "|    n_updates            | 2280        |\n",
            "|    policy_gradient_loss | -0.00274    |\n",
            "|    std                  | 0.444       |\n",
            "|    value_loss           | 1.77e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=470000, episode_reward=-50191.95 +/- 12.16\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 470000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009206517 |\n",
            "|    clip_fraction        | 0.111       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.73       |\n",
            "|    explained_variance   | 0.868       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.62e+04    |\n",
            "|    n_updates            | 2290        |\n",
            "|    policy_gradient_loss | -0.00251    |\n",
            "|    std                  | 0.441       |\n",
            "|    value_loss           | 1.77e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 230       |\n",
            "|    time_elapsed    | 14659     |\n",
            "|    total_timesteps | 471040    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 231         |\n",
            "|    time_elapsed         | 14702       |\n",
            "|    total_timesteps      | 473088      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010132934 |\n",
            "|    clip_fraction        | 0.13        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.73       |\n",
            "|    explained_variance   | 0.884       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.26e+04    |\n",
            "|    n_updates            | 2300        |\n",
            "|    policy_gradient_loss | 0.00274     |\n",
            "|    std                  | 0.44        |\n",
            "|    value_loss           | 1.65e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=475000, episode_reward=-50213.37 +/- 9.01\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 475000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010727951 |\n",
            "|    clip_fraction        | 0.126       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.72       |\n",
            "|    explained_variance   | 0.882       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 6.85e+04    |\n",
            "|    n_updates            | 2310        |\n",
            "|    policy_gradient_loss | -0.00287    |\n",
            "|    std                  | 0.44        |\n",
            "|    value_loss           | 1.67e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 232       |\n",
            "|    time_elapsed    | 14798     |\n",
            "|    total_timesteps | 475136    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 233         |\n",
            "|    time_elapsed         | 14841       |\n",
            "|    total_timesteps      | 477184      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011026319 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.71       |\n",
            "|    explained_variance   | 0.877       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.88e+04    |\n",
            "|    n_updates            | 2320        |\n",
            "|    policy_gradient_loss | -0.00242    |\n",
            "|    std                  | 0.436       |\n",
            "|    value_loss           | 1.67e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 234         |\n",
            "|    time_elapsed         | 14883       |\n",
            "|    total_timesteps      | 479232      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009689901 |\n",
            "|    clip_fraction        | 0.128       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.69       |\n",
            "|    explained_variance   | 0.876       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.16e+04    |\n",
            "|    n_updates            | 2330        |\n",
            "|    policy_gradient_loss | -0.00105    |\n",
            "|    std                  | 0.433       |\n",
            "|    value_loss           | 1.66e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=480000, episode_reward=-50208.26 +/- 10.63\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 480000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009874884 |\n",
            "|    clip_fraction        | 0.117       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.64       |\n",
            "|    explained_variance   | 0.874       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.09e+05    |\n",
            "|    n_updates            | 2340        |\n",
            "|    policy_gradient_loss | -0.00416    |\n",
            "|    std                  | 0.423       |\n",
            "|    value_loss           | 1.63e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 235       |\n",
            "|    time_elapsed    | 14981     |\n",
            "|    total_timesteps | 481280    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 236         |\n",
            "|    time_elapsed         | 15024       |\n",
            "|    total_timesteps      | 483328      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011733988 |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.6        |\n",
            "|    explained_variance   | 0.865       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 9.64e+04    |\n",
            "|    n_updates            | 2350        |\n",
            "|    policy_gradient_loss | -0.00237    |\n",
            "|    std                  | 0.423       |\n",
            "|    value_loss           | 1.69e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=485000, episode_reward=-50210.31 +/- 6.75\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 485000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013819177 |\n",
            "|    clip_fraction        | 0.147       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.58       |\n",
            "|    explained_variance   | 0.875       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.77e+04    |\n",
            "|    n_updates            | 2360        |\n",
            "|    policy_gradient_loss | -0.00492    |\n",
            "|    std                  | 0.417       |\n",
            "|    value_loss           | 1.62e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 237       |\n",
            "|    time_elapsed    | 15121     |\n",
            "|    total_timesteps | 485376    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 238         |\n",
            "|    time_elapsed         | 15164       |\n",
            "|    total_timesteps      | 487424      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007554779 |\n",
            "|    clip_fraction        | 0.102       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.56       |\n",
            "|    explained_variance   | 0.876       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.81e+04    |\n",
            "|    n_updates            | 2370        |\n",
            "|    policy_gradient_loss | -0.00219    |\n",
            "|    std                  | 0.419       |\n",
            "|    value_loss           | 1.61e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 239         |\n",
            "|    time_elapsed         | 15206       |\n",
            "|    total_timesteps      | 489472      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008914919 |\n",
            "|    clip_fraction        | 0.105       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.56       |\n",
            "|    explained_variance   | 0.873       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.22e+05    |\n",
            "|    n_updates            | 2380        |\n",
            "|    policy_gradient_loss | -0.000935   |\n",
            "|    std                  | 0.417       |\n",
            "|    value_loss           | 1.61e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=490000, episode_reward=-50202.67 +/- 9.34\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 490000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013945809 |\n",
            "|    clip_fraction        | 0.127       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.52       |\n",
            "|    explained_variance   | 0.876       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.88e+04    |\n",
            "|    n_updates            | 2390        |\n",
            "|    policy_gradient_loss | -0.00402    |\n",
            "|    std                  | 0.41        |\n",
            "|    value_loss           | 1.6e+05     |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 240       |\n",
            "|    time_elapsed    | 15303     |\n",
            "|    total_timesteps | 491520    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 241         |\n",
            "|    time_elapsed         | 15345       |\n",
            "|    total_timesteps      | 493568      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008830186 |\n",
            "|    clip_fraction        | 0.12        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.5        |\n",
            "|    explained_variance   | 0.879       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 6.52e+04    |\n",
            "|    n_updates            | 2400        |\n",
            "|    policy_gradient_loss | -0.00327    |\n",
            "|    std                  | 0.41        |\n",
            "|    value_loss           | 1.59e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=495000, episode_reward=-50215.74 +/- 10.34\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 495000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013087208 |\n",
            "|    clip_fraction        | 0.143       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.5        |\n",
            "|    explained_variance   | 0.881       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.07e+04    |\n",
            "|    n_updates            | 2410        |\n",
            "|    policy_gradient_loss | -0.00496    |\n",
            "|    std                  | 0.411       |\n",
            "|    value_loss           | 1.58e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 242       |\n",
            "|    time_elapsed    | 15443     |\n",
            "|    total_timesteps | 495616    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 243         |\n",
            "|    time_elapsed         | 15485       |\n",
            "|    total_timesteps      | 497664      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012211252 |\n",
            "|    clip_fraction        | 0.129       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.49       |\n",
            "|    explained_variance   | 0.879       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.04e+04    |\n",
            "|    n_updates            | 2420        |\n",
            "|    policy_gradient_loss | -0.00254    |\n",
            "|    std                  | 0.406       |\n",
            "|    value_loss           | 1.58e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 244         |\n",
            "|    time_elapsed         | 15528       |\n",
            "|    total_timesteps      | 499712      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013223389 |\n",
            "|    clip_fraction        | 0.127       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.47       |\n",
            "|    explained_variance   | 0.881       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.44e+04    |\n",
            "|    n_updates            | 2430        |\n",
            "|    policy_gradient_loss | -0.00245    |\n",
            "|    std                  | 0.404       |\n",
            "|    value_loss           | 1.58e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=500000, episode_reward=-50210.02 +/- 3.94\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 500000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012089543 |\n",
            "|    clip_fraction        | 0.144       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.49       |\n",
            "|    explained_variance   | 0.884       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 6.21e+04    |\n",
            "|    n_updates            | 2440        |\n",
            "|    policy_gradient_loss | -0.00403    |\n",
            "|    std                  | 0.41        |\n",
            "|    value_loss           | 1.57e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 245       |\n",
            "|    time_elapsed    | 15624     |\n",
            "|    total_timesteps | 501760    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 246         |\n",
            "|    time_elapsed         | 15666       |\n",
            "|    total_timesteps      | 503808      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012515817 |\n",
            "|    clip_fraction        | 0.129       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.52       |\n",
            "|    explained_variance   | 0.883       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 9.48e+04    |\n",
            "|    n_updates            | 2450        |\n",
            "|    policy_gradient_loss | -0.00148    |\n",
            "|    std                  | 0.414       |\n",
            "|    value_loss           | 1.57e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=505000, episode_reward=-50204.37 +/- 9.41\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 505000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012555782 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.54       |\n",
            "|    explained_variance   | 0.886       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.76e+04    |\n",
            "|    n_updates            | 2460        |\n",
            "|    policy_gradient_loss | -0.000918   |\n",
            "|    std                  | 0.414       |\n",
            "|    value_loss           | 1.57e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 247       |\n",
            "|    time_elapsed    | 15763     |\n",
            "|    total_timesteps | 505856    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 248         |\n",
            "|    time_elapsed         | 15805       |\n",
            "|    total_timesteps      | 507904      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008789505 |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.54       |\n",
            "|    explained_variance   | 0.885       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 6.24e+04    |\n",
            "|    n_updates            | 2470        |\n",
            "|    policy_gradient_loss | -0.000942   |\n",
            "|    std                  | 0.416       |\n",
            "|    value_loss           | 1.58e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 249         |\n",
            "|    time_elapsed         | 15848       |\n",
            "|    total_timesteps      | 509952      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019863669 |\n",
            "|    clip_fraction        | 0.18        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.53       |\n",
            "|    explained_variance   | 0.881       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.84e+04    |\n",
            "|    n_updates            | 2480        |\n",
            "|    policy_gradient_loss | 0.00425     |\n",
            "|    std                  | 0.413       |\n",
            "|    value_loss           | 1.38e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=510000, episode_reward=-50204.69 +/- 9.29\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 510000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010543769 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.52       |\n",
            "|    explained_variance   | 0.912       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 6.3e+04     |\n",
            "|    n_updates            | 2490        |\n",
            "|    policy_gradient_loss | -0.000847   |\n",
            "|    std                  | 0.413       |\n",
            "|    value_loss           | 1.26e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 250       |\n",
            "|    time_elapsed    | 15945     |\n",
            "|    total_timesteps | 512000    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 251         |\n",
            "|    time_elapsed         | 15988       |\n",
            "|    total_timesteps      | 514048      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012416378 |\n",
            "|    clip_fraction        | 0.136       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.52       |\n",
            "|    explained_variance   | 0.921       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.26e+04    |\n",
            "|    n_updates            | 2500        |\n",
            "|    policy_gradient_loss | -0.00456    |\n",
            "|    std                  | 0.413       |\n",
            "|    value_loss           | 1.26e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=515000, episode_reward=-50213.86 +/- 15.04\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 515000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009588056 |\n",
            "|    clip_fraction        | 0.105       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.52       |\n",
            "|    explained_variance   | 0.938       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.57e+04    |\n",
            "|    n_updates            | 2510        |\n",
            "|    policy_gradient_loss | -0.00215    |\n",
            "|    std                  | 0.412       |\n",
            "|    value_loss           | 1.15e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 252       |\n",
            "|    time_elapsed    | 16084     |\n",
            "|    total_timesteps | 516096    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 253         |\n",
            "|    time_elapsed         | 16127       |\n",
            "|    total_timesteps      | 518144      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012244896 |\n",
            "|    clip_fraction        | 0.133       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.51       |\n",
            "|    explained_variance   | 0.94        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.9e+04     |\n",
            "|    n_updates            | 2520        |\n",
            "|    policy_gradient_loss | -0.000534   |\n",
            "|    std                  | 0.408       |\n",
            "|    value_loss           | 1.15e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=520000, episode_reward=-50204.71 +/- 16.11\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 520000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012292283 |\n",
            "|    clip_fraction        | 0.136       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.49       |\n",
            "|    explained_variance   | 0.943       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.24e+04    |\n",
            "|    n_updates            | 2530        |\n",
            "|    policy_gradient_loss | -0.00621    |\n",
            "|    std                  | 0.407       |\n",
            "|    value_loss           | 1.13e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 254       |\n",
            "|    time_elapsed    | 16225     |\n",
            "|    total_timesteps | 520192    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 255         |\n",
            "|    time_elapsed         | 16267       |\n",
            "|    total_timesteps      | 522240      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014892388 |\n",
            "|    clip_fraction        | 0.158       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.48       |\n",
            "|    explained_variance   | 0.946       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.84e+04    |\n",
            "|    n_updates            | 2540        |\n",
            "|    policy_gradient_loss | -0.00663    |\n",
            "|    std                  | 0.408       |\n",
            "|    value_loss           | 1.12e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 256          |\n",
            "|    time_elapsed         | 16310        |\n",
            "|    total_timesteps      | 524288       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0126811545 |\n",
            "|    clip_fraction        | 0.13         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.49        |\n",
            "|    explained_variance   | 0.948        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 4.92e+04     |\n",
            "|    n_updates            | 2550         |\n",
            "|    policy_gradient_loss | -0.00285     |\n",
            "|    std                  | 0.411        |\n",
            "|    value_loss           | 1.12e+05     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=525000, episode_reward=-50208.41 +/- 8.33\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 525000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013534103 |\n",
            "|    clip_fraction        | 0.127       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.49       |\n",
            "|    explained_variance   | 0.95        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.61e+04    |\n",
            "|    n_updates            | 2560        |\n",
            "|    policy_gradient_loss | -0.00608    |\n",
            "|    std                  | 0.41        |\n",
            "|    value_loss           | 1.11e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 257       |\n",
            "|    time_elapsed    | 16407     |\n",
            "|    total_timesteps | 526336    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 258         |\n",
            "|    time_elapsed         | 16450       |\n",
            "|    total_timesteps      | 528384      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011132027 |\n",
            "|    clip_fraction        | 0.136       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.49       |\n",
            "|    explained_variance   | 0.955       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.87e+04    |\n",
            "|    n_updates            | 2570        |\n",
            "|    policy_gradient_loss | -0.0066     |\n",
            "|    std                  | 0.411       |\n",
            "|    value_loss           | 1.07e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=530000, episode_reward=-50200.82 +/- 6.81\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 530000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012804812 |\n",
            "|    clip_fraction        | 0.151       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.48       |\n",
            "|    explained_variance   | 0.957       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.67e+04    |\n",
            "|    n_updates            | 2580        |\n",
            "|    policy_gradient_loss | -0.00709    |\n",
            "|    std                  | 0.407       |\n",
            "|    value_loss           | 1.05e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 259       |\n",
            "|    time_elapsed    | 16547     |\n",
            "|    total_timesteps | 530432    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 260         |\n",
            "|    time_elapsed         | 16591       |\n",
            "|    total_timesteps      | 532480      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009175954 |\n",
            "|    clip_fraction        | 0.1         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.47       |\n",
            "|    explained_variance   | 0.951       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.96e+04    |\n",
            "|    n_updates            | 2590        |\n",
            "|    policy_gradient_loss | -0.00413    |\n",
            "|    std                  | 0.406       |\n",
            "|    value_loss           | 1.14e+05    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 261          |\n",
            "|    time_elapsed         | 16634        |\n",
            "|    total_timesteps      | 534528       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0102777835 |\n",
            "|    clip_fraction        | 0.127        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.47        |\n",
            "|    explained_variance   | 0.871        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 3.13e+04     |\n",
            "|    n_updates            | 2600         |\n",
            "|    policy_gradient_loss | 0.00599      |\n",
            "|    std                  | 0.407        |\n",
            "|    value_loss           | 7.03e+04     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=535000, episode_reward=-50200.98 +/- 6.87\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 535000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015194483 |\n",
            "|    clip_fraction        | 0.137       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.44       |\n",
            "|    explained_variance   | 0.961       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.98e+04    |\n",
            "|    n_updates            | 2610        |\n",
            "|    policy_gradient_loss | -0.00196    |\n",
            "|    std                  | 0.4         |\n",
            "|    value_loss           | 1.03e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 262       |\n",
            "|    time_elapsed    | 16732     |\n",
            "|    total_timesteps | 536576    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 32         |\n",
            "|    iterations           | 263        |\n",
            "|    time_elapsed         | 16774      |\n",
            "|    total_timesteps      | 538624     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01212916 |\n",
            "|    clip_fraction        | 0.163      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.44      |\n",
            "|    explained_variance   | 0.962      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 5.16e+04   |\n",
            "|    n_updates            | 2620       |\n",
            "|    policy_gradient_loss | -0.00483   |\n",
            "|    std                  | 0.402      |\n",
            "|    value_loss           | 1.02e+05   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=540000, episode_reward=-50210.42 +/- 13.94\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 540000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011704458 |\n",
            "|    clip_fraction        | 0.128       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.44       |\n",
            "|    explained_variance   | 0.96        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 6.9e+04     |\n",
            "|    n_updates            | 2630        |\n",
            "|    policy_gradient_loss | -0.00333    |\n",
            "|    std                  | 0.4         |\n",
            "|    value_loss           | 1.05e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 264       |\n",
            "|    time_elapsed    | 16871     |\n",
            "|    total_timesteps | 540672    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 265         |\n",
            "|    time_elapsed         | 16915       |\n",
            "|    total_timesteps      | 542720      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012193395 |\n",
            "|    clip_fraction        | 0.139       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.45       |\n",
            "|    explained_variance   | 0.966       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.37e+04    |\n",
            "|    n_updates            | 2640        |\n",
            "|    policy_gradient_loss | -0.00215    |\n",
            "|    std                  | 0.403       |\n",
            "|    value_loss           | 1.01e+05    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 266         |\n",
            "|    time_elapsed         | 16958       |\n",
            "|    total_timesteps      | 544768      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010550132 |\n",
            "|    clip_fraction        | 0.132       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.42       |\n",
            "|    explained_variance   | 0.966       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.47e+04    |\n",
            "|    n_updates            | 2650        |\n",
            "|    policy_gradient_loss | -0.00471    |\n",
            "|    std                  | 0.396       |\n",
            "|    value_loss           | 9.96e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=545000, episode_reward=-50200.24 +/- 14.59\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 545000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010294501 |\n",
            "|    clip_fraction        | 0.147       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.39       |\n",
            "|    explained_variance   | 0.967       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.95e+04    |\n",
            "|    n_updates            | 2660        |\n",
            "|    policy_gradient_loss | -0.00537    |\n",
            "|    std                  | 0.393       |\n",
            "|    value_loss           | 9.87e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 267       |\n",
            "|    time_elapsed    | 17056     |\n",
            "|    total_timesteps | 546816    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 268         |\n",
            "|    time_elapsed         | 17099       |\n",
            "|    total_timesteps      | 548864      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010931824 |\n",
            "|    clip_fraction        | 0.139       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.38       |\n",
            "|    explained_variance   | 0.967       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 6.31e+04    |\n",
            "|    n_updates            | 2670        |\n",
            "|    policy_gradient_loss | -0.00555    |\n",
            "|    std                  | 0.393       |\n",
            "|    value_loss           | 1.01e+05    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=550000, episode_reward=-50198.92 +/- 18.30\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 550000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015355364 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.37       |\n",
            "|    explained_variance   | 0.969       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 6.87e+04    |\n",
            "|    n_updates            | 2680        |\n",
            "|    policy_gradient_loss | -0.0104     |\n",
            "|    std                  | 0.392       |\n",
            "|    value_loss           | 9.86e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 269       |\n",
            "|    time_elapsed    | 17197     |\n",
            "|    total_timesteps | 550912    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 32         |\n",
            "|    iterations           | 270        |\n",
            "|    time_elapsed         | 17240      |\n",
            "|    total_timesteps      | 552960     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01209229 |\n",
            "|    clip_fraction        | 0.139      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.36      |\n",
            "|    explained_variance   | 0.969      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 3.26e+04   |\n",
            "|    n_updates            | 2690       |\n",
            "|    policy_gradient_loss | -0.00434   |\n",
            "|    std                  | 0.393      |\n",
            "|    value_loss           | 9.83e+04   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=555000, episode_reward=-50212.26 +/- 4.41\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 555000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010535765 |\n",
            "|    clip_fraction        | 0.135       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.33       |\n",
            "|    explained_variance   | 0.97        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.39e+04    |\n",
            "|    n_updates            | 2700        |\n",
            "|    policy_gradient_loss | -0.00239    |\n",
            "|    std                  | 0.385       |\n",
            "|    value_loss           | 9.79e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 271       |\n",
            "|    time_elapsed    | 17337     |\n",
            "|    total_timesteps | 555008    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 272         |\n",
            "|    time_elapsed         | 17380       |\n",
            "|    total_timesteps      | 557056      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014821881 |\n",
            "|    clip_fraction        | 0.16        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.31       |\n",
            "|    explained_variance   | 0.966       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.6e+04     |\n",
            "|    n_updates            | 2710        |\n",
            "|    policy_gradient_loss | -0.00363    |\n",
            "|    std                  | 0.385       |\n",
            "|    value_loss           | 9.93e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 273         |\n",
            "|    time_elapsed         | 17422       |\n",
            "|    total_timesteps      | 559104      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012767874 |\n",
            "|    clip_fraction        | 0.146       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.31       |\n",
            "|    explained_variance   | 0.971       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.85e+04    |\n",
            "|    n_updates            | 2720        |\n",
            "|    policy_gradient_loss | 0.000367    |\n",
            "|    std                  | 0.384       |\n",
            "|    value_loss           | 9.37e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=560000, episode_reward=-50214.07 +/- 7.20\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 560000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014930654 |\n",
            "|    clip_fraction        | 0.146       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0.974       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.46e+04    |\n",
            "|    n_updates            | 2730        |\n",
            "|    policy_gradient_loss | -0.00157    |\n",
            "|    std                  | 0.383       |\n",
            "|    value_loss           | 9e+04       |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 274       |\n",
            "|    time_elapsed    | 17520     |\n",
            "|    total_timesteps | 561152    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 275         |\n",
            "|    time_elapsed         | 17563       |\n",
            "|    total_timesteps      | 563200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015902577 |\n",
            "|    clip_fraction        | 0.161       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.32       |\n",
            "|    explained_variance   | 0.973       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.69e+04    |\n",
            "|    n_updates            | 2740        |\n",
            "|    policy_gradient_loss | -0.00889    |\n",
            "|    std                  | 0.388       |\n",
            "|    value_loss           | 8.9e+04     |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=565000, episode_reward=-50208.35 +/- 10.33\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 565000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0114662815 |\n",
            "|    clip_fraction        | 0.15         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.33        |\n",
            "|    explained_variance   | 0.943        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 4.41e+04     |\n",
            "|    n_updates            | 2750         |\n",
            "|    policy_gradient_loss | 0.0054       |\n",
            "|    std                  | 0.387        |\n",
            "|    value_loss           | 1.05e+05     |\n",
            "------------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 276       |\n",
            "|    time_elapsed    | 17661     |\n",
            "|    total_timesteps | 565248    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 277         |\n",
            "|    time_elapsed         | 17704       |\n",
            "|    total_timesteps      | 567296      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009894473 |\n",
            "|    clip_fraction        | 0.0797      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.32       |\n",
            "|    explained_variance   | 0.961       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.34e+04    |\n",
            "|    n_updates            | 2760        |\n",
            "|    policy_gradient_loss | 0.000624    |\n",
            "|    std                  | 0.387       |\n",
            "|    value_loss           | 9.83e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 278          |\n",
            "|    time_elapsed         | 17746        |\n",
            "|    total_timesteps      | 569344       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0057588825 |\n",
            "|    clip_fraction        | 0.0707       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.32        |\n",
            "|    explained_variance   | 0.945        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 2.27e+04     |\n",
            "|    n_updates            | 2770         |\n",
            "|    policy_gradient_loss | -0.000481    |\n",
            "|    std                  | 0.387        |\n",
            "|    value_loss           | 8.06e+04     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=570000, episode_reward=-50211.87 +/- 7.96\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 570000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014218783 |\n",
            "|    clip_fraction        | 0.183       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.31       |\n",
            "|    explained_variance   | 0.972       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.36e+04    |\n",
            "|    n_updates            | 2780        |\n",
            "|    policy_gradient_loss | -0.00382    |\n",
            "|    std                  | 0.383       |\n",
            "|    value_loss           | 8.53e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 279       |\n",
            "|    time_elapsed    | 17844     |\n",
            "|    total_timesteps | 571392    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 280         |\n",
            "|    time_elapsed         | 17888       |\n",
            "|    total_timesteps      | 573440      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010170196 |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.29       |\n",
            "|    explained_variance   | 0.971       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.58e+04    |\n",
            "|    n_updates            | 2790        |\n",
            "|    policy_gradient_loss | -0.00307    |\n",
            "|    std                  | 0.383       |\n",
            "|    value_loss           | 8.48e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=575000, episode_reward=-50199.06 +/- 10.25\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 575000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011821708 |\n",
            "|    clip_fraction        | 0.14        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.28       |\n",
            "|    explained_variance   | 0.973       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.29e+04    |\n",
            "|    n_updates            | 2800        |\n",
            "|    policy_gradient_loss | -0.0027     |\n",
            "|    std                  | 0.383       |\n",
            "|    value_loss           | 8.42e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 281       |\n",
            "|    time_elapsed    | 17986     |\n",
            "|    total_timesteps | 575488    |\n",
            "----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | -5.02e+04    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 32           |\n",
            "|    iterations           | 282          |\n",
            "|    time_elapsed         | 18028        |\n",
            "|    total_timesteps      | 577536       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0135448305 |\n",
            "|    clip_fraction        | 0.174        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.27        |\n",
            "|    explained_variance   | 0.975        |\n",
            "|    learning_rate        | 0.0005       |\n",
            "|    loss                 | 4.51e+04     |\n",
            "|    n_updates            | 2810         |\n",
            "|    policy_gradient_loss | -0.00305     |\n",
            "|    std                  | 0.38         |\n",
            "|    value_loss           | 8.32e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 283         |\n",
            "|    time_elapsed         | 18071       |\n",
            "|    total_timesteps      | 579584      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012727374 |\n",
            "|    clip_fraction        | 0.158       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.25       |\n",
            "|    explained_variance   | 0.975       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.44e+04    |\n",
            "|    n_updates            | 2820        |\n",
            "|    policy_gradient_loss | -0.00199    |\n",
            "|    std                  | 0.377       |\n",
            "|    value_loss           | 8.19e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=580000, episode_reward=-50201.22 +/- 7.13\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 580000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014979268 |\n",
            "|    clip_fraction        | 0.164       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | 0.974       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.52e+04    |\n",
            "|    n_updates            | 2830        |\n",
            "|    policy_gradient_loss | -0.00073    |\n",
            "|    std                  | 0.375       |\n",
            "|    value_loss           | 8.35e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 284       |\n",
            "|    time_elapsed    | 18170     |\n",
            "|    total_timesteps | 581632    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 285         |\n",
            "|    time_elapsed         | 18213       |\n",
            "|    total_timesteps      | 583680      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014154441 |\n",
            "|    clip_fraction        | 0.175       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | 0.976       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.15e+04    |\n",
            "|    n_updates            | 2840        |\n",
            "|    policy_gradient_loss | -0.00349    |\n",
            "|    std                  | 0.377       |\n",
            "|    value_loss           | 8.02e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=585000, episode_reward=-50215.89 +/- 10.72\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 585000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012542648 |\n",
            "|    clip_fraction        | 0.152       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0.976       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.74e+04    |\n",
            "|    n_updates            | 2850        |\n",
            "|    policy_gradient_loss | -0.00327    |\n",
            "|    std                  | 0.374       |\n",
            "|    value_loss           | 8.06e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 286       |\n",
            "|    time_elapsed    | 18311     |\n",
            "|    total_timesteps | 585728    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 32         |\n",
            "|    iterations           | 287        |\n",
            "|    time_elapsed         | 18353      |\n",
            "|    total_timesteps      | 587776     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01335671 |\n",
            "|    clip_fraction        | 0.151      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.18      |\n",
            "|    explained_variance   | 0.978      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 3.55e+04   |\n",
            "|    n_updates            | 2860       |\n",
            "|    policy_gradient_loss | -0.00653   |\n",
            "|    std                  | 0.366      |\n",
            "|    value_loss           | 7.85e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 288         |\n",
            "|    time_elapsed         | 18397       |\n",
            "|    total_timesteps      | 589824      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018994445 |\n",
            "|    clip_fraction        | 0.186       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 0.978       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.63e+04    |\n",
            "|    n_updates            | 2870        |\n",
            "|    policy_gradient_loss | -0.006      |\n",
            "|    std                  | 0.369       |\n",
            "|    value_loss           | 7.89e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=590000, episode_reward=-50213.68 +/- 14.94\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 590000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016773382 |\n",
            "|    clip_fraction        | 0.216       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 0.978       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.09e+04    |\n",
            "|    n_updates            | 2880        |\n",
            "|    policy_gradient_loss | -0.00948    |\n",
            "|    std                  | 0.366       |\n",
            "|    value_loss           | 7.82e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 32        |\n",
            "|    iterations      | 289       |\n",
            "|    time_elapsed    | 18494     |\n",
            "|    total_timesteps | 591872    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 290         |\n",
            "|    time_elapsed         | 18537       |\n",
            "|    total_timesteps      | 593920      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012796211 |\n",
            "|    clip_fraction        | 0.167       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 0.978       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.8e+04     |\n",
            "|    n_updates            | 2890        |\n",
            "|    policy_gradient_loss | -0.00227    |\n",
            "|    std                  | 0.368       |\n",
            "|    value_loss           | 7.78e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=595000, episode_reward=-50197.71 +/- 14.78\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 595000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016564514 |\n",
            "|    clip_fraction        | 0.158       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.16       |\n",
            "|    explained_variance   | 0.979       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.47e+04    |\n",
            "|    n_updates            | 2900        |\n",
            "|    policy_gradient_loss | -0.00455    |\n",
            "|    std                  | 0.368       |\n",
            "|    value_loss           | 7.77e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 291       |\n",
            "|    time_elapsed    | 18637     |\n",
            "|    total_timesteps | 595968    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 292         |\n",
            "|    time_elapsed         | 18679       |\n",
            "|    total_timesteps      | 598016      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014695032 |\n",
            "|    clip_fraction        | 0.155       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.16       |\n",
            "|    explained_variance   | 0.977       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.22e+04    |\n",
            "|    n_updates            | 2910        |\n",
            "|    policy_gradient_loss | -0.00632    |\n",
            "|    std                  | 0.37        |\n",
            "|    value_loss           | 7.77e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=600000, episode_reward=-50213.91 +/- 8.71\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 600000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03329388 |\n",
            "|    clip_fraction        | 0.294      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.16      |\n",
            "|    explained_variance   | 0.837      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 2.79e+04   |\n",
            "|    n_updates            | 2920       |\n",
            "|    policy_gradient_loss | 0.0175     |\n",
            "|    std                  | 0.37       |\n",
            "|    value_loss           | 1.39e+05   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 293       |\n",
            "|    time_elapsed    | 18777     |\n",
            "|    total_timesteps | 600064    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 294         |\n",
            "|    time_elapsed         | 18820       |\n",
            "|    total_timesteps      | 602112      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015025197 |\n",
            "|    clip_fraction        | 0.194       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | 0.981       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.54e+04    |\n",
            "|    n_updates            | 2930        |\n",
            "|    policy_gradient_loss | -0.00613    |\n",
            "|    std                  | 0.371       |\n",
            "|    value_loss           | 7.29e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 295         |\n",
            "|    time_elapsed         | 18863       |\n",
            "|    total_timesteps      | 604160      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015806831 |\n",
            "|    clip_fraction        | 0.173       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0.981       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.3e+04     |\n",
            "|    n_updates            | 2940        |\n",
            "|    policy_gradient_loss | -0.00642    |\n",
            "|    std                  | 0.373       |\n",
            "|    value_loss           | 7.05e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=605000, episode_reward=-50206.57 +/- 6.88\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 605000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013851853 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 6.86e+04    |\n",
            "|    n_updates            | 2950        |\n",
            "|    policy_gradient_loss | -0.00824    |\n",
            "|    std                  | 0.371       |\n",
            "|    value_loss           | 6.97e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 296       |\n",
            "|    time_elapsed    | 18961     |\n",
            "|    total_timesteps | 606208    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 297         |\n",
            "|    time_elapsed         | 19004       |\n",
            "|    total_timesteps      | 608256      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017676437 |\n",
            "|    clip_fraction        | 0.173       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0.98        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.55e+04    |\n",
            "|    n_updates            | 2960        |\n",
            "|    policy_gradient_loss | -0.0033     |\n",
            "|    std                  | 0.372       |\n",
            "|    value_loss           | 6.97e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=610000, episode_reward=-50213.84 +/- 9.18\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 610000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009446306 |\n",
            "|    clip_fraction        | 0.141       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0.941       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.05e+04    |\n",
            "|    n_updates            | 2970        |\n",
            "|    policy_gradient_loss | 0.00519     |\n",
            "|    std                  | 0.371       |\n",
            "|    value_loss           | 9.04e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 298       |\n",
            "|    time_elapsed    | 19101     |\n",
            "|    total_timesteps | 610304    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 299         |\n",
            "|    time_elapsed         | 19144       |\n",
            "|    total_timesteps      | 612352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014462147 |\n",
            "|    clip_fraction        | 0.188       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.2        |\n",
            "|    explained_variance   | 0.981       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.88e+04    |\n",
            "|    n_updates            | 2980        |\n",
            "|    policy_gradient_loss | -0.00439    |\n",
            "|    std                  | 0.375       |\n",
            "|    value_loss           | 6.76e+04    |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1e+03     |\n",
            "|    ep_rew_mean          | -5.02e+04 |\n",
            "| time/                   |           |\n",
            "|    fps                  | 32        |\n",
            "|    iterations           | 300       |\n",
            "|    time_elapsed         | 19187     |\n",
            "|    total_timesteps      | 614400    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.017468  |\n",
            "|    clip_fraction        | 0.185     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.17     |\n",
            "|    explained_variance   | 0.982     |\n",
            "|    learning_rate        | 0.0005    |\n",
            "|    loss                 | 2.29e+04  |\n",
            "|    n_updates            | 2990      |\n",
            "|    policy_gradient_loss | -0.00657  |\n",
            "|    std                  | 0.368     |\n",
            "|    value_loss           | 6.65e+04  |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=615000, episode_reward=-50202.49 +/- 9.23\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 615000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016555432 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.16       |\n",
            "|    explained_variance   | 0.974       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.77e+04    |\n",
            "|    n_updates            | 3000        |\n",
            "|    policy_gradient_loss | 0.000994    |\n",
            "|    std                  | 0.369       |\n",
            "|    value_loss           | 6.97e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 301       |\n",
            "|    time_elapsed    | 19284     |\n",
            "|    total_timesteps | 616448    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 302         |\n",
            "|    time_elapsed         | 19327       |\n",
            "|    total_timesteps      | 618496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015774045 |\n",
            "|    clip_fraction        | 0.186       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.7e+04     |\n",
            "|    n_updates            | 3010        |\n",
            "|    policy_gradient_loss | -0.00266    |\n",
            "|    std                  | 0.371       |\n",
            "|    value_loss           | 6.55e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=620000, episode_reward=-50206.62 +/- 12.64\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 620000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014012664 |\n",
            "|    clip_fraction        | 0.161       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.79e+04    |\n",
            "|    n_updates            | 3020        |\n",
            "|    policy_gradient_loss | -0.00634    |\n",
            "|    std                  | 0.372       |\n",
            "|    value_loss           | 6.41e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 303       |\n",
            "|    time_elapsed    | 19424     |\n",
            "|    total_timesteps | 620544    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 304         |\n",
            "|    time_elapsed         | 19467       |\n",
            "|    total_timesteps      | 622592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015694069 |\n",
            "|    clip_fraction        | 0.196       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.16       |\n",
            "|    explained_variance   | 0.983       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.82e+04    |\n",
            "|    n_updates            | 3030        |\n",
            "|    policy_gradient_loss | -0.0102     |\n",
            "|    std                  | 0.372       |\n",
            "|    value_loss           | 6.33e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 305         |\n",
            "|    time_elapsed         | 19510       |\n",
            "|    total_timesteps      | 624640      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015481159 |\n",
            "|    clip_fraction        | 0.19        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.16       |\n",
            "|    explained_variance   | 0.984       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.4e+04     |\n",
            "|    n_updates            | 3040        |\n",
            "|    policy_gradient_loss | -0.00315    |\n",
            "|    std                  | 0.373       |\n",
            "|    value_loss           | 6.29e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=625000, episode_reward=-50208.03 +/- 17.57\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 625000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016243175 |\n",
            "|    clip_fraction        | 0.179       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | 0.984       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.94e+04    |\n",
            "|    n_updates            | 3050        |\n",
            "|    policy_gradient_loss | -0.00171    |\n",
            "|    std                  | 0.373       |\n",
            "|    value_loss           | 6.21e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 306       |\n",
            "|    time_elapsed    | 19607     |\n",
            "|    total_timesteps | 626688    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 307         |\n",
            "|    time_elapsed         | 19650       |\n",
            "|    total_timesteps      | 628736      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019528883 |\n",
            "|    clip_fraction        | 0.223       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 0.984       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.53e+04    |\n",
            "|    n_updates            | 3060        |\n",
            "|    policy_gradient_loss | -0.00867    |\n",
            "|    std                  | 0.371       |\n",
            "|    value_loss           | 6.09e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=630000, episode_reward=-50203.22 +/- 12.77\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 630000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017549077 |\n",
            "|    clip_fraction        | 0.185       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.47e+04    |\n",
            "|    n_updates            | 3070        |\n",
            "|    policy_gradient_loss | -0.0071     |\n",
            "|    std                  | 0.374       |\n",
            "|    value_loss           | 6.03e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 308       |\n",
            "|    time_elapsed    | 19748     |\n",
            "|    total_timesteps | 630784    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 309         |\n",
            "|    time_elapsed         | 19791       |\n",
            "|    total_timesteps      | 632832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018691426 |\n",
            "|    clip_fraction        | 0.182       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.21       |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.88e+04    |\n",
            "|    n_updates            | 3080        |\n",
            "|    policy_gradient_loss | -0.00602    |\n",
            "|    std                  | 0.38        |\n",
            "|    value_loss           | 6.01e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 32          |\n",
            "|    iterations           | 310         |\n",
            "|    time_elapsed         | 19834       |\n",
            "|    total_timesteps      | 634880      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014325168 |\n",
            "|    clip_fraction        | 0.157       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.98e+04    |\n",
            "|    n_updates            | 3090        |\n",
            "|    policy_gradient_loss | -0.0097     |\n",
            "|    std                  | 0.379       |\n",
            "|    value_loss           | 5.96e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=635000, episode_reward=-50198.97 +/- 8.26\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 635000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015721433 |\n",
            "|    clip_fraction        | 0.169       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.2        |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.21e+04    |\n",
            "|    n_updates            | 3100        |\n",
            "|    policy_gradient_loss | -0.00177    |\n",
            "|    std                  | 0.375       |\n",
            "|    value_loss           | 5.96e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 311       |\n",
            "|    time_elapsed    | 19931     |\n",
            "|    total_timesteps | 636928    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 31         |\n",
            "|    iterations           | 312        |\n",
            "|    time_elapsed         | 19974      |\n",
            "|    total_timesteps      | 638976     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01086322 |\n",
            "|    clip_fraction        | 0.159      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.2       |\n",
            "|    explained_variance   | 0.985      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 3.58e+04   |\n",
            "|    n_updates            | 3110       |\n",
            "|    policy_gradient_loss | -0.00221   |\n",
            "|    std                  | 0.376      |\n",
            "|    value_loss           | 5.99e+04   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=640000, episode_reward=-50208.50 +/- 8.38\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 640000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015333623 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.19       |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.42e+04    |\n",
            "|    n_updates            | 3120        |\n",
            "|    policy_gradient_loss | -0.00865    |\n",
            "|    std                  | 0.375       |\n",
            "|    value_loss           | 6.01e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 313       |\n",
            "|    time_elapsed    | 20072     |\n",
            "|    total_timesteps | 641024    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 314         |\n",
            "|    time_elapsed         | 20115       |\n",
            "|    total_timesteps      | 643072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017560847 |\n",
            "|    clip_fraction        | 0.179       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.19       |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.29e+04    |\n",
            "|    n_updates            | 3130        |\n",
            "|    policy_gradient_loss | -0.00908    |\n",
            "|    std                  | 0.377       |\n",
            "|    value_loss           | 5.63e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=645000, episode_reward=-50206.75 +/- 15.91\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 645000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016882483 |\n",
            "|    clip_fraction        | 0.181       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0.988       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.78e+04    |\n",
            "|    n_updates            | 3140        |\n",
            "|    policy_gradient_loss | -0.00682    |\n",
            "|    std                  | 0.377       |\n",
            "|    value_loss           | 5.47e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 315       |\n",
            "|    time_elapsed    | 20214     |\n",
            "|    total_timesteps | 645120    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 316         |\n",
            "|    time_elapsed         | 20257       |\n",
            "|    total_timesteps      | 647168      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015320134 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.58e+04    |\n",
            "|    n_updates            | 3150        |\n",
            "|    policy_gradient_loss | -0.00664    |\n",
            "|    std                  | 0.377       |\n",
            "|    value_loss           | 5.35e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 317         |\n",
            "|    time_elapsed         | 20301       |\n",
            "|    total_timesteps      | 649216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018262323 |\n",
            "|    clip_fraction        | 0.186       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.62e+04    |\n",
            "|    n_updates            | 3160        |\n",
            "|    policy_gradient_loss | -0.0076     |\n",
            "|    std                  | 0.379       |\n",
            "|    value_loss           | 5.29e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=650000, episode_reward=-50204.72 +/- 14.11\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 650000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016524378 |\n",
            "|    clip_fraction        | 0.198       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.83e+04    |\n",
            "|    n_updates            | 3170        |\n",
            "|    policy_gradient_loss | -0.00567    |\n",
            "|    std                  | 0.377       |\n",
            "|    value_loss           | 5.11e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 318       |\n",
            "|    time_elapsed    | 20400     |\n",
            "|    total_timesteps | 651264    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 319         |\n",
            "|    time_elapsed         | 20443       |\n",
            "|    total_timesteps      | 653312      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015150903 |\n",
            "|    clip_fraction        | 0.177       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 0.988       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.49e+04    |\n",
            "|    n_updates            | 3180        |\n",
            "|    policy_gradient_loss | -0.00557    |\n",
            "|    std                  | 0.372       |\n",
            "|    value_loss           | 5.14e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=655000, episode_reward=-50211.78 +/- 7.42\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 655000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014740724 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.12       |\n",
            "|    explained_variance   | 0.988       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.12e+04    |\n",
            "|    n_updates            | 3190        |\n",
            "|    policy_gradient_loss | -0.00524    |\n",
            "|    std                  | 0.37        |\n",
            "|    value_loss           | 5.06e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 320       |\n",
            "|    time_elapsed    | 20540     |\n",
            "|    total_timesteps | 655360    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 321         |\n",
            "|    time_elapsed         | 20583       |\n",
            "|    total_timesteps      | 657408      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017497865 |\n",
            "|    clip_fraction        | 0.177       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.11       |\n",
            "|    explained_variance   | 0.98        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.72e+04    |\n",
            "|    n_updates            | 3200        |\n",
            "|    policy_gradient_loss | 0.00464     |\n",
            "|    std                  | 0.367       |\n",
            "|    value_loss           | 5.42e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 31         |\n",
            "|    iterations           | 322        |\n",
            "|    time_elapsed         | 20626      |\n",
            "|    total_timesteps      | 659456     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03710674 |\n",
            "|    clip_fraction        | 0.347      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.1       |\n",
            "|    explained_variance   | 0.94       |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.93e+04   |\n",
            "|    n_updates            | 3210       |\n",
            "|    policy_gradient_loss | 0.0312     |\n",
            "|    std                  | 0.367      |\n",
            "|    value_loss           | 3.98e+04   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=660000, episode_reward=-50216.07 +/- 10.57\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 660000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017057443 |\n",
            "|    clip_fraction        | 0.214       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | 0.988       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.32e+04    |\n",
            "|    n_updates            | 3220        |\n",
            "|    policy_gradient_loss | 0.00275     |\n",
            "|    std                  | 0.368       |\n",
            "|    value_loss           | 4.92e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 323       |\n",
            "|    time_elapsed    | 20724     |\n",
            "|    total_timesteps | 661504    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 324         |\n",
            "|    time_elapsed         | 20767       |\n",
            "|    total_timesteps      | 663552      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015134819 |\n",
            "|    clip_fraction        | 0.199       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.11       |\n",
            "|    explained_variance   | 0.988       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.61e+04    |\n",
            "|    n_updates            | 3230        |\n",
            "|    policy_gradient_loss | -0.00762    |\n",
            "|    std                  | 0.367       |\n",
            "|    value_loss           | 4.81e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=665000, episode_reward=-50214.22 +/- 4.49\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 665000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015047091 |\n",
            "|    clip_fraction        | 0.183       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.989       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.38e+04    |\n",
            "|    n_updates            | 3240        |\n",
            "|    policy_gradient_loss | -0.00594    |\n",
            "|    std                  | 0.366       |\n",
            "|    value_loss           | 4.81e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 325       |\n",
            "|    time_elapsed    | 20865     |\n",
            "|    total_timesteps | 665600    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 326         |\n",
            "|    time_elapsed         | 20908       |\n",
            "|    total_timesteps      | 667648      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016270317 |\n",
            "|    clip_fraction        | 0.2         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.989       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.51e+04    |\n",
            "|    n_updates            | 3250        |\n",
            "|    policy_gradient_loss | -0.00831    |\n",
            "|    std                  | 0.364       |\n",
            "|    value_loss           | 4.7e+04     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 327         |\n",
            "|    time_elapsed         | 20951       |\n",
            "|    total_timesteps      | 669696      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014413226 |\n",
            "|    clip_fraction        | 0.173       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.12       |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.68e+04    |\n",
            "|    n_updates            | 3260        |\n",
            "|    policy_gradient_loss | -0.00754    |\n",
            "|    std                  | 0.37        |\n",
            "|    value_loss           | 4.61e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=670000, episode_reward=-50211.94 +/- 9.69\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 670000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023398641 |\n",
            "|    clip_fraction        | 0.237       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.44e+04    |\n",
            "|    n_updates            | 3270        |\n",
            "|    policy_gradient_loss | -0.00409    |\n",
            "|    std                  | 0.369       |\n",
            "|    value_loss           | 4.7e+04     |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 328       |\n",
            "|    time_elapsed    | 21048     |\n",
            "|    total_timesteps | 671744    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 329         |\n",
            "|    time_elapsed         | 21091       |\n",
            "|    total_timesteps      | 673792      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019388035 |\n",
            "|    clip_fraction        | 0.191       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.14       |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.33e+04    |\n",
            "|    n_updates            | 3280        |\n",
            "|    policy_gradient_loss | 0.00113     |\n",
            "|    std                  | 0.37        |\n",
            "|    value_loss           | 4.8e+04     |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=675000, episode_reward=-50206.45 +/- 17.15\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 675000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016153406 |\n",
            "|    clip_fraction        | 0.191       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.11       |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.41e+04    |\n",
            "|    n_updates            | 3290        |\n",
            "|    policy_gradient_loss | -0.00481    |\n",
            "|    std                  | 0.365       |\n",
            "|    value_loss           | 4.49e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 330       |\n",
            "|    time_elapsed    | 21187     |\n",
            "|    total_timesteps | 675840    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 331         |\n",
            "|    time_elapsed         | 21230       |\n",
            "|    total_timesteps      | 677888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025559105 |\n",
            "|    clip_fraction        | 0.202       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.11       |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.87e+04    |\n",
            "|    n_updates            | 3300        |\n",
            "|    policy_gradient_loss | 0.000218    |\n",
            "|    std                  | 0.366       |\n",
            "|    value_loss           | 4.85e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 332         |\n",
            "|    time_elapsed         | 21273       |\n",
            "|    total_timesteps      | 679936      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011713404 |\n",
            "|    clip_fraction        | 0.16        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.14e+04    |\n",
            "|    n_updates            | 3310        |\n",
            "|    policy_gradient_loss | -0.00422    |\n",
            "|    std                  | 0.371       |\n",
            "|    value_loss           | 4.48e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=680000, episode_reward=-50219.58 +/- 10.81\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 680000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016213093 |\n",
            "|    clip_fraction        | 0.182       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.01e+04    |\n",
            "|    n_updates            | 3320        |\n",
            "|    policy_gradient_loss | -0.00652    |\n",
            "|    std                  | 0.373       |\n",
            "|    value_loss           | 4.5e+04     |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 333       |\n",
            "|    time_elapsed    | 21370     |\n",
            "|    total_timesteps | 681984    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 334         |\n",
            "|    time_elapsed         | 21414       |\n",
            "|    total_timesteps      | 684032      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017612154 |\n",
            "|    clip_fraction        | 0.192       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.44e+04    |\n",
            "|    n_updates            | 3330        |\n",
            "|    policy_gradient_loss | -0.00505    |\n",
            "|    std                  | 0.374       |\n",
            "|    value_loss           | 4.56e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=685000, episode_reward=-50206.06 +/- 11.27\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 685000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015018545 |\n",
            "|    clip_fraction        | 0.178       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.16       |\n",
            "|    explained_variance   | 0.991       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1e+04       |\n",
            "|    n_updates            | 3340        |\n",
            "|    policy_gradient_loss | -0.00524    |\n",
            "|    std                  | 0.371       |\n",
            "|    value_loss           | 4.18e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 335       |\n",
            "|    time_elapsed    | 21510     |\n",
            "|    total_timesteps | 686080    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 336         |\n",
            "|    time_elapsed         | 21553       |\n",
            "|    total_timesteps      | 688128      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017572355 |\n",
            "|    clip_fraction        | 0.193       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0.991       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.33e+04    |\n",
            "|    n_updates            | 3350        |\n",
            "|    policy_gradient_loss | -0.00883    |\n",
            "|    std                  | 0.368       |\n",
            "|    value_loss           | 4.09e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=690000, episode_reward=-50203.30 +/- 17.67\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 690000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016408343 |\n",
            "|    clip_fraction        | 0.206       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.11       |\n",
            "|    explained_variance   | 0.992       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.69e+04    |\n",
            "|    n_updates            | 3360        |\n",
            "|    policy_gradient_loss | -0.00555    |\n",
            "|    std                  | 0.363       |\n",
            "|    value_loss           | 4.03e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 337       |\n",
            "|    time_elapsed    | 21652     |\n",
            "|    total_timesteps | 690176    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 338         |\n",
            "|    time_elapsed         | 21695       |\n",
            "|    total_timesteps      | 692224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013802533 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.991       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.56e+04    |\n",
            "|    n_updates            | 3370        |\n",
            "|    policy_gradient_loss | -0.00612    |\n",
            "|    std                  | 0.362       |\n",
            "|    value_loss           | 3.96e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 339         |\n",
            "|    time_elapsed         | 21738       |\n",
            "|    total_timesteps      | 694272      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015189595 |\n",
            "|    clip_fraction        | 0.18        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.07       |\n",
            "|    explained_variance   | 0.991       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.89e+04    |\n",
            "|    n_updates            | 3380        |\n",
            "|    policy_gradient_loss | -0.00497    |\n",
            "|    std                  | 0.358       |\n",
            "|    value_loss           | 3.91e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=695000, episode_reward=-50210.06 +/- 9.12\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 695000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018194757 |\n",
            "|    clip_fraction        | 0.179       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.08       |\n",
            "|    explained_variance   | 0.992       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.82e+04    |\n",
            "|    n_updates            | 3390        |\n",
            "|    policy_gradient_loss | -0.00524    |\n",
            "|    std                  | 0.363       |\n",
            "|    value_loss           | 3.85e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 340       |\n",
            "|    time_elapsed    | 21835     |\n",
            "|    total_timesteps | 696320    |\n",
            "----------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1e+03     |\n",
            "|    ep_rew_mean          | -5.02e+04 |\n",
            "| time/                   |           |\n",
            "|    fps                  | 31        |\n",
            "|    iterations           | 341       |\n",
            "|    time_elapsed         | 21877     |\n",
            "|    total_timesteps      | 698368    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0149228 |\n",
            "|    clip_fraction        | 0.165     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.1      |\n",
            "|    explained_variance   | 0.991     |\n",
            "|    learning_rate        | 0.0005    |\n",
            "|    loss                 | 1.92e+04  |\n",
            "|    n_updates            | 3400      |\n",
            "|    policy_gradient_loss | -0.0101   |\n",
            "|    std                  | 0.366     |\n",
            "|    value_loss           | 3.77e+04  |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=700000, episode_reward=-50218.17 +/- 13.15\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 700000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01744625 |\n",
            "|    clip_fraction        | 0.215      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.08      |\n",
            "|    explained_variance   | 0.992      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 2.39e+04   |\n",
            "|    n_updates            | 3410       |\n",
            "|    policy_gradient_loss | -0.00212   |\n",
            "|    std                  | 0.359      |\n",
            "|    value_loss           | 3.76e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 342       |\n",
            "|    time_elapsed    | 21976     |\n",
            "|    total_timesteps | 700416    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 343         |\n",
            "|    time_elapsed         | 22019       |\n",
            "|    total_timesteps      | 702464      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016364066 |\n",
            "|    clip_fraction        | 0.218       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.05       |\n",
            "|    explained_variance   | 0.992       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.89e+04    |\n",
            "|    n_updates            | 3420        |\n",
            "|    policy_gradient_loss | -0.00247    |\n",
            "|    std                  | 0.357       |\n",
            "|    value_loss           | 3.66e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 31         |\n",
            "|    iterations           | 344        |\n",
            "|    time_elapsed         | 22062      |\n",
            "|    total_timesteps      | 704512     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01790401 |\n",
            "|    clip_fraction        | 0.193      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.03      |\n",
            "|    explained_variance   | 0.992      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.43e+04   |\n",
            "|    n_updates            | 3430       |\n",
            "|    policy_gradient_loss | -0.00473   |\n",
            "|    std                  | 0.354      |\n",
            "|    value_loss           | 3.6e+04    |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=705000, episode_reward=-50206.60 +/- 19.66\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 705000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03788537 |\n",
            "|    clip_fraction        | 0.298      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.01      |\n",
            "|    explained_variance   | 0.99       |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.34e+04   |\n",
            "|    n_updates            | 3440       |\n",
            "|    policy_gradient_loss | 0.0153     |\n",
            "|    std                  | 0.353      |\n",
            "|    value_loss           | 3.72e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 345       |\n",
            "|    time_elapsed    | 22159     |\n",
            "|    total_timesteps | 706560    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 346         |\n",
            "|    time_elapsed         | 22201       |\n",
            "|    total_timesteps      | 708608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015119575 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.998      |\n",
            "|    explained_variance   | 0.984       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.58e+04    |\n",
            "|    n_updates            | 3450        |\n",
            "|    policy_gradient_loss | 0.00153     |\n",
            "|    std                  | 0.353       |\n",
            "|    value_loss           | 5.25e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=710000, episode_reward=-50199.27 +/- 19.65\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 710000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019193716 |\n",
            "|    clip_fraction        | 0.217       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.992       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.77e+04    |\n",
            "|    n_updates            | 3460        |\n",
            "|    policy_gradient_loss | -0.00238    |\n",
            "|    std                  | 0.355       |\n",
            "|    value_loss           | 3.65e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 347       |\n",
            "|    time_elapsed    | 22299     |\n",
            "|    total_timesteps | 710656    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 348         |\n",
            "|    time_elapsed         | 22343       |\n",
            "|    total_timesteps      | 712704      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016755395 |\n",
            "|    clip_fraction        | 0.214       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1          |\n",
            "|    explained_variance   | 0.992       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.17e+04    |\n",
            "|    n_updates            | 3470        |\n",
            "|    policy_gradient_loss | -0.000238   |\n",
            "|    std                  | 0.353       |\n",
            "|    value_loss           | 3.37e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 31         |\n",
            "|    iterations           | 349        |\n",
            "|    time_elapsed         | 22387      |\n",
            "|    total_timesteps      | 714752     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03411482 |\n",
            "|    clip_fraction        | 0.248      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.997     |\n",
            "|    explained_variance   | 0.933      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.08e+04   |\n",
            "|    n_updates            | 3480       |\n",
            "|    policy_gradient_loss | 0.00964    |\n",
            "|    std                  | 0.353      |\n",
            "|    value_loss           | 1.19e+05   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=715000, episode_reward=-50195.38 +/- 17.38\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 715000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017450664 |\n",
            "|    clip_fraction        | 0.168       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1          |\n",
            "|    explained_variance   | 0.992       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.24e+04    |\n",
            "|    n_updates            | 3490        |\n",
            "|    policy_gradient_loss | -0.000859   |\n",
            "|    std                  | 0.354       |\n",
            "|    value_loss           | 3.33e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 350       |\n",
            "|    time_elapsed    | 22485     |\n",
            "|    total_timesteps | 716800    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 351         |\n",
            "|    time_elapsed         | 22528       |\n",
            "|    total_timesteps      | 718848      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018414084 |\n",
            "|    clip_fraction        | 0.201       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0.993       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.96e+04    |\n",
            "|    n_updates            | 3500        |\n",
            "|    policy_gradient_loss | -0.00931    |\n",
            "|    std                  | 0.356       |\n",
            "|    value_loss           | 3.3e+04     |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=720000, episode_reward=-50194.79 +/- 4.75\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 720000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019203657 |\n",
            "|    clip_fraction        | 0.23        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.993       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.43e+04    |\n",
            "|    n_updates            | 3510        |\n",
            "|    policy_gradient_loss | 0.00144     |\n",
            "|    std                  | 0.355       |\n",
            "|    value_loss           | 3.29e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 352       |\n",
            "|    time_elapsed    | 22625     |\n",
            "|    total_timesteps | 720896    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 353         |\n",
            "|    time_elapsed         | 22668       |\n",
            "|    total_timesteps      | 722944      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014922429 |\n",
            "|    clip_fraction        | 0.211       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.993       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.8e+03     |\n",
            "|    n_updates            | 3520        |\n",
            "|    policy_gradient_loss | -0.00783    |\n",
            "|    std                  | 0.356       |\n",
            "|    value_loss           | 3.25e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 354         |\n",
            "|    time_elapsed         | 22711       |\n",
            "|    total_timesteps      | 724992      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015251917 |\n",
            "|    clip_fraction        | 0.199       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.993       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.4e+04     |\n",
            "|    n_updates            | 3530        |\n",
            "|    policy_gradient_loss | -0.00193    |\n",
            "|    std                  | 0.356       |\n",
            "|    value_loss           | 3.3e+04     |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=725000, episode_reward=-50215.59 +/- 10.91\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 725000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.050727636 |\n",
            "|    clip_fraction        | 0.337       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.988      |\n",
            "|    explained_variance   | 0.991       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.14e+04    |\n",
            "|    n_updates            | 3540        |\n",
            "|    policy_gradient_loss | 0.0171      |\n",
            "|    std                  | 0.353       |\n",
            "|    value_loss           | 3.52e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 355       |\n",
            "|    time_elapsed    | 22807     |\n",
            "|    total_timesteps | 727040    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 356         |\n",
            "|    time_elapsed         | 22850       |\n",
            "|    total_timesteps      | 729088      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017359583 |\n",
            "|    clip_fraction        | 0.215       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.973      |\n",
            "|    explained_variance   | 0.994       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.2e+04     |\n",
            "|    n_updates            | 3550        |\n",
            "|    policy_gradient_loss | -0.00276    |\n",
            "|    std                  | 0.353       |\n",
            "|    value_loss           | 3.02e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=730000, episode_reward=-50206.67 +/- 12.58\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 730000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02688978 |\n",
            "|    clip_fraction        | 0.241      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.978     |\n",
            "|    explained_variance   | 0.994      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.34e+04   |\n",
            "|    n_updates            | 3560       |\n",
            "|    policy_gradient_loss | -0.0107    |\n",
            "|    std                  | 0.354      |\n",
            "|    value_loss           | 2.96e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 357       |\n",
            "|    time_elapsed    | 22948     |\n",
            "|    total_timesteps | 731136    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 358         |\n",
            "|    time_elapsed         | 22991       |\n",
            "|    total_timesteps      | 733184      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017916054 |\n",
            "|    clip_fraction        | 0.231       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.993      |\n",
            "|    explained_variance   | 0.994       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.47e+04    |\n",
            "|    n_updates            | 3570        |\n",
            "|    policy_gradient_loss | -0.00543    |\n",
            "|    std                  | 0.356       |\n",
            "|    value_loss           | 2.92e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=735000, episode_reward=-50197.29 +/- 6.43\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 735000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014797932 |\n",
            "|    clip_fraction        | 0.206       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.996      |\n",
            "|    explained_variance   | 0.932       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.64e+04    |\n",
            "|    n_updates            | 3580        |\n",
            "|    policy_gradient_loss | -0.000484   |\n",
            "|    std                  | 0.356       |\n",
            "|    value_loss           | 1.07e+05    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 359       |\n",
            "|    time_elapsed    | 23090     |\n",
            "|    total_timesteps | 735232    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 360         |\n",
            "|    time_elapsed         | 23133       |\n",
            "|    total_timesteps      | 737280      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018940551 |\n",
            "|    clip_fraction        | 0.216       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.994       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.16e+04    |\n",
            "|    n_updates            | 3590        |\n",
            "|    policy_gradient_loss | -0.00932    |\n",
            "|    std                  | 0.358       |\n",
            "|    value_loss           | 2.83e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 361         |\n",
            "|    time_elapsed         | 23175       |\n",
            "|    total_timesteps      | 739328      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016130326 |\n",
            "|    clip_fraction        | 0.191       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.994       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.26e+04    |\n",
            "|    n_updates            | 3600        |\n",
            "|    policy_gradient_loss | -0.00692    |\n",
            "|    std                  | 0.36        |\n",
            "|    value_loss           | 2.78e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=740000, episode_reward=-50214.18 +/- 7.37\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 740000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018437024 |\n",
            "|    clip_fraction        | 0.211       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.994      |\n",
            "|    explained_variance   | 0.994       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.15e+04    |\n",
            "|    n_updates            | 3610        |\n",
            "|    policy_gradient_loss | -0.0103     |\n",
            "|    std                  | 0.356       |\n",
            "|    value_loss           | 2.75e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 362       |\n",
            "|    time_elapsed    | 23275     |\n",
            "|    total_timesteps | 741376    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 363         |\n",
            "|    time_elapsed         | 23319       |\n",
            "|    total_timesteps      | 743424      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014667559 |\n",
            "|    clip_fraction        | 0.175       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.965      |\n",
            "|    explained_variance   | 0.994       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.68e+04    |\n",
            "|    n_updates            | 3620        |\n",
            "|    policy_gradient_loss | -0.00603    |\n",
            "|    std                  | 0.353       |\n",
            "|    value_loss           | 2.69e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=745000, episode_reward=-50214.37 +/- 7.32\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 745000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019435026 |\n",
            "|    clip_fraction        | 0.209       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.959      |\n",
            "|    explained_variance   | 0.994       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.63e+03    |\n",
            "|    n_updates            | 3630        |\n",
            "|    policy_gradient_loss | -0.00833    |\n",
            "|    std                  | 0.353       |\n",
            "|    value_loss           | 2.66e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 364       |\n",
            "|    time_elapsed    | 23419     |\n",
            "|    total_timesteps | 745472    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 365         |\n",
            "|    time_elapsed         | 23463       |\n",
            "|    total_timesteps      | 747520      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015442708 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.959      |\n",
            "|    explained_variance   | 0.995       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.09e+03    |\n",
            "|    n_updates            | 3640        |\n",
            "|    policy_gradient_loss | -0.00361    |\n",
            "|    std                  | 0.352       |\n",
            "|    value_loss           | 2.49e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 366         |\n",
            "|    time_elapsed         | 23507       |\n",
            "|    total_timesteps      | 749568      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018254839 |\n",
            "|    clip_fraction        | 0.198       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.933      |\n",
            "|    explained_variance   | 0.995       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.2e+04     |\n",
            "|    n_updates            | 3650        |\n",
            "|    policy_gradient_loss | -0.00684    |\n",
            "|    std                  | 0.347       |\n",
            "|    value_loss           | 2.38e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=750000, episode_reward=-50218.05 +/- 13.49\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 750000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014201394 |\n",
            "|    clip_fraction        | 0.192       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.913      |\n",
            "|    explained_variance   | 0.995       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.05e+04    |\n",
            "|    n_updates            | 3660        |\n",
            "|    policy_gradient_loss | -0.00941    |\n",
            "|    std                  | 0.344       |\n",
            "|    value_loss           | 2.38e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 367       |\n",
            "|    time_elapsed    | 23606     |\n",
            "|    total_timesteps | 751616    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 368         |\n",
            "|    time_elapsed         | 23649       |\n",
            "|    total_timesteps      | 753664      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020355357 |\n",
            "|    clip_fraction        | 0.221       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.911      |\n",
            "|    explained_variance   | 0.995       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.27e+04    |\n",
            "|    n_updates            | 3670        |\n",
            "|    policy_gradient_loss | -0.0045     |\n",
            "|    std                  | 0.344       |\n",
            "|    value_loss           | 2.44e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=755000, episode_reward=-50199.20 +/- 8.29\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 755000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018323189 |\n",
            "|    clip_fraction        | 0.221       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.918      |\n",
            "|    explained_variance   | 0.995       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 9.05e+03    |\n",
            "|    n_updates            | 3680        |\n",
            "|    policy_gradient_loss | -0.00928    |\n",
            "|    std                  | 0.346       |\n",
            "|    value_loss           | 2.42e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 369       |\n",
            "|    time_elapsed    | 23747     |\n",
            "|    total_timesteps | 755712    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 370         |\n",
            "|    time_elapsed         | 23790       |\n",
            "|    total_timesteps      | 757760      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017243173 |\n",
            "|    clip_fraction        | 0.21        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.929      |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.03e+03    |\n",
            "|    n_updates            | 3690        |\n",
            "|    policy_gradient_loss | -0.00503    |\n",
            "|    std                  | 0.348       |\n",
            "|    value_loss           | 2.37e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 371         |\n",
            "|    time_elapsed         | 23833       |\n",
            "|    total_timesteps      | 759808      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018877745 |\n",
            "|    clip_fraction        | 0.232       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.935      |\n",
            "|    explained_variance   | 0.995       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 9.64e+03    |\n",
            "|    n_updates            | 3700        |\n",
            "|    policy_gradient_loss | -0.00197    |\n",
            "|    std                  | 0.347       |\n",
            "|    value_loss           | 2.35e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=760000, episode_reward=-50202.71 +/- 14.05\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 760000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016244398 |\n",
            "|    clip_fraction        | 0.2         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.935      |\n",
            "|    explained_variance   | 0.995       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.34e+03    |\n",
            "|    n_updates            | 3710        |\n",
            "|    policy_gradient_loss | -0.00705    |\n",
            "|    std                  | 0.348       |\n",
            "|    value_loss           | 2.3e+04     |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 372       |\n",
            "|    time_elapsed    | 23930     |\n",
            "|    total_timesteps | 761856    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 373         |\n",
            "|    time_elapsed         | 23974       |\n",
            "|    total_timesteps      | 763904      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013932652 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.911      |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.27e+04    |\n",
            "|    n_updates            | 3720        |\n",
            "|    policy_gradient_loss | -0.00382    |\n",
            "|    std                  | 0.342       |\n",
            "|    value_loss           | 2.3e+04     |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=765000, episode_reward=-50219.43 +/- 10.97\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 765000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.054506674 |\n",
            "|    clip_fraction        | 0.358       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.891      |\n",
            "|    explained_variance   | 0.945       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.6e+04     |\n",
            "|    n_updates            | 3730        |\n",
            "|    policy_gradient_loss | 0.0295      |\n",
            "|    std                  | 0.343       |\n",
            "|    value_loss           | 7.97e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 374       |\n",
            "|    time_elapsed    | 24072     |\n",
            "|    total_timesteps | 765952    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 375         |\n",
            "|    time_elapsed         | 24115       |\n",
            "|    total_timesteps      | 768000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023461245 |\n",
            "|    clip_fraction        | 0.291       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.889      |\n",
            "|    explained_variance   | 0.992       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 9.48e+03    |\n",
            "|    n_updates            | 3740        |\n",
            "|    policy_gradient_loss | 0.00666     |\n",
            "|    std                  | 0.342       |\n",
            "|    value_loss           | 2.43e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=770000, episode_reward=-50197.03 +/- 3.74\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 770000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017505523 |\n",
            "|    clip_fraction        | 0.213       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.891      |\n",
            "|    explained_variance   | 0.995       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.91e+03    |\n",
            "|    n_updates            | 3750        |\n",
            "|    policy_gradient_loss | -0.00374    |\n",
            "|    std                  | 0.342       |\n",
            "|    value_loss           | 2.43e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 376       |\n",
            "|    time_elapsed    | 24213     |\n",
            "|    total_timesteps | 770048    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 377         |\n",
            "|    time_elapsed         | 24256       |\n",
            "|    total_timesteps      | 772096      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019111834 |\n",
            "|    clip_fraction        | 0.199       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.906      |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 9.49e+03    |\n",
            "|    n_updates            | 3760        |\n",
            "|    policy_gradient_loss | 0.00609     |\n",
            "|    std                  | 0.345       |\n",
            "|    value_loss           | 2.25e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 378         |\n",
            "|    time_elapsed         | 24299       |\n",
            "|    total_timesteps      | 774144      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018806051 |\n",
            "|    clip_fraction        | 0.202       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.922      |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.35e+04    |\n",
            "|    n_updates            | 3770        |\n",
            "|    policy_gradient_loss | -0.000924   |\n",
            "|    std                  | 0.344       |\n",
            "|    value_loss           | 2.08e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=775000, episode_reward=-50199.01 +/- 22.88\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 775000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018022612 |\n",
            "|    clip_fraction        | 0.185       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.922      |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.11e+04    |\n",
            "|    n_updates            | 3780        |\n",
            "|    policy_gradient_loss | -0.00525    |\n",
            "|    std                  | 0.347       |\n",
            "|    value_loss           | 2.04e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 379       |\n",
            "|    time_elapsed    | 24397     |\n",
            "|    total_timesteps | 776192    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 380         |\n",
            "|    time_elapsed         | 24440       |\n",
            "|    total_timesteps      | 778240      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016986012 |\n",
            "|    clip_fraction        | 0.196       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.956      |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 9.57e+03    |\n",
            "|    n_updates            | 3790        |\n",
            "|    policy_gradient_loss | -0.00703    |\n",
            "|    std                  | 0.352       |\n",
            "|    value_loss           | 2e+04       |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=780000, episode_reward=-50202.59 +/- 12.40\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 780000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01993825 |\n",
            "|    clip_fraction        | 0.222      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.978     |\n",
            "|    explained_variance   | 0.996      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.49e+04   |\n",
            "|    n_updates            | 3800       |\n",
            "|    policy_gradient_loss | -0.00027   |\n",
            "|    std                  | 0.356      |\n",
            "|    value_loss           | 1.98e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 381       |\n",
            "|    time_elapsed    | 24538     |\n",
            "|    total_timesteps | 780288    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 382         |\n",
            "|    time_elapsed         | 24581       |\n",
            "|    total_timesteps      | 782336      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014333131 |\n",
            "|    clip_fraction        | 0.171       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.992      |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.55e+04    |\n",
            "|    n_updates            | 3810        |\n",
            "|    policy_gradient_loss | -0.00683    |\n",
            "|    std                  | 0.356       |\n",
            "|    value_loss           | 1.93e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 383         |\n",
            "|    time_elapsed         | 24625       |\n",
            "|    total_timesteps      | 784384      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015258379 |\n",
            "|    clip_fraction        | 0.195       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.999      |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.4e+04     |\n",
            "|    n_updates            | 3820        |\n",
            "|    policy_gradient_loss | -0.00389    |\n",
            "|    std                  | 0.357       |\n",
            "|    value_loss           | 1.89e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=785000, episode_reward=-50210.33 +/- 6.78\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 785000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01435836 |\n",
            "|    clip_fraction        | 0.191      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.01      |\n",
            "|    explained_variance   | 0.996      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.32e+04   |\n",
            "|    n_updates            | 3830       |\n",
            "|    policy_gradient_loss | -0.00758   |\n",
            "|    std                  | 0.359      |\n",
            "|    value_loss           | 1.86e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 384       |\n",
            "|    time_elapsed    | 24723     |\n",
            "|    total_timesteps | 786432    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 385         |\n",
            "|    time_elapsed         | 24766       |\n",
            "|    total_timesteps      | 788480      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030380014 |\n",
            "|    clip_fraction        | 0.288       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1          |\n",
            "|    explained_variance   | 0.995       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.85e+03    |\n",
            "|    n_updates            | 3840        |\n",
            "|    policy_gradient_loss | 0.0102      |\n",
            "|    std                  | 0.358       |\n",
            "|    value_loss           | 1.84e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=790000, episode_reward=-50199.09 +/- 17.50\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 790000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020456241 |\n",
            "|    clip_fraction        | 0.231       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.63e+03    |\n",
            "|    n_updates            | 3850        |\n",
            "|    policy_gradient_loss | -0.0125     |\n",
            "|    std                  | 0.363       |\n",
            "|    value_loss           | 1.84e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 386       |\n",
            "|    time_elapsed    | 24863     |\n",
            "|    total_timesteps | 790528    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 387         |\n",
            "|    time_elapsed         | 24906       |\n",
            "|    total_timesteps      | 792576      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016636917 |\n",
            "|    clip_fraction        | 0.188       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.07e+04    |\n",
            "|    n_updates            | 3860        |\n",
            "|    policy_gradient_loss | -0.00693    |\n",
            "|    std                  | 0.362       |\n",
            "|    value_loss           | 1.77e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 388         |\n",
            "|    time_elapsed         | 24949       |\n",
            "|    total_timesteps      | 794624      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.037457693 |\n",
            "|    clip_fraction        | 0.25        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.96        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.1e+04     |\n",
            "|    n_updates            | 3870        |\n",
            "|    policy_gradient_loss | 0.0167      |\n",
            "|    std                  | 0.362       |\n",
            "|    value_loss           | 6.88e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=795000, episode_reward=-50206.58 +/- 10.62\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 795000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014192334 |\n",
            "|    clip_fraction        | 0.131       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.94        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.56e+04    |\n",
            "|    n_updates            | 3880        |\n",
            "|    policy_gradient_loss | 0.00851     |\n",
            "|    std                  | 0.362       |\n",
            "|    value_loss           | 8.71e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 389       |\n",
            "|    time_elapsed    | 25047     |\n",
            "|    total_timesteps | 796672    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 390         |\n",
            "|    time_elapsed         | 25090       |\n",
            "|    total_timesteps      | 798720      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020250827 |\n",
            "|    clip_fraction        | 0.21        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1e+04       |\n",
            "|    n_updates            | 3890        |\n",
            "|    policy_gradient_loss | -0.00446    |\n",
            "|    std                  | 0.362       |\n",
            "|    value_loss           | 1.7e+04     |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=800000, episode_reward=-50206.34 +/- 6.85\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 800000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020188546 |\n",
            "|    clip_fraction        | 0.206       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.21e+03    |\n",
            "|    n_updates            | 3900        |\n",
            "|    policy_gradient_loss | -0.00683    |\n",
            "|    std                  | 0.368       |\n",
            "|    value_loss           | 1.66e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 391       |\n",
            "|    time_elapsed    | 25188     |\n",
            "|    total_timesteps | 800768    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 392         |\n",
            "|    time_elapsed         | 25231       |\n",
            "|    total_timesteps      | 802816      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021341136 |\n",
            "|    clip_fraction        | 0.232       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.05       |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 9.34e+03    |\n",
            "|    n_updates            | 3910        |\n",
            "|    policy_gradient_loss | -0.000922   |\n",
            "|    std                  | 0.367       |\n",
            "|    value_loss           | 1.68e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 393         |\n",
            "|    time_elapsed         | 25274       |\n",
            "|    total_timesteps      | 804864      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018052712 |\n",
            "|    clip_fraction        | 0.194       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 9.74e+03    |\n",
            "|    n_updates            | 3920        |\n",
            "|    policy_gradient_loss | -0.002      |\n",
            "|    std                  | 0.365       |\n",
            "|    value_loss           | 1.71e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=805000, episode_reward=-50202.43 +/- 12.48\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 805000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012361094 |\n",
            "|    clip_fraction        | 0.141       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.98        |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.07e+04    |\n",
            "|    n_updates            | 3930        |\n",
            "|    policy_gradient_loss | 0.00252     |\n",
            "|    std                  | 0.365       |\n",
            "|    value_loss           | 3.82e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 394       |\n",
            "|    time_elapsed    | 25371     |\n",
            "|    total_timesteps | 806912    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 395         |\n",
            "|    time_elapsed         | 25415       |\n",
            "|    total_timesteps      | 808960      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022984851 |\n",
            "|    clip_fraction        | 0.216       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0.994       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.37e+03    |\n",
            "|    n_updates            | 3940        |\n",
            "|    policy_gradient_loss | 0.00783     |\n",
            "|    std                  | 0.364       |\n",
            "|    value_loss           | 1.92e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=810000, episode_reward=-50203.09 +/- 9.44\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 810000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018631872 |\n",
            "|    clip_fraction        | 0.2         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.35e+03    |\n",
            "|    n_updates            | 3950        |\n",
            "|    policy_gradient_loss | -0.00288    |\n",
            "|    std                  | 0.367       |\n",
            "|    value_loss           | 1.66e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 396       |\n",
            "|    time_elapsed    | 25514     |\n",
            "|    total_timesteps | 811008    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 31         |\n",
            "|    iterations           | 397        |\n",
            "|    time_elapsed         | 25557      |\n",
            "|    total_timesteps      | 813056     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01638769 |\n",
            "|    clip_fraction        | 0.205      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.03      |\n",
            "|    explained_variance   | 0.997      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 8.61e+03   |\n",
            "|    n_updates            | 3960       |\n",
            "|    policy_gradient_loss | -0.00771   |\n",
            "|    std                  | 0.364      |\n",
            "|    value_loss           | 1.64e+04   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=815000, episode_reward=-50215.86 +/- 11.06\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 815000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01695954 |\n",
            "|    clip_fraction        | 0.216      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.02      |\n",
            "|    explained_variance   | 0.997      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 6.05e+03   |\n",
            "|    n_updates            | 3970       |\n",
            "|    policy_gradient_loss | -0.00314   |\n",
            "|    std                  | 0.363      |\n",
            "|    value_loss           | 1.5e+04    |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 398       |\n",
            "|    time_elapsed    | 25656     |\n",
            "|    total_timesteps | 815104    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 399         |\n",
            "|    time_elapsed         | 25699       |\n",
            "|    total_timesteps      | 817152      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014069205 |\n",
            "|    clip_fraction        | 0.185       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.54e+03    |\n",
            "|    n_updates            | 3980        |\n",
            "|    policy_gradient_loss | -0.0077     |\n",
            "|    std                  | 0.365       |\n",
            "|    value_loss           | 1.47e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 400         |\n",
            "|    time_elapsed         | 25743       |\n",
            "|    total_timesteps      | 819200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020733913 |\n",
            "|    clip_fraction        | 0.211       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.87e+03    |\n",
            "|    n_updates            | 3990        |\n",
            "|    policy_gradient_loss | -0.00133    |\n",
            "|    std                  | 0.363       |\n",
            "|    value_loss           | 1.44e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=820000, episode_reward=-50208.57 +/- 10.46\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 820000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028325051 |\n",
            "|    clip_fraction        | 0.304       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.948       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.52e+04    |\n",
            "|    n_updates            | 4000        |\n",
            "|    policy_gradient_loss | 0.0281      |\n",
            "|    std                  | 0.363       |\n",
            "|    value_loss           | 4.96e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 401       |\n",
            "|    time_elapsed    | 25842     |\n",
            "|    total_timesteps | 821248    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 402         |\n",
            "|    time_elapsed         | 25885       |\n",
            "|    total_timesteps      | 823296      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019473832 |\n",
            "|    clip_fraction        | 0.215       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.21e+03    |\n",
            "|    n_updates            | 4010        |\n",
            "|    policy_gradient_loss | -0.00312    |\n",
            "|    std                  | 0.364       |\n",
            "|    value_loss           | 1.42e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=825000, episode_reward=-50218.10 +/- 11.54\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 825000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014588646 |\n",
            "|    clip_fraction        | 0.19        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 6.71e+03    |\n",
            "|    n_updates            | 4020        |\n",
            "|    policy_gradient_loss | -0.00499    |\n",
            "|    std                  | 0.363       |\n",
            "|    value_loss           | 1.36e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 403       |\n",
            "|    time_elapsed    | 25984     |\n",
            "|    total_timesteps | 825344    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 404         |\n",
            "|    time_elapsed         | 26028       |\n",
            "|    total_timesteps      | 827392      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017535262 |\n",
            "|    clip_fraction        | 0.199       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.13e+03    |\n",
            "|    n_updates            | 4030        |\n",
            "|    policy_gradient_loss | -0.00924    |\n",
            "|    std                  | 0.364       |\n",
            "|    value_loss           | 1.34e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 31         |\n",
            "|    iterations           | 405        |\n",
            "|    time_elapsed         | 26072      |\n",
            "|    total_timesteps      | 829440     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01973148 |\n",
            "|    clip_fraction        | 0.205      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.02      |\n",
            "|    explained_variance   | 0.997      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 3.83e+03   |\n",
            "|    n_updates            | 4040       |\n",
            "|    policy_gradient_loss | -0.00619   |\n",
            "|    std                  | 0.365      |\n",
            "|    value_loss           | 1.33e+04   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=830000, episode_reward=-50217.27 +/- 15.51\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 830000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016425733 |\n",
            "|    clip_fraction        | 0.208       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.77e+03    |\n",
            "|    n_updates            | 4050        |\n",
            "|    policy_gradient_loss | -0.00517    |\n",
            "|    std                  | 0.368       |\n",
            "|    value_loss           | 1.3e+04     |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 406       |\n",
            "|    time_elapsed    | 26173     |\n",
            "|    total_timesteps | 831488    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 407         |\n",
            "|    time_elapsed         | 26216       |\n",
            "|    total_timesteps      | 833536      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021761866 |\n",
            "|    clip_fraction        | 0.217       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.97e+03    |\n",
            "|    n_updates            | 4060        |\n",
            "|    policy_gradient_loss | -0.00694    |\n",
            "|    std                  | 0.364       |\n",
            "|    value_loss           | 1.28e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=835000, episode_reward=-50199.04 +/- 15.84\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 835000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020600008 |\n",
            "|    clip_fraction        | 0.208       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.57e+03    |\n",
            "|    n_updates            | 4070        |\n",
            "|    policy_gradient_loss | -0.00788    |\n",
            "|    std                  | 0.369       |\n",
            "|    value_loss           | 1.24e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 408       |\n",
            "|    time_elapsed    | 26315     |\n",
            "|    total_timesteps | 835584    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 409         |\n",
            "|    time_elapsed         | 26358       |\n",
            "|    total_timesteps      | 837632      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015449384 |\n",
            "|    clip_fraction        | 0.206       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 6.19e+03    |\n",
            "|    n_updates            | 4080        |\n",
            "|    policy_gradient_loss | -0.00445    |\n",
            "|    std                  | 0.363       |\n",
            "|    value_loss           | 1.22e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 410         |\n",
            "|    time_elapsed         | 26402       |\n",
            "|    total_timesteps      | 839680      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.040994026 |\n",
            "|    clip_fraction        | 0.303       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.995       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.47e+03    |\n",
            "|    n_updates            | 4090        |\n",
            "|    policy_gradient_loss | 0.0143      |\n",
            "|    std                  | 0.363       |\n",
            "|    value_loss           | 1.44e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=840000, episode_reward=-50202.88 +/- 18.28\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 840000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017810049 |\n",
            "|    clip_fraction        | 0.228       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.07       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 6.92e+03    |\n",
            "|    n_updates            | 4100        |\n",
            "|    policy_gradient_loss | -0.00937    |\n",
            "|    std                  | 0.37        |\n",
            "|    value_loss           | 1.18e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 411       |\n",
            "|    time_elapsed    | 26500     |\n",
            "|    total_timesteps | 841728    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 412         |\n",
            "|    time_elapsed         | 26544       |\n",
            "|    total_timesteps      | 843776      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014204926 |\n",
            "|    clip_fraction        | 0.202       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.11       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.89e+03    |\n",
            "|    n_updates            | 4110        |\n",
            "|    policy_gradient_loss | -0.00706    |\n",
            "|    std                  | 0.373       |\n",
            "|    value_loss           | 1.17e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=845000, episode_reward=-50208.22 +/- 15.74\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 845000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01731537 |\n",
            "|    clip_fraction        | 0.185      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.13      |\n",
            "|    explained_variance   | 0.998      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 9.83e+03   |\n",
            "|    n_updates            | 4120       |\n",
            "|    policy_gradient_loss | -0.00373   |\n",
            "|    std                  | 0.378      |\n",
            "|    value_loss           | 1.14e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 413       |\n",
            "|    time_elapsed    | 26642     |\n",
            "|    total_timesteps | 845824    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 31         |\n",
            "|    iterations           | 414        |\n",
            "|    time_elapsed         | 26685      |\n",
            "|    total_timesteps      | 847872     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03001406 |\n",
            "|    clip_fraction        | 0.313      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.14      |\n",
            "|    explained_variance   | 0.991      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.12e+04   |\n",
            "|    n_updates            | 4130       |\n",
            "|    policy_gradient_loss | 0.0208     |\n",
            "|    std                  | 0.379      |\n",
            "|    value_loss           | 1.76e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 415         |\n",
            "|    time_elapsed         | 26729       |\n",
            "|    total_timesteps      | 849920      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021787472 |\n",
            "|    clip_fraction        | 0.217       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.26e+03    |\n",
            "|    n_updates            | 4140        |\n",
            "|    policy_gradient_loss | 0.00319     |\n",
            "|    std                  | 0.374       |\n",
            "|    value_loss           | 9.18e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=850000, episode_reward=-50208.70 +/- 14.75\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 850000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016002549 |\n",
            "|    clip_fraction        | 0.194       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.12       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 8.64e+03    |\n",
            "|    n_updates            | 4150        |\n",
            "|    policy_gradient_loss | -0.00685    |\n",
            "|    std                  | 0.373       |\n",
            "|    value_loss           | 1.12e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 416       |\n",
            "|    time_elapsed    | 26828     |\n",
            "|    total_timesteps | 851968    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 417         |\n",
            "|    time_elapsed         | 26872       |\n",
            "|    total_timesteps      | 854016      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015269786 |\n",
            "|    clip_fraction        | 0.183       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.12       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.63e+03    |\n",
            "|    n_updates            | 4160        |\n",
            "|    policy_gradient_loss | -0.00872    |\n",
            "|    std                  | 0.374       |\n",
            "|    value_loss           | 1.17e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=855000, episode_reward=-50209.65 +/- 16.18\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 855000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017593196 |\n",
            "|    clip_fraction        | 0.19        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.2e+03     |\n",
            "|    n_updates            | 4170        |\n",
            "|    policy_gradient_loss | -0.00611    |\n",
            "|    std                  | 0.377       |\n",
            "|    value_loss           | 1.14e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 418       |\n",
            "|    time_elapsed    | 26968     |\n",
            "|    total_timesteps | 856064    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 419         |\n",
            "|    time_elapsed         | 27012       |\n",
            "|    total_timesteps      | 858112      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020255191 |\n",
            "|    clip_fraction        | 0.226       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.69e+03    |\n",
            "|    n_updates            | 4180        |\n",
            "|    policy_gradient_loss | 0.00128     |\n",
            "|    std                  | 0.377       |\n",
            "|    value_loss           | 1.09e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=860000, episode_reward=-50200.93 +/- 15.98\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 860000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016995143 |\n",
            "|    clip_fraction        | 0.205       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.14       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.89e+03    |\n",
            "|    n_updates            | 4190        |\n",
            "|    policy_gradient_loss | -0.00546    |\n",
            "|    std                  | 0.373       |\n",
            "|    value_loss           | 1.04e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 420       |\n",
            "|    time_elapsed    | 27110     |\n",
            "|    total_timesteps | 860160    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 421         |\n",
            "|    time_elapsed         | 27154       |\n",
            "|    total_timesteps      | 862208      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015160963 |\n",
            "|    clip_fraction        | 0.202       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.14       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.58e+03    |\n",
            "|    n_updates            | 4200        |\n",
            "|    policy_gradient_loss | -0.00436    |\n",
            "|    std                  | 0.375       |\n",
            "|    value_loss           | 1.02e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 422         |\n",
            "|    time_elapsed         | 27197       |\n",
            "|    total_timesteps      | 864256      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021822408 |\n",
            "|    clip_fraction        | 0.207       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 6.5e+03     |\n",
            "|    n_updates            | 4210        |\n",
            "|    policy_gradient_loss | -0.00568    |\n",
            "|    std                  | 0.369       |\n",
            "|    value_loss           | 1.01e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=865000, episode_reward=-50204.48 +/- 7.79\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 865000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014431513 |\n",
            "|    clip_fraction        | 0.196       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.11       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.49e+03    |\n",
            "|    n_updates            | 4220        |\n",
            "|    policy_gradient_loss | -0.00533    |\n",
            "|    std                  | 0.367       |\n",
            "|    value_loss           | 9.59e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 423       |\n",
            "|    time_elapsed    | 27296     |\n",
            "|    total_timesteps | 866304    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 424         |\n",
            "|    time_elapsed         | 27339       |\n",
            "|    total_timesteps      | 868352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020822857 |\n",
            "|    clip_fraction        | 0.226       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.07       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.66e+03    |\n",
            "|    n_updates            | 4230        |\n",
            "|    policy_gradient_loss | -0.00781    |\n",
            "|    std                  | 0.363       |\n",
            "|    value_loss           | 9.77e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=870000, episode_reward=-50206.47 +/- 10.28\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 870000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018162452 |\n",
            "|    clip_fraction        | 0.217       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.52e+03    |\n",
            "|    n_updates            | 4240        |\n",
            "|    policy_gradient_loss | -0.00198    |\n",
            "|    std                  | 0.362       |\n",
            "|    value_loss           | 9.64e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 425       |\n",
            "|    time_elapsed    | 27437     |\n",
            "|    total_timesteps | 870400    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 426         |\n",
            "|    time_elapsed         | 27480       |\n",
            "|    total_timesteps      | 872448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019839862 |\n",
            "|    clip_fraction        | 0.2         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 5.35e+03    |\n",
            "|    n_updates            | 4250        |\n",
            "|    policy_gradient_loss | -0.00894    |\n",
            "|    std                  | 0.36        |\n",
            "|    value_loss           | 9.4e+03     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 427         |\n",
            "|    time_elapsed         | 27523       |\n",
            "|    total_timesteps      | 874496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013445029 |\n",
            "|    clip_fraction        | 0.191       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 7.9e+03     |\n",
            "|    n_updates            | 4260        |\n",
            "|    policy_gradient_loss | -0.00679    |\n",
            "|    std                  | 0.36        |\n",
            "|    value_loss           | 9.26e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=875000, episode_reward=-50213.82 +/- 9.49\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 875000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032094743 |\n",
            "|    clip_fraction        | 0.307       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.32e+03    |\n",
            "|    n_updates            | 4270        |\n",
            "|    policy_gradient_loss | 0.00376     |\n",
            "|    std                  | 0.361       |\n",
            "|    value_loss           | 8.36e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 428       |\n",
            "|    time_elapsed    | 27621     |\n",
            "|    total_timesteps | 876544    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 429         |\n",
            "|    time_elapsed         | 27665       |\n",
            "|    total_timesteps      | 878592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017557537 |\n",
            "|    clip_fraction        | 0.228       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.78e+03    |\n",
            "|    n_updates            | 4280        |\n",
            "|    policy_gradient_loss | 0.00569     |\n",
            "|    std                  | 0.358       |\n",
            "|    value_loss           | 6.78e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=880000, episode_reward=-50210.22 +/- 16.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 880000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018889433 |\n",
            "|    clip_fraction        | 0.215       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.77e+03    |\n",
            "|    n_updates            | 4290        |\n",
            "|    policy_gradient_loss | -0.00509    |\n",
            "|    std                  | 0.357       |\n",
            "|    value_loss           | 6.51e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 430       |\n",
            "|    time_elapsed    | 27762     |\n",
            "|    total_timesteps | 880640    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 31         |\n",
            "|    iterations           | 431        |\n",
            "|    time_elapsed         | 27806      |\n",
            "|    total_timesteps      | 882688     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01851771 |\n",
            "|    clip_fraction        | 0.208      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.984     |\n",
            "|    explained_variance   | 0.999      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 3.96e+03   |\n",
            "|    n_updates            | 4300       |\n",
            "|    policy_gradient_loss | -0.00251   |\n",
            "|    std                  | 0.353      |\n",
            "|    value_loss           | 6.37e+03   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 432         |\n",
            "|    time_elapsed         | 27849       |\n",
            "|    total_timesteps      | 884736      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027334973 |\n",
            "|    clip_fraction        | 0.235       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.968      |\n",
            "|    explained_variance   | 0.995       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 6.62e+03    |\n",
            "|    n_updates            | 4310        |\n",
            "|    policy_gradient_loss | 0.0127      |\n",
            "|    std                  | 0.353       |\n",
            "|    value_loss           | 1.57e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=885000, episode_reward=-50208.03 +/- 15.56\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 885000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028880231 |\n",
            "|    clip_fraction        | 0.245       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.969      |\n",
            "|    explained_variance   | 0.953       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.35e+03    |\n",
            "|    n_updates            | 4320        |\n",
            "|    policy_gradient_loss | 0.0193      |\n",
            "|    std                  | 0.353       |\n",
            "|    value_loss           | 6.13e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 433       |\n",
            "|    time_elapsed    | 27947     |\n",
            "|    total_timesteps | 886784    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 434         |\n",
            "|    time_elapsed         | 27990       |\n",
            "|    total_timesteps      | 888832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024132209 |\n",
            "|    clip_fraction        | 0.224       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.955      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.42e+03    |\n",
            "|    n_updates            | 4330        |\n",
            "|    policy_gradient_loss | -0.0107     |\n",
            "|    std                  | 0.352       |\n",
            "|    value_loss           | 6.89e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=890000, episode_reward=-50209.98 +/- 10.95\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 890000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016788766 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.934      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.01e+03    |\n",
            "|    n_updates            | 4340        |\n",
            "|    policy_gradient_loss | -0.0105     |\n",
            "|    std                  | 0.35        |\n",
            "|    value_loss           | 6.64e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 435       |\n",
            "|    time_elapsed    | 28087     |\n",
            "|    total_timesteps | 890880    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 31         |\n",
            "|    iterations           | 436        |\n",
            "|    time_elapsed         | 28130      |\n",
            "|    total_timesteps      | 892928     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02435828 |\n",
            "|    clip_fraction        | 0.211      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.898     |\n",
            "|    explained_variance   | 0.999      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 4.23e+03   |\n",
            "|    n_updates            | 4350       |\n",
            "|    policy_gradient_loss | -0.0082    |\n",
            "|    std                  | 0.344      |\n",
            "|    value_loss           | 6.6e+03    |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 437         |\n",
            "|    time_elapsed         | 28174       |\n",
            "|    total_timesteps      | 894976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019234315 |\n",
            "|    clip_fraction        | 0.219       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.888      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.35e+03    |\n",
            "|    n_updates            | 4360        |\n",
            "|    policy_gradient_loss | -0.0139     |\n",
            "|    std                  | 0.344       |\n",
            "|    value_loss           | 6.63e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=895000, episode_reward=-50200.70 +/- 6.65\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 895000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017728914 |\n",
            "|    clip_fraction        | 0.216       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.886      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.88e+03    |\n",
            "|    n_updates            | 4370        |\n",
            "|    policy_gradient_loss | -0.00514    |\n",
            "|    std                  | 0.343       |\n",
            "|    value_loss           | 7e+03       |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 438       |\n",
            "|    time_elapsed    | 28272     |\n",
            "|    total_timesteps | 897024    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 439         |\n",
            "|    time_elapsed         | 28316       |\n",
            "|    total_timesteps      | 899072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018951979 |\n",
            "|    clip_fraction        | 0.24        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.863      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.07e+03    |\n",
            "|    n_updates            | 4380        |\n",
            "|    policy_gradient_loss | -0.0033     |\n",
            "|    std                  | 0.339       |\n",
            "|    value_loss           | 6.68e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=900000, episode_reward=-50212.36 +/- 9.63\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 900000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018537598 |\n",
            "|    clip_fraction        | 0.215       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.836      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 529         |\n",
            "|    n_updates            | 4390        |\n",
            "|    policy_gradient_loss | -0.00599    |\n",
            "|    std                  | 0.337       |\n",
            "|    value_loss           | 5.37e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 440       |\n",
            "|    time_elapsed    | 28414     |\n",
            "|    total_timesteps | 901120    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 441         |\n",
            "|    time_elapsed         | 28458       |\n",
            "|    total_timesteps      | 903168      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021486681 |\n",
            "|    clip_fraction        | 0.222       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.846      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.09e+03    |\n",
            "|    n_updates            | 4400        |\n",
            "|    policy_gradient_loss | -0.00604    |\n",
            "|    std                  | 0.34        |\n",
            "|    value_loss           | 5.74e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=905000, episode_reward=-50206.99 +/- 10.79\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 905000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02226841 |\n",
            "|    clip_fraction        | 0.212      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.873     |\n",
            "|    explained_variance   | 0.999      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.21e+03   |\n",
            "|    n_updates            | 4410       |\n",
            "|    policy_gradient_loss | -0.00916   |\n",
            "|    std                  | 0.344      |\n",
            "|    value_loss           | 5.23e+03   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 442       |\n",
            "|    time_elapsed    | 28557     |\n",
            "|    total_timesteps | 905216    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 443         |\n",
            "|    time_elapsed         | 28600       |\n",
            "|    total_timesteps      | 907264      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027506512 |\n",
            "|    clip_fraction        | 0.256       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.871      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.35e+03    |\n",
            "|    n_updates            | 4420        |\n",
            "|    policy_gradient_loss | -0.00915    |\n",
            "|    std                  | 0.34        |\n",
            "|    value_loss           | 5.36e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 444         |\n",
            "|    time_elapsed         | 28643       |\n",
            "|    total_timesteps      | 909312      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022760112 |\n",
            "|    clip_fraction        | 0.248       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.855      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.7e+03     |\n",
            "|    n_updates            | 4430        |\n",
            "|    policy_gradient_loss | -0.00608    |\n",
            "|    std                  | 0.34        |\n",
            "|    value_loss           | 5.04e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=910000, episode_reward=-50217.69 +/- 8.29\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 910000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021468963 |\n",
            "|    clip_fraction        | 0.223       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.861      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 4.67e+03    |\n",
            "|    n_updates            | 4440        |\n",
            "|    policy_gradient_loss | -0.00844    |\n",
            "|    std                  | 0.343       |\n",
            "|    value_loss           | 5.25e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 445       |\n",
            "|    time_elapsed    | 28742     |\n",
            "|    total_timesteps | 911360    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 446         |\n",
            "|    time_elapsed         | 28785       |\n",
            "|    total_timesteps      | 913408      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025419667 |\n",
            "|    clip_fraction        | 0.236       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.865      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.49e+03    |\n",
            "|    n_updates            | 4450        |\n",
            "|    policy_gradient_loss | -0.00766    |\n",
            "|    std                  | 0.342       |\n",
            "|    value_loss           | 4.96e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=915000, episode_reward=-50202.80 +/- 9.21\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 915000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023870725 |\n",
            "|    clip_fraction        | 0.23        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.861      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.05e+03    |\n",
            "|    n_updates            | 4460        |\n",
            "|    policy_gradient_loss | -0.00484    |\n",
            "|    std                  | 0.342       |\n",
            "|    value_loss           | 4.94e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 447       |\n",
            "|    time_elapsed    | 28883     |\n",
            "|    total_timesteps | 915456    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 448         |\n",
            "|    time_elapsed         | 28926       |\n",
            "|    total_timesteps      | 917504      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027418979 |\n",
            "|    clip_fraction        | 0.252       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.865      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.3e+03     |\n",
            "|    n_updates            | 4470        |\n",
            "|    policy_gradient_loss | 1.14e-07    |\n",
            "|    std                  | 0.344       |\n",
            "|    value_loss           | 5.05e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 449         |\n",
            "|    time_elapsed         | 28971       |\n",
            "|    total_timesteps      | 919552      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026164243 |\n",
            "|    clip_fraction        | 0.271       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.879      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.88e+03    |\n",
            "|    n_updates            | 4480        |\n",
            "|    policy_gradient_loss | 0.000391    |\n",
            "|    std                  | 0.344       |\n",
            "|    value_loss           | 4.9e+03     |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=920000, episode_reward=-50202.42 +/- 15.09\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 920000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028531577 |\n",
            "|    clip_fraction        | 0.259       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.902      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 3.35e+03    |\n",
            "|    n_updates            | 4490        |\n",
            "|    policy_gradient_loss | -0.0114     |\n",
            "|    std                  | 0.347       |\n",
            "|    value_loss           | 4.69e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 450       |\n",
            "|    time_elapsed    | 29068     |\n",
            "|    total_timesteps | 921600    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 451         |\n",
            "|    time_elapsed         | 29112       |\n",
            "|    total_timesteps      | 923648      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019203307 |\n",
            "|    clip_fraction        | 0.234       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.929      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.51e+03    |\n",
            "|    n_updates            | 4500        |\n",
            "|    policy_gradient_loss | -0.00846    |\n",
            "|    std                  | 0.351       |\n",
            "|    value_loss           | 4.5e+03     |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=925000, episode_reward=-50210.34 +/- 7.04\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 925000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027112475 |\n",
            "|    clip_fraction        | 0.23        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.956      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 515         |\n",
            "|    n_updates            | 4510        |\n",
            "|    policy_gradient_loss | -0.00934    |\n",
            "|    std                  | 0.356       |\n",
            "|    value_loss           | 4.41e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 452       |\n",
            "|    time_elapsed    | 29210     |\n",
            "|    total_timesteps | 925696    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 31         |\n",
            "|    iterations           | 453        |\n",
            "|    time_elapsed         | 29253      |\n",
            "|    total_timesteps      | 927744     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05794379 |\n",
            "|    clip_fraction        | 0.384      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.975     |\n",
            "|    explained_variance   | 0.995      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 2.92e+03   |\n",
            "|    n_updates            | 4520       |\n",
            "|    policy_gradient_loss | 0.0321     |\n",
            "|    std                  | 0.355      |\n",
            "|    value_loss           | 9.07e+03   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 31         |\n",
            "|    iterations           | 454        |\n",
            "|    time_elapsed         | 29296      |\n",
            "|    total_timesteps      | 929792     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03359952 |\n",
            "|    clip_fraction        | 0.251      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.966     |\n",
            "|    explained_variance   | 0.999      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.67e+03   |\n",
            "|    n_updates            | 4530       |\n",
            "|    policy_gradient_loss | -0.00404   |\n",
            "|    std                  | 0.354      |\n",
            "|    value_loss           | 4.61e+03   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=930000, episode_reward=-50191.14 +/- 13.64\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 930000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016754095 |\n",
            "|    clip_fraction        | 0.222       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.983      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.62e+03    |\n",
            "|    n_updates            | 4540        |\n",
            "|    policy_gradient_loss | -0.00783    |\n",
            "|    std                  | 0.358       |\n",
            "|    value_loss           | 4.05e+03    |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 455       |\n",
            "|    time_elapsed    | 29393     |\n",
            "|    total_timesteps | 931840    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 31         |\n",
            "|    iterations           | 456        |\n",
            "|    time_elapsed         | 29437      |\n",
            "|    total_timesteps      | 933888     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01882019 |\n",
            "|    clip_fraction        | 0.224      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.995     |\n",
            "|    explained_variance   | 0.999      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.89e+03   |\n",
            "|    n_updates            | 4550       |\n",
            "|    policy_gradient_loss | -0.0106    |\n",
            "|    std                  | 0.358      |\n",
            "|    value_loss           | 3.92e+03   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=935000, episode_reward=-50215.78 +/- 8.94\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 935000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020430366 |\n",
            "|    clip_fraction        | 0.198       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 2.29e+03    |\n",
            "|    n_updates            | 4560        |\n",
            "|    policy_gradient_loss | -0.00575    |\n",
            "|    std                  | 0.359       |\n",
            "|    value_loss           | 3.86e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 457       |\n",
            "|    time_elapsed    | 29536     |\n",
            "|    total_timesteps | 935936    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 458         |\n",
            "|    time_elapsed         | 29580       |\n",
            "|    total_timesteps      | 937984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022450069 |\n",
            "|    clip_fraction        | 0.228       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 575         |\n",
            "|    n_updates            | 4570        |\n",
            "|    policy_gradient_loss | -0.0129     |\n",
            "|    std                  | 0.359       |\n",
            "|    value_loss           | 3.89e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=940000, episode_reward=-50217.29 +/- 13.33\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 940000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029335218 |\n",
            "|    clip_fraction        | 0.32        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.98e+03    |\n",
            "|    n_updates            | 4580        |\n",
            "|    policy_gradient_loss | 0.00654     |\n",
            "|    std                  | 0.36        |\n",
            "|    value_loss           | 4.61e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 459       |\n",
            "|    time_elapsed    | 29677     |\n",
            "|    total_timesteps | 940032    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 460         |\n",
            "|    time_elapsed         | 29721       |\n",
            "|    total_timesteps      | 942080      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025474388 |\n",
            "|    clip_fraction        | 0.225       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 845         |\n",
            "|    n_updates            | 4590        |\n",
            "|    policy_gradient_loss | -0.00823    |\n",
            "|    std                  | 0.363       |\n",
            "|    value_loss           | 3.7e+03     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 461         |\n",
            "|    time_elapsed         | 29766       |\n",
            "|    total_timesteps      | 944128      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027578589 |\n",
            "|    clip_fraction        | 0.242       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.07       |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.72e+03    |\n",
            "|    n_updates            | 4600        |\n",
            "|    policy_gradient_loss | -0.00727    |\n",
            "|    std                  | 0.368       |\n",
            "|    value_loss           | 3.53e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=945000, episode_reward=-50212.22 +/- 7.59\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 945000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022337254 |\n",
            "|    clip_fraction        | 0.236       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.07       |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.43e+03    |\n",
            "|    n_updates            | 4610        |\n",
            "|    policy_gradient_loss | -0.0132     |\n",
            "|    std                  | 0.365       |\n",
            "|    value_loss           | 3.6e+03     |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 462       |\n",
            "|    time_elapsed    | 29864     |\n",
            "|    total_timesteps | 946176    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 463         |\n",
            "|    time_elapsed         | 29906       |\n",
            "|    total_timesteps      | 948224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026216485 |\n",
            "|    clip_fraction        | 0.25        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 6.39e+03    |\n",
            "|    n_updates            | 4620        |\n",
            "|    policy_gradient_loss | 0.0126      |\n",
            "|    std                  | 0.366       |\n",
            "|    value_loss           | 1.84e+04    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=950000, episode_reward=-50193.57 +/- 7.61\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 950000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022010967 |\n",
            "|    clip_fraction        | 0.251       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.05       |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 855         |\n",
            "|    n_updates            | 4630        |\n",
            "|    policy_gradient_loss | -0.0088     |\n",
            "|    std                  | 0.364       |\n",
            "|    value_loss           | 3.25e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 464       |\n",
            "|    time_elapsed    | 30004     |\n",
            "|    total_timesteps | 950272    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 465         |\n",
            "|    time_elapsed         | 30048       |\n",
            "|    total_timesteps      | 952320      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020876437 |\n",
            "|    clip_fraction        | 0.227       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.05       |\n",
            "|    explained_variance   | 1           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.54e+03    |\n",
            "|    n_updates            | 4640        |\n",
            "|    policy_gradient_loss | -0.00738    |\n",
            "|    std                  | 0.365       |\n",
            "|    value_loss           | 3.18e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 466         |\n",
            "|    time_elapsed         | 30091       |\n",
            "|    total_timesteps      | 954368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026646007 |\n",
            "|    clip_fraction        | 0.254       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.2e+03     |\n",
            "|    n_updates            | 4650        |\n",
            "|    policy_gradient_loss | -0.00393    |\n",
            "|    std                  | 0.362       |\n",
            "|    value_loss           | 3.09e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=955000, episode_reward=-50202.84 +/- 4.44\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 955000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02396217 |\n",
            "|    clip_fraction        | 0.234      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.01      |\n",
            "|    explained_variance   | 1          |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.85e+03   |\n",
            "|    n_updates            | 4660       |\n",
            "|    policy_gradient_loss | -0.00813   |\n",
            "|    std                  | 0.358      |\n",
            "|    value_loss           | 2.97e+03   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 467       |\n",
            "|    time_elapsed    | 30189     |\n",
            "|    total_timesteps | 956416    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 468         |\n",
            "|    time_elapsed         | 30232       |\n",
            "|    total_timesteps      | 958464      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022313317 |\n",
            "|    clip_fraction        | 0.261       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 1           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 921         |\n",
            "|    n_updates            | 4670        |\n",
            "|    policy_gradient_loss | -0.00593    |\n",
            "|    std                  | 0.363       |\n",
            "|    value_loss           | 2.9e+03     |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=960000, episode_reward=-50206.47 +/- 13.96\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 960000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032066606 |\n",
            "|    clip_fraction        | 0.258       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.05       |\n",
            "|    explained_variance   | 1           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.38e+03    |\n",
            "|    n_updates            | 4680        |\n",
            "|    policy_gradient_loss | -0.00194    |\n",
            "|    std                  | 0.363       |\n",
            "|    value_loss           | 2.8e+03     |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 469       |\n",
            "|    time_elapsed    | 30331     |\n",
            "|    total_timesteps | 960512    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 470         |\n",
            "|    time_elapsed         | 30375       |\n",
            "|    total_timesteps      | 962560      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022307176 |\n",
            "|    clip_fraction        | 0.246       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 1           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 986         |\n",
            "|    n_updates            | 4690        |\n",
            "|    policy_gradient_loss | -0.00523    |\n",
            "|    std                  | 0.366       |\n",
            "|    value_loss           | 2.77e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 471         |\n",
            "|    time_elapsed         | 30419       |\n",
            "|    total_timesteps      | 964608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.042799354 |\n",
            "|    clip_fraction        | 0.321       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.07       |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.4e+03     |\n",
            "|    n_updates            | 4700        |\n",
            "|    policy_gradient_loss | -0.00142    |\n",
            "|    std                  | 0.367       |\n",
            "|    value_loss           | 3.19e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=965000, episode_reward=-50210.49 +/- 9.01\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 965000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025989003 |\n",
            "|    clip_fraction        | 0.287       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.08       |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.57e+03    |\n",
            "|    n_updates            | 4710        |\n",
            "|    policy_gradient_loss | 0.000656    |\n",
            "|    std                  | 0.367       |\n",
            "|    value_loss           | 2.9e+03     |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 472       |\n",
            "|    time_elapsed    | 30518     |\n",
            "|    total_timesteps | 966656    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 473         |\n",
            "|    time_elapsed         | 30562       |\n",
            "|    total_timesteps      | 968704      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026050137 |\n",
            "|    clip_fraction        | 0.257       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.08       |\n",
            "|    explained_variance   | 1           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.41e+03    |\n",
            "|    n_updates            | 4720        |\n",
            "|    policy_gradient_loss | -0.00402    |\n",
            "|    std                  | 0.368       |\n",
            "|    value_loss           | 2.66e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=970000, episode_reward=-50210.80 +/- 15.08\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 970000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03119827 |\n",
            "|    clip_fraction        | 0.269      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.07      |\n",
            "|    explained_variance   | 1          |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.38e+03   |\n",
            "|    n_updates            | 4730       |\n",
            "|    policy_gradient_loss | -0.0117    |\n",
            "|    std                  | 0.367      |\n",
            "|    value_loss           | 2.47e+03   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 474       |\n",
            "|    time_elapsed    | 30662     |\n",
            "|    total_timesteps | 970752    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 475         |\n",
            "|    time_elapsed         | 30705       |\n",
            "|    total_timesteps      | 972800      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019472355 |\n",
            "|    clip_fraction        | 0.224       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 1           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 789         |\n",
            "|    n_updates            | 4740        |\n",
            "|    policy_gradient_loss | -0.0106     |\n",
            "|    std                  | 0.366       |\n",
            "|    value_loss           | 2.36e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 31         |\n",
            "|    iterations           | 476        |\n",
            "|    time_elapsed         | 30748      |\n",
            "|    total_timesteps      | 974848     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02423846 |\n",
            "|    clip_fraction        | 0.229      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.04      |\n",
            "|    explained_variance   | 1          |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 507        |\n",
            "|    n_updates            | 4750       |\n",
            "|    policy_gradient_loss | -0.00644   |\n",
            "|    std                  | 0.36       |\n",
            "|    value_loss           | 2.3e+03    |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=975000, episode_reward=-50208.58 +/- 11.85\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 975000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.050232902 |\n",
            "|    clip_fraction        | 0.386       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.26e+03    |\n",
            "|    n_updates            | 4760        |\n",
            "|    policy_gradient_loss | 0.0138      |\n",
            "|    std                  | 0.361       |\n",
            "|    value_loss           | 2.64e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 477       |\n",
            "|    time_elapsed    | 30847     |\n",
            "|    total_timesteps | 976896    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 31         |\n",
            "|    iterations           | 478        |\n",
            "|    time_elapsed         | 30890      |\n",
            "|    total_timesteps      | 978944     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17709708 |\n",
            "|    clip_fraction        | 0.514      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.03      |\n",
            "|    explained_variance   | 0.941      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 8.32e+04   |\n",
            "|    n_updates            | 4770       |\n",
            "|    policy_gradient_loss | 0.16       |\n",
            "|    std                  | 0.36       |\n",
            "|    value_loss           | 9.44e+04   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=980000, episode_reward=-50202.72 +/- 9.11\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1e+03      |\n",
            "|    mean_reward          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 980000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05503515 |\n",
            "|    clip_fraction        | 0.374      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.04      |\n",
            "|    explained_variance   | 0.993      |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 840        |\n",
            "|    n_updates            | 4780       |\n",
            "|    policy_gradient_loss | 0.018      |\n",
            "|    std                  | 0.361      |\n",
            "|    value_loss           | 4.38e+03   |\n",
            "----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 479       |\n",
            "|    time_elapsed    | 30988     |\n",
            "|    total_timesteps | 980992    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 31         |\n",
            "|    iterations           | 480        |\n",
            "|    time_elapsed         | 31031      |\n",
            "|    total_timesteps      | 983040     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02290222 |\n",
            "|    clip_fraction        | 0.25       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.04      |\n",
            "|    explained_variance   | 1          |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.19e+03   |\n",
            "|    n_updates            | 4790       |\n",
            "|    policy_gradient_loss | -0.00874   |\n",
            "|    std                  | 0.363      |\n",
            "|    value_loss           | 2.38e+03   |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=985000, episode_reward=-50210.23 +/- 15.99\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 985000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031644695 |\n",
            "|    clip_fraction        | 0.3         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.07       |\n",
            "|    explained_variance   | 1           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.26e+03    |\n",
            "|    n_updates            | 4800        |\n",
            "|    policy_gradient_loss | 0.00113     |\n",
            "|    std                  | 0.368       |\n",
            "|    value_loss           | 2.12e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 481       |\n",
            "|    time_elapsed    | 31130     |\n",
            "|    total_timesteps | 985088    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 482         |\n",
            "|    time_elapsed         | 31174       |\n",
            "|    total_timesteps      | 987136      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021240892 |\n",
            "|    clip_fraction        | 0.217       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | 1           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 754         |\n",
            "|    n_updates            | 4810        |\n",
            "|    policy_gradient_loss | -0.0117     |\n",
            "|    std                  | 0.372       |\n",
            "|    value_loss           | 2.05e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 483         |\n",
            "|    time_elapsed         | 31217       |\n",
            "|    total_timesteps      | 989184      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027481226 |\n",
            "|    clip_fraction        | 0.256       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.12       |\n",
            "|    explained_variance   | 1           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 454         |\n",
            "|    n_updates            | 4820        |\n",
            "|    policy_gradient_loss | -0.00662    |\n",
            "|    std                  | 0.372       |\n",
            "|    value_loss           | 2.13e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=990000, episode_reward=-50210.84 +/- 15.73\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 990000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026608031 |\n",
            "|    clip_fraction        | 0.244       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 1           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.23e+03    |\n",
            "|    n_updates            | 4830        |\n",
            "|    policy_gradient_loss | -0.0109     |\n",
            "|    std                  | 0.373       |\n",
            "|    value_loss           | 2.02e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 484       |\n",
            "|    time_elapsed    | 31316     |\n",
            "|    total_timesteps | 991232    |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 485         |\n",
            "|    time_elapsed         | 31359       |\n",
            "|    total_timesteps      | 993280      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025136441 |\n",
            "|    clip_fraction        | 0.248       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 1           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.4e+03     |\n",
            "|    n_updates            | 4840        |\n",
            "|    policy_gradient_loss | -0.0108     |\n",
            "|    std                  | 0.372       |\n",
            "|    value_loss           | 1.91e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=995000, episode_reward=-50204.83 +/- 12.76\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 995000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020503638 |\n",
            "|    clip_fraction        | 0.24        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 1           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.15e+03    |\n",
            "|    n_updates            | 4850        |\n",
            "|    policy_gradient_loss | -0.00995    |\n",
            "|    std                  | 0.374       |\n",
            "|    value_loss           | 1.88e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 486       |\n",
            "|    time_elapsed    | 31459     |\n",
            "|    total_timesteps | 995328    |\n",
            "----------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1e+03      |\n",
            "|    ep_rew_mean          | -5.02e+04  |\n",
            "| time/                   |            |\n",
            "|    fps                  | 31         |\n",
            "|    iterations           | 487        |\n",
            "|    time_elapsed         | 31503      |\n",
            "|    total_timesteps      | 997376     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01963307 |\n",
            "|    clip_fraction        | 0.218      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.14      |\n",
            "|    explained_variance   | 1          |\n",
            "|    learning_rate        | 0.0005     |\n",
            "|    loss                 | 1.04e+03   |\n",
            "|    n_updates            | 4860       |\n",
            "|    policy_gradient_loss | -0.00658   |\n",
            "|    std                  | 0.37       |\n",
            "|    value_loss           | 1.88e+03   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1e+03       |\n",
            "|    ep_rew_mean          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 31          |\n",
            "|    iterations           | 488         |\n",
            "|    time_elapsed         | 31546       |\n",
            "|    total_timesteps      | 999424      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018704627 |\n",
            "|    clip_fraction        | 0.22        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 1           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 1.29e+03    |\n",
            "|    n_updates            | 4870        |\n",
            "|    policy_gradient_loss | -0.00676    |\n",
            "|    std                  | 0.367       |\n",
            "|    value_loss           | 1.77e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=1000000, episode_reward=-50191.85 +/- 7.13\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | -5.02e+04   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 1000000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022009078 |\n",
            "|    clip_fraction        | 0.215       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | 1           |\n",
            "|    learning_rate        | 0.0005      |\n",
            "|    loss                 | 812         |\n",
            "|    n_updates            | 4880        |\n",
            "|    policy_gradient_loss | -0.00938    |\n",
            "|    std                  | 0.363       |\n",
            "|    value_loss           | 1.71e+03    |\n",
            "-----------------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 31        |\n",
            "|    iterations      | 489       |\n",
            "|    time_elapsed    | 31644     |\n",
            "|    total_timesteps | 1001472   |\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Simple DQN architecture\n",
        "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
        "\n",
        "# Set up the model with simpler hyperparameters\n",
        "model = PPO('CnnPolicy', custom_env, policy_kwargs=policy_kwargs,\n",
        "            verbose=1, learning_rate=0.0005, batch_size=32)\n",
        "\n",
        "# Setup evaluation and checkpoint callbacks\n",
        "eval_callback = EvalCallback(custom_env, best_model_save_path=model_dir,\n",
        "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
        "                             deterministic=True, render=False)\n",
        "\n",
        "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
        "                                         name_prefix='ppo_model_checkpoint')\n",
        "\n",
        "\n",
        "\n",
        "# Start training the model with callbacks for evaluation and checkpoints\n",
        "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
        "\n",
        "# Save the final model after training\n",
        "model.save(\"ppo_custom_env_model\")\n",
        "custom_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGiErpWg5iaT"
      },
      "outputs": [],
      "source": [
        "env.close()\n",
        "del env\n",
        "foo = gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOx5Z_Dn5iaU"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45S3gsi75iaV"
      },
      "source": [
        "# 3. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEtsmtRc5iaV"
      },
      "outputs": [],
      "source": [
        "# Adjust number of episodes based on the environment's characteristics\n",
        "if hasattr(env, \"max_episode_steps\"):\n",
        "    # If the environment has predefined max steps, use a higher number for evaluation\n",
        "    num_episodes = 50\n",
        "else:\n",
        "    # For simpler environments, use fewer episodes\n",
        "    num_episodes = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfI_2wYn5iaW"
      },
      "outputs": [],
      "source": [
        "model = foo #load best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaj9d6bA5iaX"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "episode_rewards = []\n",
        "for episode in range(num_episodes):\n",
        "    total_reward = 0\n",
        "    while True:\n",
        "        action = model.predict(obs)  # Use trained policy\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        record_agent_dynamics(env)  # Record smoothness metrics\n",
        "        if done:\n",
        "            break\n",
        "    episode_rewards.append(total_reward)\n",
        "    obs = env.reset()\n",
        "\n",
        "print(f\"Average Reward: {np.mean(episode_rewards)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwQVoKey5iaX"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjCbHD-f5iaX"
      },
      "source": [
        "# Gravar os video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5h6hfRmg5iaX",
        "outputId": "e6f6990f-72ee-486b-ffec-79d10864a9f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/psuper/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.77GB > 0.74GB\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running episode 0\n"
          ]
        }
      ],
      "source": [
        "# DQN original\n",
        "\n",
        "# Define a Environment\n",
        "env = gym.make(\"CarRacing-v3\", continuous=False, render_mode='rgb_array')\n",
        "\n",
        "# Video recorder wrapper\n",
        "trigger = lambda t: t == 0\n",
        "env = RecordVideo(\n",
        "    env,\n",
        "    video_folder=f\"./recordings/original/DQN\",\n",
        "    episode_trigger=trigger,\n",
        "    video_length=0,\n",
        "    disable_logger=True,\n",
        ")\n",
        "\n",
        "model = DQN.load(path=\"best_model/best_model_2.1.1.zip\", env=env)\n",
        "\n",
        "\n",
        "# Perform N Episodes\n",
        "for ep in range(1):\n",
        "    print(\"Running episode\", ep)\n",
        "    obs, info = env.reset()\n",
        "    trunc = False\n",
        "    while not trunc:\n",
        "        # pass observation to model to get predicted action\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "\n",
        "        # pass action to env and get info back\n",
        "        obs, rewards, trunc, done, info = env.step(action)\n",
        "\n",
        "        # show the environment on the screen\n",
        "        env.render()\n",
        "        # print(ep, rewards, trunc)\n",
        "        # print(\"---------------\")\n",
        "\n",
        "    # Close the Environment\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blVMXb6U5iaY",
        "outputId": "f76bae00-13b8-4069-d690-3c30da47aa81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/psuper/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/psuper/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/reinforcement-learning-with-gymnasium/recordings/PPO folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/home/psuper/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: code expected at most 16 arguments, got 18\n",
            "  warnings.warn(\n",
            "/home/psuper/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: code expected at most 16 arguments, got 18\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Action spaces do not match: Box([-1.  0.  0.], 1.0, (3,), float32) != Discrete(5)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m      9\u001b[0m trigger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m t: t \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m env \u001b[38;5;241m=\u001b[39m RecordVideo(\n\u001b[1;32m     11\u001b[0m     env,\n\u001b[1;32m     12\u001b[0m     video_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./recordings/PPO\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     disable_logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_model/best_model_2.2.2.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Perform N Episodes\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n",
            "File \u001b[0;32m~/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:716\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[0;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_env(env, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# Check if given env is valid\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m \u001b[43mcheck_for_correct_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservation_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Discard `_last_obs`, this will force the env to reset before training\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;66;03m# See issue https://github.com/DLR-RM/stable-baselines3/issues/597\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_reset \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/stable_baselines3/common/utils.py:233\u001b[0m, in \u001b[0;36mcheck_for_correct_spaces\u001b[0;34m(env, observation_space, action_space)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation spaces do not match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_space \u001b[38;5;241m!=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space:\n\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction spaces do not match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: Action spaces do not match: Box([-1.  0.  0.], 1.0, (3,), float32) != Discrete(5)"
          ]
        }
      ],
      "source": [
        "# PPO custom\n",
        "\n",
        "# Define a Environment\n",
        "env = EnhancedCarRacing(render_mode=\"rgb_array\")\n",
        "env = GrayscaleObservation(env, keep_dim=True)\n",
        "# env = TimeLimit(env, max_episode_steps=1000)\n",
        "\n",
        "# Video recorder wrapper\n",
        "trigger = lambda t: t == 0\n",
        "env = RecordVideo(\n",
        "    env,\n",
        "    video_folder=f\"./recordings/custom/PPO\",\n",
        "    episode_trigger=trigger,\n",
        "    video_length=0,\n",
        "    disable_logger=True,\n",
        ")\n",
        "\n",
        "model = PPO.load(path=\"best_model/best_model_2.2.2.zip\", env=env)\n",
        "\n",
        "\n",
        "# Perform N Episodes\n",
        "for ep in range(1):\n",
        "    print(\"Running episode\", ep)\n",
        "    obs, info = env.reset()\n",
        "    trunc = False\n",
        "    while not trunc:\n",
        "        # pass observation to model to get predicted action\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "\n",
        "        # pass action to env and get info back\n",
        "        obs, rewards, trunc, done, info = env.step(action)\n",
        "\n",
        "        # show the environment on the screen\n",
        "        env.render()\n",
        "        # print(ep, rewards, trunc)\n",
        "        # print(\"---------------\")\n",
        "\n",
        "    # Close the Environment\n",
        "    env.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "430452755d6a43de8022bdc6814372e0": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_84f59904e1ec48e3abdce717dc6d0b18",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[35m 100%\u001b[0m \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100,000/100,000 \u001b[0m [ \u001b[33m2:14:43\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">100,000/100,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">2:14:43</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span> , <span style=\"color: #800000; text-decoration-color: #800000\">? it/s</span> ]\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "84f59904e1ec48e3abdce717dc6d0b18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}