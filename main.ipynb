{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing OpenAI Gym Environments and Implementing Reinforcement Learning Agents with Stable Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme: Car Racing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Constança\n",
    "- Daniela Osório, 202208679\n",
    "- Inês Amorim, 202108108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
      "\u001b[0mFiles removed: 0\n"
     ]
    }
   ],
   "source": [
    "!pip cache purge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Union\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.box2d.car_dynamics import Car\n",
    "from gymnasium.error import DependencyNotInstalled, InvalidAction\n",
    "from gymnasium.utils import EzPickle\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import pygame\n",
    "from pygame import gfxdraw\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "#from pyvirtualdisplay import Display\n",
    "from stable_baselines3 import A2C, PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecVideoRecorder\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3.common.logger import configure\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CarRacing-v3 environment from Gymnasium (previously Gym) is part of the Box2D environments, and it offers an interesting challenge for training reinforcement learning agents. It's a top-down racing simulation where the track is randomly generated at the start of each episode. The environment offers both continuous and discrete action spaces, making it adaptable to different types of reinforcement learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Action Space:**\n",
    "\n",
    "   - **Continuous:** Three actions: steering, gas, and braking. Steering ranges from -1 (full left) to +1 (full right).\n",
    "   -  **Discrete:** Five possible actions: do nothing, steer left, steer right, gas, and brake.\n",
    "\n",
    "- **Observation Space:**\n",
    "\n",
    "    - The environment provides a 96x96 RGB image of the car and the track, which serves as the state input for the agent.\n",
    "\n",
    "- **Rewards:**\n",
    "\n",
    "    - The agent receives a -0.1 penalty for every frame, encouraging efficiency.\n",
    "    - It earns a positive reward for visiting track tiles: the formula is Reward=1000−0.1×framesReward=1000−0.1×frames, where \"frames\" is the number of frames taken to complete the lap. The reward for completing a lap depends on how many track tiles are visited.\n",
    "\n",
    "- Episode Termination:\n",
    "\n",
    "    - The episode ends either when all track tiles are visited or if the car goes off the track, which incurs a significant penalty (-100 reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False, render_mode='rgb_array') \n",
    "obs, info = env.reset()\n",
    "#continuous = False to use Discrete space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'rgb_array', 'state_pixels']\n"
     ]
    }
   ],
   "source": [
    "#check render modes\n",
    "print(env.metadata[\"render_modes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking if everything is okay and working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment and render the first frame\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "print(\"Environment initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(5)\n",
      "Observation Space: Box(0, 255, (96, 96, 3), uint8)\n",
      "Environment Metadata: {'render_modes': ['human', 'rgb_array', 'state_pixels'], 'render_fps': 50}\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Environment Metadata:\", env.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(10):\n",
    "    \"\"\"action = env.action_space.sample()  # Random action\n",
    "    print(f\"Action before step: {action}, Type: {type(action)}\")\n",
    "    obs, reward, done, info = env.step(action)\"\"\"\n",
    "    env.step(env.action_space.sample())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Training with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = '../../labiacd/models/#isia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'DQN_baseline_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "#video_dir = os.path.join(logs_path, \"videos\")\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "#os.makedirs(video_dir, exist_ok=True)\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.77GB > 2.74GB\n",
      "  warnings.warn(\n",
      "[W1215 19:01:32.741724176 NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x797289c0d600> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x79728cd2fc70>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -56.9    |\n",
      "|    exploration_rate | 0.962    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 18       |\n",
      "|    time_elapsed     | 219      |\n",
      "|    total_timesteps  | 4000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 6.54e-05 |\n",
      "|    n_updates        | 749      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=-38.60 +/- 52.41\n",
      "Episode length: 341.20 +/- 37.61\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 341      |\n",
      "|    mean_reward      | -38.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.953    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000218 |\n",
      "|    n_updates        | 999      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -53.3    |\n",
      "|    exploration_rate | 0.924    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 14       |\n",
      "|    time_elapsed     | 539      |\n",
      "|    total_timesteps  | 8000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000138 |\n",
      "|    n_updates        | 1749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-90.26 +/- 5.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -90.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.905    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000215 |\n",
      "|    n_updates        | 2249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -50.3    |\n",
      "|    exploration_rate | 0.886    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 954      |\n",
      "|    total_timesteps  | 12000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0933   |\n",
      "|    n_updates        | 2749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-84.36 +/- 3.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -84.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.858    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 3.37e-05 |\n",
      "|    n_updates        | 3499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -51.2    |\n",
      "|    exploration_rate | 0.848    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 1373     |\n",
      "|    total_timesteps  | 16000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000128 |\n",
      "|    n_updates        | 3749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-93.02 +/- 0.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -93      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 7.06e-05 |\n",
      "|    n_updates        | 4749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -51.6    |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 1740     |\n",
      "|    total_timesteps  | 20000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -50.8    |\n",
      "|    exploration_rate | 0.772    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 1936     |\n",
      "|    total_timesteps  | 24000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000118 |\n",
      "|    n_updates        | 5749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-92.59 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -92.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.763    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 25000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 5.43e-05 |\n",
      "|    n_updates        | 5999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -49.7    |\n",
      "|    exploration_rate | 0.734    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 2202     |\n",
      "|    total_timesteps  | 28000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.09     |\n",
      "|    n_updates        | 6749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-56.63 +/- 14.58\n",
      "Episode length: 337.40 +/- 46.71\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 337      |\n",
      "|    mean_reward      | -56.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.715    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000112 |\n",
      "|    n_updates        | 7249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -47.8    |\n",
      "|    exploration_rate | 0.696    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2410     |\n",
      "|    total_timesteps  | 32000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 8.02e-05 |\n",
      "|    n_updates        | 7749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-81.72 +/- 2.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -81.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.668    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 35000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 7.83e-05 |\n",
      "|    n_updates        | 8499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -47.4    |\n",
      "|    exploration_rate | 0.658    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2668     |\n",
      "|    total_timesteps  | 36000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0899   |\n",
      "|    n_updates        | 8749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-93.17 +/- 0.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -93.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.12     |\n",
      "|    n_updates        | 9749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -45.2    |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2933     |\n",
      "|    total_timesteps  | 40000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -44.4    |\n",
      "|    exploration_rate | 0.582    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 14       |\n",
      "|    time_elapsed     | 3103     |\n",
      "|    total_timesteps  | 44000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0631   |\n",
      "|    n_updates        | 10749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-93.20 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -93.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.573    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 45000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 4.93e-05 |\n",
      "|    n_updates        | 10999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -43.8    |\n",
      "|    exploration_rate | 0.544    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 14       |\n",
      "|    time_elapsed     | 3338     |\n",
      "|    total_timesteps  | 48000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.000208 |\n",
      "|    n_updates        | 11749    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simple DQN architecture\n",
    "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
    "\n",
    "# Set up the model with simpler hyperparameters\n",
    "model = DQN('CnnPolicy', env, policy_kwargs=policy_kwargs, \n",
    "            verbose=1, buffer_size=50000, learning_starts=1000, \n",
    "            learning_rate=0.0005, batch_size=32, exploration_fraction=0.1,\n",
    "            exploration_final_eps=0.05)\n",
    "\n",
    "# Setup evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(env, best_model_save_path=model_dir,\n",
    "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
    "                                         name_prefix='dqn_model_checkpoint')\n",
    "\n",
    "# Start training the model with callbacks for evaluation and checkpoints\n",
    "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the final model after training\n",
    "model.save(\"dqn_car_racing_model\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3212"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.close()\n",
    "del env\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treinar o modelo e guardar logs e vídeos\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"PPO_baseline\")\n",
    "model.save(f\"{models_dir}/{TIMESTEPS*i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "model = PPO.load(\"trained_model_ppo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store model\n",
    "model.save(\"trained_model_ppo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the environment\n",
    "env = gym.make('CarRacing-v3')  # continuous: LunarLanderContinuous-v2\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "treinar modelo sem se ver treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ver treino, img giras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parte para se ver \n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "\tobs = env.reset()\n",
    "\tdone = False\n",
    "\twhile not done:\n",
    "\t\taction, _states = model.predict(obs)\n",
    "\t\tobs, rewards, done, info = env.step(action)\n",
    "\t\tenv.render()  #permite ver as animações\n",
    "\t\tprint(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
