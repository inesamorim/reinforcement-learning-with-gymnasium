{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing OpenAI Gym Environments and Implementing Reinforcement Learning Agents with Stable Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme: Car Racing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Constança\n",
    "- Daniela Osório, 202208679\n",
    "- Inês Amorim, 202108108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import GrayscaleObservation, ResizeObservation, RecordEpisodeStatistics, RecordVideo\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "import os\n",
    "import gc\n",
    "from eval import *\n",
    "#from custom_cr import CustomCarRacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CarRacing-v3 environment from Gymnasium (previously Gym) is part of the Box2D environments, and it offers an interesting challenge for training reinforcement learning agents. It's a top-down racing simulation where the track is randomly generated at the start of each episode. The environment offers both continuous and discrete action spaces, making it adaptable to different types of reinforcement learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Action Space:**\n",
    "\n",
    "   - **Continuous:** Three actions: steering, gas, and braking. Steering ranges from -1 (full left) to +1 (full right).\n",
    "   -  **Discrete:** Five possible actions: do nothing, steer left, steer right, gas, and brake.\n",
    "\n",
    "- **Observation Space:**\n",
    "\n",
    "    - The environment provides a 96x96 RGB image of the car and the track, which serves as the state input for the agent.\n",
    "\n",
    "- **Rewards:**\n",
    "\n",
    "    - The agent receives a -0.1 penalty for every frame, encouraging efficiency.\n",
    "    - It earns a positive reward for visiting track tiles: the formula is Reward=1000−0.1×framesReward=1000−0.1×frames, where \"frames\" is the number of frames taken to complete the lap. The reward for completing a lap depends on how many track tiles are visited.\n",
    "\n",
    "- Episode Termination:\n",
    "\n",
    "    - The episode ends either when all track tiles are visited or if the car goes off the track, which incurs a significant penalty (-100 reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False, render_mode='rgb_array') \n",
    "obs, info = env.reset()\n",
    "#continuous = False to use Discrete space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'rgb_array', 'state_pixels']\n"
     ]
    }
   ],
   "source": [
    "#check render modes\n",
    "print(env.metadata[\"render_modes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking if everything is okay and working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment and render the first frame\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "print(\"Environment initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(5)\n",
      "Observation Space: Box(0, 255, (96, 96, 3), uint8)\n",
      "Environment Metadata: {'render_modes': ['human', 'rgb_array', 'state_pixels'], 'render_fps': 50}\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Environment Metadata:\", env.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(10):\n",
    "    \"\"\"action = env.action_space.sample()  # Random action\n",
    "    print(f\"Action before step: {action}, Type: {type(action)}\")\n",
    "    obs, reward, done, info = env.step(action)\"\"\"\n",
    "    env.step(env.action_space.sample())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Training with DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-Learning (DQN) is a reinforcement learning algorithm that extends the traditional Q-Learning method using neural networks to approximate the Q-values for state-action pairs.\n",
    "DQN is inherently designed for discrete action spaces, as the neural network outputs a separate Q-value for each action. For each state, the algorithm selects actions based on the highest Q-value, making it ideal for problems where actions are discrete and finite. This is a key advantage compared to other reinforcement learning methods, which may require modifications or different approaches for discrete action selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = '../../labiacd/models/#isia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(obs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'DQN_baseline_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "video_dir = os.path.join(logs_path, \"videos\")\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple DQN architecture\n",
    "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
    "\n",
    "# Set up the model with simpler hyperparameters\n",
    "model = DQN('CnnPolicy', env, policy_kwargs=policy_kwargs, \n",
    "            verbose=1, buffer_size=50000, learning_starts=1000, \n",
    "            learning_rate=0.0005, batch_size=32, exploration_fraction=0.1,\n",
    "            exploration_final_eps=0.05)\n",
    "\n",
    "# Setup evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(env, best_model_save_path=model_dir,\n",
    "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
    "                                         name_prefix='dqn_model_checkpoint')\n",
    "\n",
    "\n",
    "\n",
    "# Start training the model with callbacks for evaluation and checkpoints\n",
    "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the final model after training\n",
    "model.save(\"dqn_car_racing_model\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "del env\n",
    "foo = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(obs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'PPO_baseline_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "video_dir = os.path.join(logs_path, \"videos\")\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ResizeObservation(env, shape=(64,64))\n",
    "env = GrayscaleObservation(env, keep_dim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = RecordVideo(env, video_folder=video_dir, episode_trigger= lambda t: t % 100000 == 0, disable_logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7bd24c3f0940> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7bd243dc0040>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -49.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 46       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -46.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 38          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011774453 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.0157      |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.463       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 0.947       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -35.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 36          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 167         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009177452 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0301      |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.207       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 0.716       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -31.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 37          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 220         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014188305 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0719      |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.292       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 0.99        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=-30.39 +/- 39.60\n",
      "Episode length: 317.00 +/- 45.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 317         |\n",
      "|    mean_reward          | -30.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015941966 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.27        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.245       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 0.628       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -30.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 33       |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 304      |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -30.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 34         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 353        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02742214 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.48      |\n",
      "|    explained_variance   | 0.474      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 0.192      |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    value_loss           | 0.723      |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -31          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 35           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 409          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0115716085 |\n",
      "|    clip_fraction        | 0.108        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.755        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 0.0742       |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00872     |\n",
      "|    value_loss           | 0.438        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -30.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 35          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 466         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013159602 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.13        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 0.396       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -31         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 34          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 535         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035409406 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.0254      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00649    |\n",
      "|    value_loss           | 0.23        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-16.98 +/- 19.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -17       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 20000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0402839 |\n",
      "|    clip_fraction        | 0.208     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.49     |\n",
      "|    explained_variance   | 0.958     |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 0.0596    |\n",
      "|    n_updates            | 90        |\n",
      "|    policy_gradient_loss | -0.00871  |\n",
      "|    value_loss           | 0.27      |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -28.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 731      |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -27.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 28           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 792          |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0080212215 |\n",
      "|    clip_fraction        | 0.0783       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.962        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 0.0532       |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00373     |\n",
      "|    value_loss           | 0.511        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -27.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 852         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029258445 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.0477      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00812    |\n",
      "|    value_loss           | 0.272       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -27.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 911         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019609392 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.0116      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 0.221       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -27.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 29         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 970        |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01116706 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.43      |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 0.045      |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.00807   |\n",
      "|    value_loss           | 0.217      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-46.98 +/- 20.62\n",
      "Episode length: 760.80 +/- 293.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 761         |\n",
      "|    mean_reward          | -47         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018782444 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.004      |\n",
      "|    value_loss           | 0.347       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -27.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 1120     |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -27.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 1231        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020227239 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | -0.0096     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -26.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 1289        |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015232333 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.0522      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00903    |\n",
      "|    value_loss           | 0.165       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -24.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 27         |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 1356       |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01896347 |\n",
      "|    clip_fraction        | 0.19       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.41      |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 0.128      |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | 0.00219    |\n",
      "|    value_loss           | 0.545      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -10.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 1416        |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040104154 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.335       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | 0.00179     |\n",
      "|    value_loss           | 3.06        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=214.95 +/- 30.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 215         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013494495 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.644       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00205    |\n",
      "|    value_loss           | 8.21        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -5.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 25       |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 1588     |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.21       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 1649        |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020045765 |\n",
      "|    clip_fraction        | 0.0987      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.0294      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 0.42        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -4.07       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 1717        |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017764555 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.000463   |\n",
      "|    value_loss           | 0.803       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.76       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 1791        |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015085956 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.136       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00582    |\n",
      "|    value_loss           | 0.578       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.53      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 26         |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 1865       |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01538844 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 0.111      |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.00431   |\n",
      "|    value_loss           | 0.648      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=3.88 +/- 46.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.88        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030957974 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.0323      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00437    |\n",
      "|    value_loss           | 0.447       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -6.96    |\n",
      "| time/              |          |\n",
      "|    fps             | 24       |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 2059     |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -6.96       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 2130        |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008944231 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.091       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0058     |\n",
      "|    value_loss           | 0.439       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.15       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 2190        |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018635813 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.13        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00991    |\n",
      "|    value_loss           | 0.423       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -6.31       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 2256        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010040279 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.0345      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0086     |\n",
      "|    value_loss           | 0.268       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -3.86       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 2322        |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015851092 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.328       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0072     |\n",
      "|    value_loss           | 1.38        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=266.23 +/- 161.91\n",
      "Episode length: 704.80 +/- 225.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 705         |\n",
      "|    mean_reward          | 266         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018083064 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00277    |\n",
      "|    value_loss           | 4.76        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 24       |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 2484     |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 6.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 2552        |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011401081 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.339       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00582    |\n",
      "|    value_loss           | 3.76        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 14          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 2620        |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016870148 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.943       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00253    |\n",
      "|    value_loss           | 5.27        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 16          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 2686        |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018329155 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.257       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00681    |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 15.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 2747        |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016111588 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.42        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00525    |\n",
      "|    value_loss           | 3.72        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=115.93 +/- 47.75\n",
      "Episode length: 596.00 +/- 63.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 596         |\n",
      "|    mean_reward          | 116         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015606383 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.66        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00618    |\n",
      "|    value_loss           | 1.71        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 16.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 24       |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 2876     |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 21.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 2935        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014147373 |\n",
      "|    clip_fraction        | 0.0697      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.153       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00389    |\n",
      "|    value_loss           | 2.48        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 26.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 2999       |\n",
      "|    total_timesteps      | 75776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02203308 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.36      |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 0.148      |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | 0.00139    |\n",
      "|    value_loss           | 0.766      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 31.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 3061        |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030777263 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.87        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00361    |\n",
      "|    value_loss           | 4.22        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 34.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 3120       |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01435853 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.28      |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 0.264      |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    value_loss           | 2.09       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=366.60 +/- 298.39\n",
      "Episode length: 716.40 +/- 178.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 716         |\n",
      "|    mean_reward          | 367         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024619859 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.166       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.006      |\n",
      "|    value_loss           | 1.26        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 36.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 25       |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 3264     |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 41          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 3327        |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013650333 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.729       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00581    |\n",
      "|    value_loss           | 5.88        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 57.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 3384        |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021628253 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.966      |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | 0.00192     |\n",
      "|    value_loss           | 13.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 64.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 3439        |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023367906 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00112    |\n",
      "|    value_loss           | 8.48        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=210.68 +/- 294.32\n",
      "Episode length: 555.80 +/- 251.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 556         |\n",
      "|    mean_reward          | 211         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 90000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010317318 |\n",
      "|    clip_fraction        | 0.0966      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.4         |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | 0.000142    |\n",
      "|    value_loss           | 12.5        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 999      |\n",
      "|    ep_rew_mean     | 65.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 25       |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 3550     |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 66.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 3611        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013958021 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 28.4        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | 0.0241      |\n",
      "|    value_loss           | 32.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 71.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 3680        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017423874 |\n",
      "|    clip_fraction        | 0.0992      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.12        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00331    |\n",
      "|    value_loss           | 1.42        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 72.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 3735       |\n",
      "|    total_timesteps      | 96256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01214676 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.14       |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.00522   |\n",
      "|    value_loss           | 10.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 76.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 3797        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012359132 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.0948      |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 1.69        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=261.27 +/- 140.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 261         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015193451 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.425       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00978    |\n",
      "|    value_loss           | 3.39        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 80.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 25       |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 3977     |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 84.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 4045        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028285233 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.731       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 6.97        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 91          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 4102        |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029000469 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.167       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00828    |\n",
      "|    value_loss           | 1.99        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 94.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 4168        |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015763476 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.269       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.00992    |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 98          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 4231        |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026391374 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.279       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=434.31 +/- 223.12\n",
      "Episode length: 875.60 +/- 104.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 876       |\n",
      "|    mean_reward          | 434       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 110000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0259486 |\n",
      "|    clip_fraction        | 0.205     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.14     |\n",
      "|    explained_variance   | 0.97      |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 0.3       |\n",
      "|    n_updates            | 530       |\n",
      "|    policy_gradient_loss | -0.0121   |\n",
      "|    value_loss           | 4.08      |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 100      |\n",
      "| time/              |          |\n",
      "|    fps             | 25       |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 4410     |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 104         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 4467        |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028353978 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.184       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 1.03        |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 1e+03    |\n",
      "|    ep_rew_mean          | 108      |\n",
      "| time/                   |          |\n",
      "|    fps                  | 25       |\n",
      "|    iterations           | 56       |\n",
      "|    time_elapsed         | 4529     |\n",
      "|    total_timesteps      | 114688   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.028653 |\n",
      "|    clip_fraction        | 0.222    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.13    |\n",
      "|    explained_variance   | 0.96     |\n",
      "|    learning_rate        | 0.0005   |\n",
      "|    loss                 | 0.406    |\n",
      "|    n_updates            | 550      |\n",
      "|    policy_gradient_loss | -0.00902 |\n",
      "|    value_loss           | 2.85     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 114        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 57         |\n",
      "|    time_elapsed         | 4593       |\n",
      "|    total_timesteps      | 116736     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08469571 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.12      |\n",
      "|    explained_variance   | 0.956      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 0.381      |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0197    |\n",
      "|    value_loss           | 2.93       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 123        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 58         |\n",
      "|    time_elapsed         | 4649       |\n",
      "|    total_timesteps      | 118784     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01983897 |\n",
      "|    clip_fraction        | 0.19       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.06      |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 0.618      |\n",
      "|    n_updates            | 570        |\n",
      "|    policy_gradient_loss | -0.00597   |\n",
      "|    value_loss           | 4.18       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=163.69 +/- 157.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 164         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019048583 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.575       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 5.16        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 127      |\n",
      "| time/              |          |\n",
      "|    fps             | 25       |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 4810     |\n",
      "|    total_timesteps | 120832   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 130         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 4866        |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017293347 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.36        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.00845    |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 138         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 4928        |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026687706 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.397       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 1.91        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 143         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 5003        |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021758882 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.936      |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.452       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 5.29        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 148         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 5062        |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018562432 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.184       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-13.40 +/- 19.85\n",
      "Episode length: 750.40 +/- 224.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 750         |\n",
      "|    mean_reward          | -13.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 130000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021595534 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.267       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 150      |\n",
      "| time/              |          |\n",
      "|    fps             | 25       |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 5216     |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 152         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 5275        |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023538481 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.386       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.00709    |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 155         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 5336        |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025082838 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 0.262       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 1.34        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simple DQN architecture\n",
    "policy_kwargs = dict(net_arch=[256])  # Simpler architecture with 2 layers of 64 units\n",
    "\n",
    "# Set up the model with simpler hyperparameters\n",
    "model = PPO('CnnPolicy', env, policy_kwargs=policy_kwargs, \n",
    "            verbose=1, learning_rate=0.0005, batch_size=128)\n",
    "\n",
    "# Setup evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(env, best_model_save_path=model_dir,\n",
    "                             log_path=model_dir, eval_freq=10000, n_eval_episodes=5,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
    "                                         name_prefix='model_checkpoint')\n",
    "\n",
    "\n",
    "\n",
    "# Start training the model with callbacks for evaluation and checkpoints\n",
    "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the final model after training\n",
    "model.save(\"ppo_baseline_model\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "del env\n",
    "foo = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Environment Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'DQN_env_mod_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Rewards Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'DQN_rewards_mod_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Environment and Rewards Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'DQN_env_rewards_mod_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust number of episodes based on the environment's characteristics\n",
    "if hasattr(env, \"max_episode_steps\"):\n",
    "    # If the environment has predefined max steps, use a higher number for evaluation\n",
    "    num_episodes = 50  \n",
    "else:\n",
    "    # For simpler environments, use fewer episodes\n",
    "    num_episodes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = foo #load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "episode_rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = model.predict(obs)  # Use trained policy\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        record_agent_dynamics(env)  # Record smoothness metrics\n",
    "        if done:\n",
    "            break\n",
    "    episode_rewards.append(total_reward)\n",
    "    obs = env.reset()\n",
    "\n",
    "print(f\"Average Reward: {np.mean(episode_rewards)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
