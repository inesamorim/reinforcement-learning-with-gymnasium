{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing OpenAI Gym Environments and Implementing Reinforcement Learning Agents with Stable Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme: Car Racing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Constança\n",
    "- Daniela Osório, 202208679\n",
    "- Inês Amorim, 202108108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import GrayscaleObservation, ResizeObservation, RecordEpisodeStatistics, RecordVideo, TimeLimit\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "import os\n",
    "import gc\n",
    "from eval import *\n",
    "from custom_cr import EnhancedCarRacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CarRacing-v3 environment from Gymnasium (previously Gym) is part of the Box2D environments, and it offers an interesting challenge for training reinforcement learning agents. It's a top-down racing simulation where the track is randomly generated at the start of each episode. The environment offers both continuous and discrete action spaces, making it adaptable to different types of reinforcement learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False, render_mode='rgb_array') \n",
    "obs, info = env.reset()\n",
    "#continuous = False to use Discrete space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Action Space:**\n",
    "\n",
    "   - **Continuous:** Three actions: steering, gas, and braking. Steering ranges from -1 (full left) to +1 (full right).\n",
    "   -  **Discrete:** Five possible actions: do nothing, steer left, steer right, gas, and brake.\n",
    "\n",
    "- **Observation Space:**\n",
    "\n",
    "    - The environment provides a 96x96 RGB image of the car and the track, which serves as the state input for the agent.\n",
    "\n",
    "- **Rewards:**\n",
    "\n",
    "    - The agent receives a -0.1 penalty for every frame, encouraging efficiency.\n",
    "    - It earns a positive reward for visiting track tiles: the formula is Reward=1000−0.1×framesReward=1000−0.1×frames, where \"frames\" is the number of frames taken to complete the lap. The reward for completing a lap depends on how many track tiles are visited.\n",
    "\n",
    "- **Episode Termination:**\n",
    "\n",
    "    - The episode ends either when all track tiles are visited or if the car goes off the track, which incurs a significant penalty (-100 reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'rgb_array', 'state_pixels']\n"
     ]
    }
   ],
   "source": [
    "#check render modes\n",
    "print(env.metadata[\"render_modes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking if everything is okay and working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment and render the first frame\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "print(\"Environment initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(5)\n",
      "Observation Space: Box(0, 255, (96, 96, 3), uint8)\n",
      "Environment Metadata: {'render_modes': ['human', 'rgb_array', 'state_pixels'], 'render_fps': 50}\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Environment Metadata:\", env.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(10):\n",
    "    \"\"\"action = env.action_space.sample()  # Random action\n",
    "    print(f\"Action before step: {action}, Type: {type(action)}\")\n",
    "    obs, reward, done, info = env.step(action)\"\"\"\n",
    "    env.step(env.action_space.sample())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-Learning (DQN) is a reinforcement learning algorithm that extends the traditional Q-Learning method using neural networks to approximate the Q-values for state-action pairs.\n",
    "DQN is inherently designed for discrete action spaces, as the neural network outputs a separate Q-value for each action. For each state, the algorithm selects actions based on the highest Q-value, making it ideal for problems where actions are discrete and finite. This is a key advantage compared to other reinforcement learning methods, which may require modifications or different approaches for discrete action selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = '../2.2.1/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvSpec(id='CarRacing-v3', entry_point='gymnasium.envs.box2d.car_racing:CarRacing', reward_threshold=900, nondeterministic=False, max_episode_steps=1000, order_enforce=True, disable_env_checker=False, kwargs={'continuous': False}, namespace=None, name='CarRacing', version=3, additional_wrappers=(), vector_entry_point=None)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False)\n",
    "print(env.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(obs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'DQN_baseline_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "video_dir = os.path.join(logs_path, \"videos\")\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple DQN architecture\n",
    "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
    "\n",
    "# Set up the model with simpler hyperparameters\n",
    "model = DQN('CnnPolicy', env, policy_kwargs=policy_kwargs, \n",
    "            verbose=1, buffer_size=50000, learning_starts=1000, \n",
    "            learning_rate=0.0005, batch_size=32, exploration_fraction=0.1,\n",
    "            exploration_final_eps=0.05)\n",
    "\n",
    "# Setup evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(env, best_model_save_path=model_dir,\n",
    "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
    "                                         name_prefix='dqn_model_checkpoint')\n",
    "\n",
    "\n",
    "\n",
    "# Start training the model with callbacks for evaluation and checkpoints\n",
    "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the final model after training\n",
    "model.save(\"dqn_car_racing_model\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "del env\n",
    "foo = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'PPO_baseline_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "video_dir = os.path.join(logs_path, \"videos\")\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple DQN architecture\n",
    "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
    "\n",
    "# Set up the model with simpler hyperparameters\n",
    "model = PPO('CnnPolicy', env, policy_kwargs=policy_kwargs, \n",
    "            verbose=1, learning_rate=0.0005, batch_size=32)\n",
    "\n",
    "# Setup evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(env, best_model_save_path=model_dir,\n",
    "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
    "                                         name_prefix='ppo_model_checkpoint')\n",
    "\n",
    "\n",
    "\n",
    "# Start training the model with callbacks for evaluation and checkpoints\n",
    "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the final model after training\n",
    "model.save(\"ppo_env_model\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "del env\n",
    "foo = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Environment and Reward Modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Modification**            | **Description**                                                                                      | **Effect**                                     |\n",
    "|-----------------------------|------------------------------------------------------------------------------------------------------|-----------------------------------------------|\n",
    "| **Obstacles**               | Randomly placed obstacles on the track.                                                             | Requires avoidance and navigation skills.     |\n",
    "| **Track Width Variability** | Random track width adjustments between `[0.8, 1.2]`.                                                | Simulates narrow/wide tracks dynamically.     |\n",
    "| **Weather Conditions**      | Introduces \"rain\" and \"snow,\" which alter action effectiveness.                                     | Adds randomness and realism to driving.       |\n",
    "| **Off-Track Penalty**       | Reward reduced by `-10` if the car leaves the track.                                                | Encourages the agent to stay on track.        |\n",
    "| **Distance Reward**         | Positive reward based on the distance traveled per step.                                            | Incentivizes efficient driving.               |\n",
    "| **Obstacle Proximity Penalty** | Penalty inversely proportional to the distance to nearby obstacles (`1 / (d + 1e-6)`).             | Encourages the car to avoid obstacles safely. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observation shape: (96, 96, 3)\n",
      "Initial info: {}\n",
      "\n",
      "Step 1:\n",
      "Action taken: 2\n",
      "Reward: 5.804336925215569\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 2:\n",
      "Action taken: 3\n",
      "Reward: -0.37469043040753186\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 3:\n",
      "Action taken: 0\n",
      "Reward: -0.4746904304075319\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 4:\n",
      "Action taken: 0\n",
      "Reward: -0.5746904304075319\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 5:\n",
      "Action taken: 1\n",
      "Reward: -0.6746904304075318\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 6:\n",
      "Action taken: 3\n",
      "Reward: -0.7746904304075318\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 7:\n",
      "Action taken: 1\n",
      "Reward: -0.8746904304075318\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 8:\n",
      "Action taken: 0\n",
      "Reward: -0.9746904304075318\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 9:\n",
      "Action taken: 4\n",
      "Reward: -1.0746904304075318\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 10:\n",
      "Action taken: 4\n",
      "Reward: -1.1746904304075318\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n"
     ]
    }
   ],
   "source": [
    "custom_env = EnhancedCarRacing(render_mode=\"rgb_array\")\n",
    "obs, info = custom_env.reset()\n",
    "\n",
    "print(\"Initial observation shape:\", obs.shape)\n",
    "print(\"Initial info:\", info)\n",
    "\n",
    "for i in range(10):\n",
    "    action = custom_env.action_space.sample()  # Your agent would make a decision here\n",
    "    observation, reward, terminated, truncated, info = custom_env.step(action)\n",
    "    print(f\"\\nStep {i+1}:\")\n",
    "    print(\"Action taken:\", action)\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"Terminated:\", terminated)\n",
    "    print(\"Truncated:\", truncated)\n",
    "    print(\"Info:\", info)\n",
    "\n",
    "    if hasattr(custom_env, 'weather_condition'):\n",
    "        print(\"Current weather:\", custom_env.weather_condition)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        print(\"Episode ended\")\n",
    "        break\n",
    "\n",
    "custom_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env = GrayscaleObservation(custom_env, keep_dim=True)\n",
    "custom_env = TimeLimit(custom_env, max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TimeLimit<GrayscaleObservation<EnhancedCarRacing instance>>>\n"
     ]
    }
   ],
   "source": [
    "print(custom_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\", custom_env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'DQN_env_mod_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1218 10:05:02.050906178 NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "/home/derpy/Documents/Fac/ano3/semestre1/ISIA/reinforcement-learning-with-gymnasium/.venv/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7eaf196b5eb0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7eaf159685c0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.962     |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 28        |\n",
      "|    time_elapsed     | 141       |\n",
      "|    total_timesteps  | 4000      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.12      |\n",
      "|    n_updates        | 749       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=-50215.65 +/- 9.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.953     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 5000      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.93      |\n",
      "|    n_updates        | 999       |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.924     |\n",
      "| time/               |           |\n",
      "|    episodes         | 8         |\n",
      "|    fps              | 17        |\n",
      "|    time_elapsed     | 463       |\n",
      "|    total_timesteps  | 8000      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.19      |\n",
      "|    n_updates        | 1749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-50167.65 +/- 36.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.905     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 10000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.34      |\n",
      "|    n_updates        | 2249      |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.886     |\n",
      "| time/               |           |\n",
      "|    episodes         | 12        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 750       |\n",
      "|    total_timesteps  | 12000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.74      |\n",
      "|    n_updates        | 2749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-50211.32 +/- 8.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.858     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 15000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.64      |\n",
      "|    n_updates        | 3499      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.848     |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 14        |\n",
      "|    time_elapsed     | 1127      |\n",
      "|    total_timesteps  | 16000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.41      |\n",
      "|    n_updates        | 3749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-50201.12 +/- 9.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.81      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 20000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 1.52      |\n",
      "|    n_updates        | 4749      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.81      |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 1526      |\n",
      "|    total_timesteps  | 20000     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.772     |\n",
      "| time/               |           |\n",
      "|    episodes         | 24        |\n",
      "|    fps              | 14        |\n",
      "|    time_elapsed     | 1682      |\n",
      "|    total_timesteps  | 24000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.47      |\n",
      "|    n_updates        | 5749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-50205.71 +/- 9.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.763     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 25000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.99      |\n",
      "|    n_updates        | 5999      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.734     |\n",
      "| time/               |           |\n",
      "|    episodes         | 28        |\n",
      "|    fps              | 14        |\n",
      "|    time_elapsed     | 1909      |\n",
      "|    total_timesteps  | 28000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.69      |\n",
      "|    n_updates        | 6749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-33812.91 +/- 20146.91\n",
      "Episode length: 768.60 +/- 286.51\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 769       |\n",
      "|    mean_reward      | -3.38e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.715     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 30000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.51      |\n",
      "|    n_updates        | 7249      |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.696     |\n",
      "| time/               |           |\n",
      "|    episodes         | 32        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 2111      |\n",
      "|    total_timesteps  | 32000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.8       |\n",
      "|    n_updates        | 7749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-50206.97 +/- 7.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.668     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 35000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.63      |\n",
      "|    n_updates        | 8499      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.658     |\n",
      "| time/               |           |\n",
      "|    episodes         | 36        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 2339      |\n",
      "|    total_timesteps  | 36000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.92      |\n",
      "|    n_updates        | 8749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-50201.85 +/- 12.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.62      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 40000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.66      |\n",
      "|    n_updates        | 9749      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.62      |\n",
      "| time/               |           |\n",
      "|    episodes         | 40        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 2567      |\n",
      "|    total_timesteps  | 40000     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.582     |\n",
      "| time/               |           |\n",
      "|    episodes         | 44        |\n",
      "|    fps              | 16        |\n",
      "|    time_elapsed     | 2696      |\n",
      "|    total_timesteps  | 44000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.39      |\n",
      "|    n_updates        | 10749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-50172.30 +/- 43.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.573     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 45000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.68      |\n",
      "|    n_updates        | 10999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.544     |\n",
      "| time/               |           |\n",
      "|    episodes         | 48        |\n",
      "|    fps              | 16        |\n",
      "|    time_elapsed     | 2971      |\n",
      "|    total_timesteps  | 48000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4         |\n",
      "|    n_updates        | 11749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-50216.08 +/- 13.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.525     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 50000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.76      |\n",
      "|    n_updates        | 12249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.506     |\n",
      "| time/               |           |\n",
      "|    episodes         | 52        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 3257      |\n",
      "|    total_timesteps  | 52000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 6         |\n",
      "|    n_updates        | 12749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-50112.29 +/- 20.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.01e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.478     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 55000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.88      |\n",
      "|    n_updates        | 13499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.468     |\n",
      "| time/               |           |\n",
      "|    episodes         | 56        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 3561      |\n",
      "|    total_timesteps  | 56000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.84      |\n",
      "|    n_updates        | 13749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-50207.37 +/- 12.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.43      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 60000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.37      |\n",
      "|    n_updates        | 14749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.43      |\n",
      "| time/               |           |\n",
      "|    episodes         | 60        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 3874      |\n",
      "|    total_timesteps  | 60000     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.392     |\n",
      "| time/               |           |\n",
      "|    episodes         | 64        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 4087      |\n",
      "|    total_timesteps  | 64000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 7.42      |\n",
      "|    n_updates        | 15749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-50208.35 +/- 11.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.383     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 65000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.9       |\n",
      "|    n_updates        | 15999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.354     |\n",
      "| time/               |           |\n",
      "|    episodes         | 68        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 4352      |\n",
      "|    total_timesteps  | 68000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.87      |\n",
      "|    n_updates        | 16749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-50192.87 +/- 21.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.335     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 70000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.99      |\n",
      "|    n_updates        | 17249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.316     |\n",
      "| time/               |           |\n",
      "|    episodes         | 72        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 4582      |\n",
      "|    total_timesteps  | 72000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.44      |\n",
      "|    n_updates        | 17749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-50206.63 +/- 16.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.288     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 75000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.2       |\n",
      "|    n_updates        | 18499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.278     |\n",
      "| time/               |           |\n",
      "|    episodes         | 76        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 4837      |\n",
      "|    total_timesteps  | 76000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.99      |\n",
      "|    n_updates        | 18749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-50205.96 +/- 11.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.24      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 80000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.08      |\n",
      "|    n_updates        | 19749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.24      |\n",
      "| time/               |           |\n",
      "|    episodes         | 80        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 5146      |\n",
      "|    total_timesteps  | 80000     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.202     |\n",
      "| time/               |           |\n",
      "|    episodes         | 84        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 5345      |\n",
      "|    total_timesteps  | 84000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.51      |\n",
      "|    n_updates        | 20749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-50171.45 +/- 25.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.193     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 85000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.73      |\n",
      "|    n_updates        | 20999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.164     |\n",
      "| time/               |           |\n",
      "|    episodes         | 88        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 5611      |\n",
      "|    total_timesteps  | 88000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.12      |\n",
      "|    n_updates        | 21749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-50212.26 +/- 4.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.145     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 90000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.17      |\n",
      "|    n_updates        | 22249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.126     |\n",
      "| time/               |           |\n",
      "|    episodes         | 92        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 5915      |\n",
      "|    total_timesteps  | 92000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.2      |\n",
      "|    n_updates        | 22749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-50202.68 +/- 18.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.0975    |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 95000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.79      |\n",
      "|    n_updates        | 23499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.088     |\n",
      "| time/               |           |\n",
      "|    episodes         | 96        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 6232      |\n",
      "|    total_timesteps  | 96000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.39      |\n",
      "|    n_updates        | 23749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-8896.11 +/- 3242.86\n",
      "Episode length: 413.00 +/- 71.52\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 413      |\n",
      "|    mean_reward      | -8.9e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 6.79     |\n",
      "|    n_updates        | 24749    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 100       |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 6458      |\n",
      "|    total_timesteps  | 100000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 104       |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 6697      |\n",
      "|    total_timesteps  | 104000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.64      |\n",
      "|    n_updates        | 25749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-50208.89 +/- 7.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 105000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.3      |\n",
      "|    n_updates        | 25999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 108       |\n",
      "|    fps              | 14        |\n",
      "|    time_elapsed     | 7687      |\n",
      "|    total_timesteps  | 108000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.63      |\n",
      "|    n_updates        | 26749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-50201.73 +/- 18.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 110000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.6      |\n",
      "|    n_updates        | 27249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 8188     |\n",
      "|    total_timesteps  | 112000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 10       |\n",
      "|    n_updates        | 27749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-50202.84 +/- 10.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 115000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.98      |\n",
      "|    n_updates        | 28499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 8754     |\n",
      "|    total_timesteps  | 116000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 10.6     |\n",
      "|    n_updates        | 28749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-50211.57 +/- 13.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 120000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 6.73      |\n",
      "|    n_updates        | 29749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 9395     |\n",
      "|    total_timesteps  | 120000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 9793     |\n",
      "|    total_timesteps  | 124000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 11.5     |\n",
      "|    n_updates        | 30749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-50191.04 +/- 17.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 125000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.8      |\n",
      "|    n_updates        | 30999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 10314    |\n",
      "|    total_timesteps  | 128000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 5.79     |\n",
      "|    n_updates        | 31749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-50197.73 +/- 11.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 130000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.03      |\n",
      "|    n_updates        | 32249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 10866    |\n",
      "|    total_timesteps  | 131959   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 9.18     |\n",
      "|    n_updates        | 32739    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=-13544.02 +/- 2978.08\n",
      "Episode length: 514.40 +/- 61.34\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 514       |\n",
      "|    mean_reward      | -1.35e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 135000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.11      |\n",
      "|    n_updates        | 33499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 11222    |\n",
      "|    total_timesteps  | 136000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 8.33     |\n",
      "|    n_updates        | 33749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-50120.15 +/- 43.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.01e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 140000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9         |\n",
      "|    n_updates        | 34749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 11781    |\n",
      "|    total_timesteps  | 140000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 12117    |\n",
      "|    total_timesteps  | 144000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 4.79     |\n",
      "|    n_updates        | 35749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=-50199.28 +/- 11.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 145000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.4      |\n",
      "|    n_updates        | 35999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 12571    |\n",
      "|    total_timesteps  | 148000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 9.67     |\n",
      "|    n_updates        | 36749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-50188.72 +/- 9.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 150000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.68      |\n",
      "|    n_updates        | 37249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 12851    |\n",
      "|    total_timesteps  | 152000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.8     |\n",
      "|    n_updates        | 37749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=-50208.45 +/- 15.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 155000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 6.76      |\n",
      "|    n_updates        | 38499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 13089    |\n",
      "|    total_timesteps  | 156000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 6.42     |\n",
      "|    n_updates        | 38749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-50194.16 +/- 5.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 160000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.2      |\n",
      "|    n_updates        | 39749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 13326    |\n",
      "|    total_timesteps  | 160000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 13461    |\n",
      "|    total_timesteps  | 164000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.3     |\n",
      "|    n_updates        | 40749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=-50206.46 +/- 11.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 165000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.6      |\n",
      "|    n_updates        | 40999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 13755    |\n",
      "|    total_timesteps  | 168000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 6.07     |\n",
      "|    n_updates        | 41749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-50197.88 +/- 13.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 170000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.97      |\n",
      "|    n_updates        | 42249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 14065    |\n",
      "|    total_timesteps  | 172000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 11.3     |\n",
      "|    n_updates        | 42749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=-35280.44 +/- 18393.08\n",
      "Episode length: 800.20 +/- 247.88\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 800       |\n",
      "|    mean_reward      | -3.53e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 175000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.32      |\n",
      "|    n_updates        | 43499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 14309    |\n",
      "|    total_timesteps  | 176000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 7.75     |\n",
      "|    n_updates        | 43749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-5775.34 +/- 1716.65\n",
      "Episode length: 332.20 +/- 52.93\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 332       |\n",
      "|    mean_reward      | -5.78e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 180000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13        |\n",
      "|    n_updates        | 44749     |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 14518    |\n",
      "|    total_timesteps  | 180000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 14713    |\n",
      "|    total_timesteps  | 184000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.1     |\n",
      "|    n_updates        | 45749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-50205.64 +/- 15.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 185000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15        |\n",
      "|    n_updates        | 45999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 14969    |\n",
      "|    total_timesteps  | 188000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 7.79     |\n",
      "|    n_updates        | 46749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-50208.19 +/- 10.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 190000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 7.75      |\n",
      "|    n_updates        | 47249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 15277    |\n",
      "|    total_timesteps  | 192000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 9.87     |\n",
      "|    n_updates        | 47749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-12453.23 +/- 4103.91\n",
      "Episode length: 488.80 +/- 86.46\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 489       |\n",
      "|    mean_reward      | -1.25e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 195000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.31      |\n",
      "|    n_updates        | 48499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 15532    |\n",
      "|    total_timesteps  | 196000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 8.6      |\n",
      "|    n_updates        | 48749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-50193.29 +/- 9.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 200000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.5      |\n",
      "|    n_updates        | 49749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 15801    |\n",
      "|    total_timesteps  | 200000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 16023    |\n",
      "|    total_timesteps  | 204000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 10.4     |\n",
      "|    n_updates        | 50749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=-50207.50 +/- 6.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 205000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11        |\n",
      "|    n_updates        | 50999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 16343    |\n",
      "|    total_timesteps  | 208000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 9.61     |\n",
      "|    n_updates        | 51749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-50207.03 +/- 12.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 210000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.1      |\n",
      "|    n_updates        | 52249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 212       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 16594     |\n",
      "|    total_timesteps  | 212000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.8      |\n",
      "|    n_updates        | 52749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=-50199.78 +/- 8.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 215000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.1      |\n",
      "|    n_updates        | 53499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 216       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 16949     |\n",
      "|    total_timesteps  | 216000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 6.23      |\n",
      "|    n_updates        | 53749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-50209.80 +/- 12.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 220000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 7.95      |\n",
      "|    n_updates        | 54749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 220       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 17235     |\n",
      "|    total_timesteps  | 220000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 224       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 17476     |\n",
      "|    total_timesteps  | 224000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.7      |\n",
      "|    n_updates        | 55749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=-50203.18 +/- 9.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 225000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.8      |\n",
      "|    n_updates        | 55999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 228       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 17747     |\n",
      "|    total_timesteps  | 228000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.3      |\n",
      "|    n_updates        | 56749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-50210.61 +/- 10.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 230000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 7.12      |\n",
      "|    n_updates        | 57249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 232       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 18058     |\n",
      "|    total_timesteps  | 232000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.8      |\n",
      "|    n_updates        | 57749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=-50197.35 +/- 13.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 235000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 7.38      |\n",
      "|    n_updates        | 58499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 236       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 18404     |\n",
      "|    total_timesteps  | 236000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.2      |\n",
      "|    n_updates        | 58749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 999       |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 240       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 18631     |\n",
      "|    total_timesteps  | 239917    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.5      |\n",
      "|    n_updates        | 59729     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-50201.37 +/- 16.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 240000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.4      |\n",
      "|    n_updates        | 59749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 244       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 18947     |\n",
      "|    total_timesteps  | 244000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.88      |\n",
      "|    n_updates        | 60749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=-50140.61 +/- 14.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.01e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 245000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.4      |\n",
      "|    n_updates        | 60999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 248       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 19185     |\n",
      "|    total_timesteps  | 248000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.4      |\n",
      "|    n_updates        | 61749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=-50199.17 +/- 10.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 250000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.7      |\n",
      "|    n_updates        | 62249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 252       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 19424     |\n",
      "|    total_timesteps  | 252000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.7      |\n",
      "|    n_updates        | 62749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=-50196.72 +/- 12.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 255000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.1      |\n",
      "|    n_updates        | 63499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 256       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 19708     |\n",
      "|    total_timesteps  | 256000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.8      |\n",
      "|    n_updates        | 63749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-50212.10 +/- 19.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 260000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.7      |\n",
      "|    n_updates        | 64749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 260       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 20027     |\n",
      "|    total_timesteps  | 260000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 264       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 20238     |\n",
      "|    total_timesteps  | 264000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.25      |\n",
      "|    n_updates        | 65749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=-50192.29 +/- 23.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 265000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.3      |\n",
      "|    n_updates        | 65999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 268       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 20503     |\n",
      "|    total_timesteps  | 268000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.2      |\n",
      "|    n_updates        | 66749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=-50206.48 +/- 7.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 270000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.2      |\n",
      "|    n_updates        | 67249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 272       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 20809     |\n",
      "|    total_timesteps  | 272000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.3      |\n",
      "|    n_updates        | 67749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=-50203.23 +/- 14.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 275000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9         |\n",
      "|    n_updates        | 68499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 276       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 21096     |\n",
      "|    total_timesteps  | 276000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.21      |\n",
      "|    n_updates        | 68749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-50217.40 +/- 7.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 280000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 26        |\n",
      "|    n_updates        | 69749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 280       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 21458     |\n",
      "|    total_timesteps  | 280000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 284       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 21695     |\n",
      "|    total_timesteps  | 284000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.9      |\n",
      "|    n_updates        | 70749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=-50188.95 +/- 21.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 285000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.2      |\n",
      "|    n_updates        | 70999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 288       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 22075     |\n",
      "|    total_timesteps  | 288000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10        |\n",
      "|    n_updates        | 71749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-50205.28 +/- 8.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 290000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.7      |\n",
      "|    n_updates        | 72249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 292       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 22438     |\n",
      "|    total_timesteps  | 292000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.6      |\n",
      "|    n_updates        | 72749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=-50205.35 +/- 12.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 295000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.2      |\n",
      "|    n_updates        | 73499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 296       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 22801     |\n",
      "|    total_timesteps  | 296000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.8      |\n",
      "|    n_updates        | 73749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-50201.09 +/- 6.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 300000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.8      |\n",
      "|    n_updates        | 74749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 300       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 23161     |\n",
      "|    total_timesteps  | 300000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 304       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 23397     |\n",
      "|    total_timesteps  | 304000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.1      |\n",
      "|    n_updates        | 75749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=305000, episode_reward=-50206.24 +/- 8.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 305000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.3      |\n",
      "|    n_updates        | 75999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 308       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 23758     |\n",
      "|    total_timesteps  | 308000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.4      |\n",
      "|    n_updates        | 76749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=-50206.38 +/- 10.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 310000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.4      |\n",
      "|    n_updates        | 77249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 312       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 24120     |\n",
      "|    total_timesteps  | 312000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.26      |\n",
      "|    n_updates        | 77749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=315000, episode_reward=-50198.97 +/- 6.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 315000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.7      |\n",
      "|    n_updates        | 78499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 316       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 24485     |\n",
      "|    total_timesteps  | 316000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.8      |\n",
      "|    n_updates        | 78749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-50204.55 +/- 9.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 320000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.34      |\n",
      "|    n_updates        | 79749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 320       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 24848     |\n",
      "|    total_timesteps  | 320000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 324       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 25085     |\n",
      "|    total_timesteps  | 324000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.6      |\n",
      "|    n_updates        | 80749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=325000, episode_reward=-50201.25 +/- 12.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 325000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16        |\n",
      "|    n_updates        | 80999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 997       |\n",
      "|    ep_rew_mean      | -4.98e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 328       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 25433     |\n",
      "|    total_timesteps  | 327697    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.2      |\n",
      "|    n_updates        | 81674     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=-50213.95 +/- 4.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 330000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.6      |\n",
      "|    n_updates        | 82249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -4.99e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 332       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 25814     |\n",
      "|    total_timesteps  | 332000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.4      |\n",
      "|    n_updates        | 82749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=335000, episode_reward=-50207.87 +/- 8.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 335000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.8      |\n",
      "|    n_updates        | 83499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -4.99e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 336       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 26180     |\n",
      "|    total_timesteps  | 336000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.9      |\n",
      "|    n_updates        | 83749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-50192.00 +/- 22.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 340000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.8      |\n",
      "|    n_updates        | 84749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 26546    |\n",
      "|    total_timesteps  | 340000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 26785    |\n",
      "|    total_timesteps  | 344000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13       |\n",
      "|    n_updates        | 85749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=345000, episode_reward=-50208.73 +/- 11.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 345000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 27        |\n",
      "|    n_updates        | 85999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 27153    |\n",
      "|    total_timesteps  | 348000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.6     |\n",
      "|    n_updates        | 86749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=-50210.24 +/- 6.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 350000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.8      |\n",
      "|    n_updates        | 87249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 27516    |\n",
      "|    total_timesteps  | 352000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 14.5     |\n",
      "|    n_updates        | 87749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=355000, episode_reward=-50197.29 +/- 17.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 355000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.7      |\n",
      "|    n_updates        | 88499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 27880    |\n",
      "|    total_timesteps  | 356000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 16       |\n",
      "|    n_updates        | 88749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-50214.35 +/- 11.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 360000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 24.8      |\n",
      "|    n_updates        | 89749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 28245    |\n",
      "|    total_timesteps  | 360000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 28483    |\n",
      "|    total_timesteps  | 364000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.6     |\n",
      "|    n_updates        | 90749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=365000, episode_reward=-41878.43 +/- 4469.47\n",
      "Episode length: 911.80 +/- 48.47\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 912       |\n",
      "|    mean_reward      | -4.19e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 365000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.4      |\n",
      "|    n_updates        | 90999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 28834    |\n",
      "|    total_timesteps  | 368000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 11.6     |\n",
      "|    n_updates        | 91749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=-50216.02 +/- 11.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 370000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.4      |\n",
      "|    n_updates        | 92249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 29199    |\n",
      "|    total_timesteps  | 372000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 21.4     |\n",
      "|    n_updates        | 92749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=375000, episode_reward=-50195.37 +/- 11.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 375000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.1      |\n",
      "|    n_updates        | 93499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 29565    |\n",
      "|    total_timesteps  | 376000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 28.1     |\n",
      "|    n_updates        | 93749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-36103.28 +/- 6437.66\n",
      "Episode length: 844.00 +/- 73.12\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 844       |\n",
      "|    mean_reward      | -3.61e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 380000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.2      |\n",
      "|    n_updates        | 94749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 29909    |\n",
      "|    total_timesteps  | 380000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 30145    |\n",
      "|    total_timesteps  | 384000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 14.6     |\n",
      "|    n_updates        | 95749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=385000, episode_reward=-50204.37 +/- 15.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 385000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.7      |\n",
      "|    n_updates        | 95999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 30509    |\n",
      "|    total_timesteps  | 388000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 12.2     |\n",
      "|    n_updates        | 96749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=-50209.47 +/- 12.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 390000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.9      |\n",
      "|    n_updates        | 97249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 30875    |\n",
      "|    total_timesteps  | 392000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 11.5     |\n",
      "|    n_updates        | 97749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=395000, episode_reward=-50208.11 +/- 7.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 395000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.4      |\n",
      "|    n_updates        | 98499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 31241    |\n",
      "|    total_timesteps  | 396000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 12.3     |\n",
      "|    n_updates        | 98749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-50201.55 +/- 8.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 400000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.7      |\n",
      "|    n_updates        | 99749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 31608    |\n",
      "|    total_timesteps  | 400000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 31849    |\n",
      "|    total_timesteps  | 404000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 11.2     |\n",
      "|    n_updates        | 100749   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=405000, episode_reward=-50211.67 +/- 3.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 405000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.2      |\n",
      "|    n_updates        | 100999    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 32223    |\n",
      "|    total_timesteps  | 408000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.2     |\n",
      "|    n_updates        | 101749   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=-50202.65 +/- 19.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 410000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.5      |\n",
      "|    n_updates        | 102249    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 32595    |\n",
      "|    total_timesteps  | 412000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 14.2     |\n",
      "|    n_updates        | 102749   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=415000, episode_reward=-50214.33 +/- 9.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 415000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.6      |\n",
      "|    n_updates        | 103499    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 32979    |\n",
      "|    total_timesteps  | 416000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 8.73     |\n",
      "|    n_updates        | 103749   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-50147.66 +/- 31.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.01e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 420000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.7      |\n",
      "|    n_updates        | 104749    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 33351    |\n",
      "|    total_timesteps  | 420000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 33595    |\n",
      "|    total_timesteps  | 424000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 16.8     |\n",
      "|    n_updates        | 105749   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=425000, episode_reward=-50205.00 +/- 13.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 425000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.23      |\n",
      "|    n_updates        | 105999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 428       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 33963     |\n",
      "|    total_timesteps  | 428000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.5      |\n",
      "|    n_updates        | 106749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=-50181.91 +/- 14.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 430000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.1      |\n",
      "|    n_updates        | 107249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 432       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 34334     |\n",
      "|    total_timesteps  | 432000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.9      |\n",
      "|    n_updates        | 107749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=435000, episode_reward=-50208.04 +/- 13.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 435000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.5      |\n",
      "|    n_updates        | 108499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 436       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 34706     |\n",
      "|    total_timesteps  | 436000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.3      |\n",
      "|    n_updates        | 108749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-10668.34 +/- 7193.35\n",
      "Episode length: 439.00 +/- 136.50\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 439       |\n",
      "|    mean_reward      | -1.07e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 440000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.8      |\n",
      "|    n_updates        | 109749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 440       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 35003     |\n",
      "|    total_timesteps  | 440000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 444       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 35246     |\n",
      "|    total_timesteps  | 444000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.51      |\n",
      "|    n_updates        | 110749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=445000, episode_reward=-50214.01 +/- 7.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 445000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.4      |\n",
      "|    n_updates        | 110999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 448       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 35616     |\n",
      "|    total_timesteps  | 448000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.3      |\n",
      "|    n_updates        | 111749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=-50187.17 +/- 20.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 450000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19        |\n",
      "|    n_updates        | 112249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 452       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 35986     |\n",
      "|    total_timesteps  | 452000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.6      |\n",
      "|    n_updates        | 112749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=455000, episode_reward=-29709.04 +/- 21149.71\n",
      "Episode length: 692.60 +/- 334.04\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 693       |\n",
      "|    mean_reward      | -2.97e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 455000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.8      |\n",
      "|    n_updates        | 113499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 456       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 36318     |\n",
      "|    total_timesteps  | 456000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.5      |\n",
      "|    n_updates        | 113749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-50212.48 +/- 7.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 460000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.8      |\n",
      "|    n_updates        | 114749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 460       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 36687     |\n",
      "|    total_timesteps  | 460000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 464       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 36929     |\n",
      "|    total_timesteps  | 464000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.8      |\n",
      "|    n_updates        | 115749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=465000, episode_reward=-50200.33 +/- 11.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 465000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.6      |\n",
      "|    n_updates        | 115999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 468       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 37301     |\n",
      "|    total_timesteps  | 468000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.1      |\n",
      "|    n_updates        | 116749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=-50210.62 +/- 12.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 470000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.7      |\n",
      "|    n_updates        | 117249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 472       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 37676     |\n",
      "|    total_timesteps  | 472000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.5      |\n",
      "|    n_updates        | 117749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=475000, episode_reward=-50171.23 +/- 25.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 475000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 29.3      |\n",
      "|    n_updates        | 118499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 476       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 38048     |\n",
      "|    total_timesteps  | 476000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.9      |\n",
      "|    n_updates        | 118749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-50188.59 +/- 30.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 480000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.4      |\n",
      "|    n_updates        | 119749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 480       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 38418     |\n",
      "|    total_timesteps  | 480000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 484       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 38661     |\n",
      "|    total_timesteps  | 484000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.6      |\n",
      "|    n_updates        | 120749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=485000, episode_reward=-50210.66 +/- 15.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 485000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.9      |\n",
      "|    n_updates        | 120999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 488       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 39035     |\n",
      "|    total_timesteps  | 488000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14        |\n",
      "|    n_updates        | 121749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=-50198.43 +/- 4.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 490000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.2      |\n",
      "|    n_updates        | 122249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 492       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 39409     |\n",
      "|    total_timesteps  | 492000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.4      |\n",
      "|    n_updates        | 122749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=495000, episode_reward=-50212.10 +/- 11.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 495000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.4      |\n",
      "|    n_updates        | 123499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 496       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 39779     |\n",
      "|    total_timesteps  | 496000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.4      |\n",
      "|    n_updates        | 123749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-50209.52 +/- 11.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 500000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.8      |\n",
      "|    n_updates        | 124749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 500       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 40168     |\n",
      "|    total_timesteps  | 500000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 504       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 40417     |\n",
      "|    total_timesteps  | 504000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.9      |\n",
      "|    n_updates        | 125749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=505000, episode_reward=-50199.54 +/- 15.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 505000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.1      |\n",
      "|    n_updates        | 125999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 508       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 40798     |\n",
      "|    total_timesteps  | 508000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.3      |\n",
      "|    n_updates        | 126749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=-50215.74 +/- 10.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 510000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19        |\n",
      "|    n_updates        | 127249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 512       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 41172     |\n",
      "|    total_timesteps  | 512000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.9      |\n",
      "|    n_updates        | 127749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=515000, episode_reward=-50194.78 +/- 13.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 515000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.5      |\n",
      "|    n_updates        | 128499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 516       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 41547     |\n",
      "|    total_timesteps  | 516000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.6      |\n",
      "|    n_updates        | 128749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-50204.87 +/- 15.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 520000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.9      |\n",
      "|    n_updates        | 129749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 520       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 41927     |\n",
      "|    total_timesteps  | 520000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 524       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 42174     |\n",
      "|    total_timesteps  | 524000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15        |\n",
      "|    n_updates        | 130749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=525000, episode_reward=-50212.83 +/- 14.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 525000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.2      |\n",
      "|    n_updates        | 130999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 528       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 42547     |\n",
      "|    total_timesteps  | 528000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.4      |\n",
      "|    n_updates        | 131749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=-50217.56 +/- 8.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 530000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.4      |\n",
      "|    n_updates        | 132249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 532       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 42926     |\n",
      "|    total_timesteps  | 532000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.4      |\n",
      "|    n_updates        | 132749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=535000, episode_reward=-50216.35 +/- 5.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 535000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.7      |\n",
      "|    n_updates        | 133499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 536       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 43302     |\n",
      "|    total_timesteps  | 536000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.6      |\n",
      "|    n_updates        | 133749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-50165.65 +/- 26.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 540000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.3      |\n",
      "|    n_updates        | 134749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 540       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 43680     |\n",
      "|    total_timesteps  | 540000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 544       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 43929     |\n",
      "|    total_timesteps  | 544000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.6      |\n",
      "|    n_updates        | 135749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=545000, episode_reward=-50213.11 +/- 15.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 545000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.5      |\n",
      "|    n_updates        | 135999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 548       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 44306     |\n",
      "|    total_timesteps  | 548000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.5      |\n",
      "|    n_updates        | 136749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=-50215.36 +/- 12.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 550000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.8      |\n",
      "|    n_updates        | 137249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 552       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 44683     |\n",
      "|    total_timesteps  | 552000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.8      |\n",
      "|    n_updates        | 137749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=555000, episode_reward=-50208.38 +/- 10.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 555000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.1      |\n",
      "|    n_updates        | 138499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 556       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 45061     |\n",
      "|    total_timesteps  | 556000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14        |\n",
      "|    n_updates        | 138749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-47902.41 +/- 3268.16\n",
      "Episode length: 976.40 +/- 34.58\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 976       |\n",
      "|    mean_reward      | -4.79e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 560000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.4      |\n",
      "|    n_updates        | 139749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 560       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 45434     |\n",
      "|    total_timesteps  | 560000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 564       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 45684     |\n",
      "|    total_timesteps  | 564000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.7      |\n",
      "|    n_updates        | 140749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=565000, episode_reward=-50194.98 +/- 5.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 565000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.7      |\n",
      "|    n_updates        | 140999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 568       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 46064     |\n",
      "|    total_timesteps  | 568000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.2      |\n",
      "|    n_updates        | 141749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=-50202.83 +/- 9.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 570000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.6      |\n",
      "|    n_updates        | 142249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 572       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 46448     |\n",
      "|    total_timesteps  | 572000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.8      |\n",
      "|    n_updates        | 142749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=575000, episode_reward=-50191.37 +/- 11.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 575000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.4      |\n",
      "|    n_updates        | 143499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 576       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 46825     |\n",
      "|    total_timesteps  | 576000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.9      |\n",
      "|    n_updates        | 143749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-50210.62 +/- 7.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 580000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.8      |\n",
      "|    n_updates        | 144749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 580       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 47203     |\n",
      "|    total_timesteps  | 580000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 584       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 47453     |\n",
      "|    total_timesteps  | 584000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 25.8      |\n",
      "|    n_updates        | 145749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=585000, episode_reward=-50217.94 +/- 10.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 585000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.1      |\n",
      "|    n_updates        | 145999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 588       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 47836     |\n",
      "|    total_timesteps  | 588000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.18      |\n",
      "|    n_updates        | 146749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=-50205.60 +/- 19.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 590000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.5      |\n",
      "|    n_updates        | 147249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 592       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 48214     |\n",
      "|    total_timesteps  | 592000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 25.9      |\n",
      "|    n_updates        | 147749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=595000, episode_reward=-50209.31 +/- 8.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 595000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.7      |\n",
      "|    n_updates        | 148499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 596       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 48595     |\n",
      "|    total_timesteps  | 596000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.2      |\n",
      "|    n_updates        | 148749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-50202.02 +/- 12.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 600000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.2      |\n",
      "|    n_updates        | 149749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 600       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 48983     |\n",
      "|    total_timesteps  | 600000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 604       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 49234     |\n",
      "|    total_timesteps  | 604000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.8      |\n",
      "|    n_updates        | 150749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=605000, episode_reward=-50208.29 +/- 13.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 605000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.9      |\n",
      "|    n_updates        | 150999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 608       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 49614     |\n",
      "|    total_timesteps  | 608000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16        |\n",
      "|    n_updates        | 151749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=-50205.95 +/- 19.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 610000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.2      |\n",
      "|    n_updates        | 152249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 999       |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 612       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 49992     |\n",
      "|    total_timesteps  | 611941    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.5      |\n",
      "|    n_updates        | 152735    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=615000, episode_reward=-50201.18 +/- 18.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 615000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.7      |\n",
      "|    n_updates        | 153499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 616       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 50377     |\n",
      "|    total_timesteps  | 616000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.9      |\n",
      "|    n_updates        | 153749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-13082.57 +/- 5533.32\n",
      "Episode length: 494.20 +/- 123.18\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 494       |\n",
      "|    mean_reward      | -1.31e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 620000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.1      |\n",
      "|    n_updates        | 154749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 620       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 50693     |\n",
      "|    total_timesteps  | 620000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 624       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 50944     |\n",
      "|    total_timesteps  | 624000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.3      |\n",
      "|    n_updates        | 155749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=625000, episode_reward=-50195.24 +/- 16.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 625000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 25.6      |\n",
      "|    n_updates        | 155999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 628       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 51328     |\n",
      "|    total_timesteps  | 628000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.8      |\n",
      "|    n_updates        | 156749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=-50199.18 +/- 13.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 630000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.9      |\n",
      "|    n_updates        | 157249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 632       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 51714     |\n",
      "|    total_timesteps  | 632000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19        |\n",
      "|    n_updates        | 157749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=635000, episode_reward=-50213.33 +/- 15.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 635000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.5      |\n",
      "|    n_updates        | 158499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 636       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 52098     |\n",
      "|    total_timesteps  | 636000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.9      |\n",
      "|    n_updates        | 158749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=-50206.66 +/- 7.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 640000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 24.8      |\n",
      "|    n_updates        | 159749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 640       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 52457     |\n",
      "|    total_timesteps  | 640000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 644       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 52693     |\n",
      "|    total_timesteps  | 644000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.1      |\n",
      "|    n_updates        | 160749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=645000, episode_reward=-50193.48 +/- 15.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 645000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 23.1      |\n",
      "|    n_updates        | 160999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 648       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 53017     |\n",
      "|    total_timesteps  | 648000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 24.5      |\n",
      "|    n_updates        | 161749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=-50216.17 +/- 10.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 650000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.2      |\n",
      "|    n_updates        | 162249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 652       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 53305     |\n",
      "|    total_timesteps  | 652000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 26.3      |\n",
      "|    n_updates        | 162749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=655000, episode_reward=-50208.79 +/- 19.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 655000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.5      |\n",
      "|    n_updates        | 163499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 656       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 53593     |\n",
      "|    total_timesteps  | 656000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22        |\n",
      "|    n_updates        | 163749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-50203.31 +/- 12.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 660000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.7      |\n",
      "|    n_updates        | 164749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 660       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 53843     |\n",
      "|    total_timesteps  | 660000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 664       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 54100     |\n",
      "|    total_timesteps  | 664000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.1      |\n",
      "|    n_updates        | 165749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=665000, episode_reward=-50216.05 +/- 12.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 665000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.4      |\n",
      "|    n_updates        | 165999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 668       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 54510     |\n",
      "|    total_timesteps  | 668000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.9      |\n",
      "|    n_updates        | 166749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=-50196.67 +/- 6.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 670000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.9      |\n",
      "|    n_updates        | 167249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 672       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 55077     |\n",
      "|    total_timesteps  | 672000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.2      |\n",
      "|    n_updates        | 167749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=675000, episode_reward=-50202.87 +/- 16.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 675000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.9      |\n",
      "|    n_updates        | 168499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 676       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 55545     |\n",
      "|    total_timesteps  | 676000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.2      |\n",
      "|    n_updates        | 168749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-50219.50 +/- 8.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 680000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.1      |\n",
      "|    n_updates        | 169749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 680       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 55896     |\n",
      "|    total_timesteps  | 680000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 684       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 56113     |\n",
      "|    total_timesteps  | 684000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 43.2      |\n",
      "|    n_updates        | 170749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=685000, episode_reward=-50208.22 +/- 15.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 685000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 34.6      |\n",
      "|    n_updates        | 170999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 688       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 56446     |\n",
      "|    total_timesteps  | 688000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.3      |\n",
      "|    n_updates        | 171749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=-50198.93 +/- 17.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 690000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.6      |\n",
      "|    n_updates        | 172249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 692       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 56782     |\n",
      "|    total_timesteps  | 692000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11        |\n",
      "|    n_updates        | 172749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=695000, episode_reward=-50208.12 +/- 9.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 695000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.5      |\n",
      "|    n_updates        | 173499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 696       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 57140     |\n",
      "|    total_timesteps  | 696000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.5      |\n",
      "|    n_updates        | 173749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=-30409.64 +/- 1976.13\n",
      "Episode length: 777.00 +/- 25.44\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 777       |\n",
      "|    mean_reward      | -3.04e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 700000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.3      |\n",
      "|    n_updates        | 174749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 700       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 57523     |\n",
      "|    total_timesteps  | 700000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 704       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 57802     |\n",
      "|    total_timesteps  | 704000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14        |\n",
      "|    n_updates        | 175749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=705000, episode_reward=-50212.34 +/- 7.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 705000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.9      |\n",
      "|    n_updates        | 175999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 708       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 58248     |\n",
      "|    total_timesteps  | 708000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.8      |\n",
      "|    n_updates        | 176749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=-50205.71 +/- 6.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 710000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 27.9      |\n",
      "|    n_updates        | 177249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 712       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 58654     |\n",
      "|    total_timesteps  | 712000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.1      |\n",
      "|    n_updates        | 177749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=715000, episode_reward=-50202.98 +/- 11.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 715000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 26.4      |\n",
      "|    n_updates        | 178499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 716       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 59012     |\n",
      "|    total_timesteps  | 716000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 34        |\n",
      "|    n_updates        | 178749    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 999      |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 59243    |\n",
      "|    total_timesteps  | 719871   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 22.9     |\n",
      "|    n_updates        | 179717   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-50212.21 +/- 12.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 720000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.6      |\n",
      "|    n_updates        | 179749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 724       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 59617     |\n",
      "|    total_timesteps  | 724000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.4      |\n",
      "|    n_updates        | 180749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=725000, episode_reward=-50196.83 +/- 8.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 725000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13        |\n",
      "|    n_updates        | 180999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 728       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 59986     |\n",
      "|    total_timesteps  | 728000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.1      |\n",
      "|    n_updates        | 181749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=-50212.45 +/- 7.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 730000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.3      |\n",
      "|    n_updates        | 182249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 732       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 60334     |\n",
      "|    total_timesteps  | 732000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 23.8      |\n",
      "|    n_updates        | 182749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=735000, episode_reward=-50214.12 +/- 7.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 735000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.5      |\n",
      "|    n_updates        | 183499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 736       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 60672     |\n",
      "|    total_timesteps  | 736000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.53      |\n",
      "|    n_updates        | 183749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=-43336.48 +/- 6079.35\n",
      "Episode length: 926.60 +/- 66.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 927       |\n",
      "|    mean_reward      | -4.33e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 740000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.4      |\n",
      "|    n_updates        | 184749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 740       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 61000     |\n",
      "|    total_timesteps  | 740000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 744       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 61220     |\n",
      "|    total_timesteps  | 744000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.7      |\n",
      "|    n_updates        | 185749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=745000, episode_reward=-50206.16 +/- 10.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 745000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.3      |\n",
      "|    n_updates        | 185999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 748       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 61557     |\n",
      "|    total_timesteps  | 748000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.9      |\n",
      "|    n_updates        | 186749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=-50199.46 +/- 17.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 750000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19        |\n",
      "|    n_updates        | 187249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 752       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 61896     |\n",
      "|    total_timesteps  | 752000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.7      |\n",
      "|    n_updates        | 187749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=755000, episode_reward=-31639.41 +/- 9715.70\n",
      "Episode length: 785.00 +/- 115.08\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 785       |\n",
      "|    mean_reward      | -3.16e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 755000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.1      |\n",
      "|    n_updates        | 188499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 756       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 62209     |\n",
      "|    total_timesteps  | 756000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.7      |\n",
      "|    n_updates        | 188749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=-50204.60 +/- 9.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 760000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.9      |\n",
      "|    n_updates        | 189749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 760       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 62547     |\n",
      "|    total_timesteps  | 760000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 764       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 62767     |\n",
      "|    total_timesteps  | 764000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 33.6      |\n",
      "|    n_updates        | 190749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=765000, episode_reward=-50185.43 +/- 32.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 765000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.6      |\n",
      "|    n_updates        | 190999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 768       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 63106     |\n",
      "|    total_timesteps  | 768000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 30.1      |\n",
      "|    n_updates        | 191749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=-50210.65 +/- 9.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 770000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.2      |\n",
      "|    n_updates        | 192249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 772       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 63448     |\n",
      "|    total_timesteps  | 772000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 33.9      |\n",
      "|    n_updates        | 192749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=775000, episode_reward=-50206.59 +/- 10.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 775000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13        |\n",
      "|    n_updates        | 193499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 776       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 63790     |\n",
      "|    total_timesteps  | 776000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 32        |\n",
      "|    n_updates        | 193749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-50199.40 +/- 29.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 780000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.1      |\n",
      "|    n_updates        | 194749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 780       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 64129     |\n",
      "|    total_timesteps  | 780000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 784       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 64347     |\n",
      "|    total_timesteps  | 784000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 33.6      |\n",
      "|    n_updates        | 195749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=785000, episode_reward=-50203.04 +/- 7.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 785000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.9      |\n",
      "|    n_updates        | 195999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 788       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 64687     |\n",
      "|    total_timesteps  | 788000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 23.5      |\n",
      "|    n_updates        | 196749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=-50209.26 +/- 11.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 790000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17        |\n",
      "|    n_updates        | 197249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 792       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 65026     |\n",
      "|    total_timesteps  | 792000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.1      |\n",
      "|    n_updates        | 197749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=795000, episode_reward=-50194.26 +/- 9.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 795000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 38.4      |\n",
      "|    n_updates        | 198499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 796       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 65367     |\n",
      "|    total_timesteps  | 796000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 31.8      |\n",
      "|    n_updates        | 198749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=-50212.25 +/- 13.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 800000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 27.1      |\n",
      "|    n_updates        | 199749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 800       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 65709     |\n",
      "|    total_timesteps  | 800000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 804       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 65930     |\n",
      "|    total_timesteps  | 804000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.7      |\n",
      "|    n_updates        | 200749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=805000, episode_reward=-50210.46 +/- 6.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 805000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.3      |\n",
      "|    n_updates        | 200999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 808       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 66274     |\n",
      "|    total_timesteps  | 808000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.56      |\n",
      "|    n_updates        | 201749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=-50207.07 +/- 12.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 810000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 31        |\n",
      "|    n_updates        | 202249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 812       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 66620     |\n",
      "|    total_timesteps  | 812000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.1      |\n",
      "|    n_updates        | 202749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=815000, episode_reward=-50210.20 +/- 8.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 815000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 26.7      |\n",
      "|    n_updates        | 203499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 816       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 66965     |\n",
      "|    total_timesteps  | 816000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22        |\n",
      "|    n_updates        | 203749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=-50183.96 +/- 25.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 820000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13        |\n",
      "|    n_updates        | 204749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 820       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 67296     |\n",
      "|    total_timesteps  | 820000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 824       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 67526     |\n",
      "|    total_timesteps  | 824000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 28.7      |\n",
      "|    n_updates        | 205749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=825000, episode_reward=-50204.71 +/- 15.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 825000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.7      |\n",
      "|    n_updates        | 205999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 828       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 67890     |\n",
      "|    total_timesteps  | 828000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.1      |\n",
      "|    n_updates        | 206749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=-50204.52 +/- 14.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 830000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.3      |\n",
      "|    n_updates        | 207249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 832       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 68236     |\n",
      "|    total_timesteps  | 832000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.5      |\n",
      "|    n_updates        | 207749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=835000, episode_reward=-50208.19 +/- 5.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 835000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 27.3      |\n",
      "|    n_updates        | 208499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 836       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 68559     |\n",
      "|    total_timesteps  | 836000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.3      |\n",
      "|    n_updates        | 208749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=-50202.96 +/- 4.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 840000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.8      |\n",
      "|    n_updates        | 209749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 840       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 68879     |\n",
      "|    total_timesteps  | 840000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 844       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 69081     |\n",
      "|    total_timesteps  | 844000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16        |\n",
      "|    n_updates        | 210749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=845000, episode_reward=-50196.14 +/- 13.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 845000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.4      |\n",
      "|    n_updates        | 210999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 848       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 69399     |\n",
      "|    total_timesteps  | 848000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.1      |\n",
      "|    n_updates        | 211749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=-41928.25 +/- 6133.67\n",
      "Episode length: 910.60 +/- 70.55\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 911       |\n",
      "|    mean_reward      | -4.19e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 850000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 23.3      |\n",
      "|    n_updates        | 212249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 852       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 69705     |\n",
      "|    total_timesteps  | 852000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.4      |\n",
      "|    n_updates        | 212749    |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simple DQN architecture\n",
    "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
    "\n",
    "# Set up the model with simpler hyperparameters\n",
    "model = DQN('CnnPolicy', custom_env, policy_kwargs=policy_kwargs, \n",
    "            verbose=1, buffer_size=50000, learning_starts=1000, \n",
    "            learning_rate=0.0005, batch_size=32, exploration_fraction=0.1,\n",
    "            exploration_final_eps=0.05)\n",
    "\n",
    "# Setup evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(custom_env, best_model_save_path=model_dir,\n",
    "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
    "                                         name_prefix='dqn_model_checkpoint')\n",
    "\n",
    "\n",
    "\n",
    "# Start training the model with callbacks for evaluation and checkpoints\n",
    "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the final model after training\n",
    "model.save(\"dqn_custom_env_model\")\n",
    "custom_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env.close()\n",
    "del custom_env\n",
    "foo = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env = EnhancedCarRacing(render_mode=\"rgb_array\")\n",
    "obs, info = custom_env.reset()\n",
    "custom_env = GrayscaleObservation(custom_env, keep_dim=True)\n",
    "custom_env = TimeLimit(custom_env, max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'PPO_env_mod_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1217 21:07:15.281412367 NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "/home/derpy/Documents/Fac/ano3/semestre1/ISIA/reinforcement-learning-with-gymnasium/.venv/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x76bd92db0620> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x76bd92db1970>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 77        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 26        |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 55           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 73           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035775134 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.22        |\n",
      "|    explained_variance   | 4.82e-05     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 4.2e+05      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000235    |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 8.75e+05     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derpy/Documents/Fac/ano3/semestre1/ISIA/reinforcement-learning-with-gymnasium/.venv/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=-50217.84 +/- 10.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011489361 |\n",
      "|    clip_fraction        | 0.0149       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.18        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.68e+05     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | 0.000451     |\n",
      "|    std                  | 0.975        |\n",
      "|    value_loss           | 8.78e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 36        |\n",
      "|    iterations      | 3         |\n",
      "|    time_elapsed    | 170       |\n",
      "|    total_timesteps | 6144      |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 37           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 216          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055847196 |\n",
      "|    clip_fraction        | 0.0579       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.2         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 4.27e+05     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00345     |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 8.73e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-50212.01 +/- 7.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005333224 |\n",
      "|    clip_fraction        | 0.0343      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.21       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.34e+05    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 8.68e+05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 33        |\n",
      "|    iterations      | 5         |\n",
      "|    time_elapsed    | 310       |\n",
      "|    total_timesteps | 10240     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 35           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 350          |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031201052 |\n",
      "|    clip_fraction        | 0.0192       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.22        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.94e+05     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00081     |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 8.63e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 36           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 391          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020285302 |\n",
      "|    clip_fraction        | 0.0167       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.22        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.61e+05     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000487    |\n",
      "|    std                  | 0.984        |\n",
      "|    value_loss           | 8.58e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-50208.68 +/- 10.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004211774 |\n",
      "|    clip_fraction        | 0.0249      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.21       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.21e+05    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.000776   |\n",
      "|    std                  | 0.983       |\n",
      "|    value_loss           | 8.53e+05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 33        |\n",
      "|    iterations      | 8         |\n",
      "|    time_elapsed    | 488       |\n",
      "|    total_timesteps | 16384     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 34          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 529         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003739717 |\n",
      "|    clip_fraction        | 0.0218      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.2        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.53e+05    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.000129   |\n",
      "|    std                  | 0.983       |\n",
      "|    value_loss           | 8.49e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-50201.02 +/- 12.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005139638 |\n",
      "|    clip_fraction        | 0.039       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.21       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.11e+05    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    std                  | 0.985       |\n",
      "|    value_loss           | 8.44e+05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 10        |\n",
      "|    time_elapsed    | 625       |\n",
      "|    total_timesteps | 20480     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 33           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 666          |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053378707 |\n",
      "|    clip_fraction        | 0.0519       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.19        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 4.44e+05     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00325     |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 8.41e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 34           |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 706          |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020102104 |\n",
      "|    clip_fraction        | 0.00435      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.17        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 5.03e+05     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | 0.00102      |\n",
      "|    std                  | 0.972        |\n",
      "|    value_loss           | 8.36e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-50212.16 +/- 12.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004693583 |\n",
      "|    clip_fraction        | 0.0378      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.17       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.6e+05     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00251    |\n",
      "|    std                  | 0.973       |\n",
      "|    value_loss           | 8.32e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 33        |\n",
      "|    iterations      | 13        |\n",
      "|    time_elapsed    | 801       |\n",
      "|    total_timesteps | 26624     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 34           |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 842          |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035684058 |\n",
      "|    clip_fraction        | 0.0347       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.17        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 5.17e+05     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000967    |\n",
      "|    std                  | 0.971        |\n",
      "|    value_loss           | 8.29e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-50205.93 +/- 10.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051206285 |\n",
      "|    clip_fraction        | 0.0476       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.18        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 4.49e+05     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00327     |\n",
      "|    std                  | 0.975        |\n",
      "|    value_loss           | 8.25e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 15        |\n",
      "|    time_elapsed    | 936       |\n",
      "|    total_timesteps | 30720     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 33          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 977         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004766228 |\n",
      "|    clip_fraction        | 0.0184      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.17       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.82e+05    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.000203   |\n",
      "|    std                  | 0.969       |\n",
      "|    value_loss           | 8.22e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 34          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 1018        |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004825528 |\n",
      "|    clip_fraction        | 0.0423      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.14       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.59e+05    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00177    |\n",
      "|    std                  | 0.957       |\n",
      "|    value_loss           | 8.19e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-50216.07 +/- 13.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004123984 |\n",
      "|    clip_fraction        | 0.0331      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.11       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.42e+05    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0013     |\n",
      "|    std                  | 0.953       |\n",
      "|    value_loss           | 8.16e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 33        |\n",
      "|    iterations      | 18        |\n",
      "|    time_elapsed    | 1114      |\n",
      "|    total_timesteps | 36864     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 33           |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 1154         |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040102806 |\n",
      "|    clip_fraction        | 0.0146       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.11        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.19e+05     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.000293    |\n",
      "|    std                  | 0.953        |\n",
      "|    value_loss           | 8.13e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-50210.18 +/- 21.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005955025 |\n",
      "|    clip_fraction        | 0.0626      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.11       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.51e+05    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00444    |\n",
      "|    std                  | 0.952       |\n",
      "|    value_loss           | 8.11e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 20        |\n",
      "|    time_elapsed    | 1249      |\n",
      "|    total_timesteps | 40960     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 33           |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 1290         |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058530797 |\n",
      "|    clip_fraction        | 0.0484       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.11        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.93e+05     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00292     |\n",
      "|    std                  | 0.951        |\n",
      "|    value_loss           | 8.08e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-50199.35 +/- 13.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 45000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060578343 |\n",
      "|    clip_fraction        | 0.0574       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.1         |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 4.09e+05     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00377     |\n",
      "|    std                  | 0.947        |\n",
      "|    value_loss           | 7.98e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 22        |\n",
      "|    time_elapsed    | 1386      |\n",
      "|    total_timesteps | 45056     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 1427         |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044201384 |\n",
      "|    clip_fraction        | 0.0347       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.08        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.96e+05     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    std                  | 0.941        |\n",
      "|    value_loss           | 7.71e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 33          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 1469        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004622897 |\n",
      "|    clip_fraction        | 0.0478      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.07       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.11e+05    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00271    |\n",
      "|    std                  | 0.938       |\n",
      "|    value_loss           | 7.67e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-50211.83 +/- 9.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056048874 |\n",
      "|    clip_fraction        | 0.0639       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.03        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.76e+05     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00491     |\n",
      "|    std                  | 0.919        |\n",
      "|    value_loss           | 7.61e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 25        |\n",
      "|    time_elapsed    | 1564      |\n",
      "|    total_timesteps | 51200     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 33           |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 1604         |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057532904 |\n",
      "|    clip_fraction        | 0.0624       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4           |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 4.23e+05     |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00395     |\n",
      "|    std                  | 0.917        |\n",
      "|    value_loss           | 7.57e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-50210.49 +/- 12.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 55000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014560053 |\n",
      "|    clip_fraction        | 0.0152       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.02        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.49e+05     |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -9.29e-06    |\n",
      "|    std                  | 0.929        |\n",
      "|    value_loss           | 7.52e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 27        |\n",
      "|    time_elapsed    | 1700      |\n",
      "|    total_timesteps | 55296     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 1740        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006216801 |\n",
      "|    clip_fraction        | 0.0321      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.02       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.84e+05    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00226    |\n",
      "|    std                  | 0.926       |\n",
      "|    value_loss           | 7.48e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 33           |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 1781         |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044402946 |\n",
      "|    clip_fraction        | 0.0494       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.97        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 4.76e+05     |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00347     |\n",
      "|    std                  | 0.903        |\n",
      "|    value_loss           | 7.44e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-50210.22 +/- 19.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 60000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00445793 |\n",
      "|    clip_fraction        | 0.0231     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.92      |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 2.85e+05   |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | -0.00131   |\n",
      "|    std                  | 0.887      |\n",
      "|    value_loss           | 7.39e+05   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 30        |\n",
      "|    time_elapsed    | 1876      |\n",
      "|    total_timesteps | 61440     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 33           |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 1917         |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050120624 |\n",
      "|    clip_fraction        | 0.0367       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.87        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 4.01e+05     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    std                  | 0.877        |\n",
      "|    value_loss           | 7.35e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-50209.90 +/- 10.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 65000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003344505 |\n",
      "|    clip_fraction        | 0.0294      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.89       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.96e+05    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00123    |\n",
      "|    std                  | 0.892       |\n",
      "|    value_loss           | 7.31e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 32        |\n",
      "|    time_elapsed    | 2010      |\n",
      "|    total_timesteps | 65536     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 2051        |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005554796 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.92       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.37e+05    |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00167    |\n",
      "|    std                  | 0.895       |\n",
      "|    value_loss           | 7.28e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 33           |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 2091         |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063190586 |\n",
      "|    clip_fraction        | 0.0379       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.91        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.09e+05     |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.0021      |\n",
      "|    std                  | 0.889        |\n",
      "|    value_loss           | 7.25e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-50211.83 +/- 12.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031381242 |\n",
      "|    clip_fraction        | 0.0193       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.89        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 4.83e+05     |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.000404    |\n",
      "|    std                  | 0.884        |\n",
      "|    value_loss           | 7.21e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 35        |\n",
      "|    time_elapsed    | 2186      |\n",
      "|    total_timesteps | 71680     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 33           |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 2227         |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036438962 |\n",
      "|    clip_fraction        | 0.0177       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.89        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.31e+05     |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | 0.000471     |\n",
      "|    std                  | 0.886        |\n",
      "|    value_loss           | 7.18e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-50208.16 +/- 9.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 75000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038138516 |\n",
      "|    clip_fraction        | 0.0258       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.88        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.67e+05     |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.000555    |\n",
      "|    std                  | 0.882        |\n",
      "|    value_loss           | 7.16e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 37        |\n",
      "|    time_elapsed    | 2322      |\n",
      "|    total_timesteps | 75776     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 2364         |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041654394 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.89        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.26e+05     |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -2.8e-05     |\n",
      "|    std                  | 0.89         |\n",
      "|    value_loss           | 7.13e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 33           |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 2404         |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050997892 |\n",
      "|    clip_fraction        | 0.0304       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.92        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.38e+05     |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    std                  | 0.9          |\n",
      "|    value_loss           | 7.1e+05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-50212.20 +/- 14.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051898835 |\n",
      "|    clip_fraction        | 0.0417       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.91        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 4.4e+05      |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.0028      |\n",
      "|    std                  | 0.886        |\n",
      "|    value_loss           | 7.08e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 40        |\n",
      "|    time_elapsed    | 2500      |\n",
      "|    total_timesteps | 81920     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 33           |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 2541         |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044674054 |\n",
      "|    clip_fraction        | 0.0285       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.88        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.56e+05     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.000351    |\n",
      "|    std                  | 0.885        |\n",
      "|    value_loss           | 7.05e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-50199.13 +/- 18.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 85000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042907177 |\n",
      "|    clip_fraction        | 0.0327       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.87        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.28e+05     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00139     |\n",
      "|    std                  | 0.877        |\n",
      "|    value_loss           | 7.03e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 42        |\n",
      "|    time_elapsed    | 2636      |\n",
      "|    total_timesteps | 86016     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 2677        |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005490194 |\n",
      "|    clip_fraction        | 0.0419      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.84       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.1e+05     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0019     |\n",
      "|    std                  | 0.869       |\n",
      "|    value_loss           | 6.88e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-50210.92 +/- 10.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 90000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006035866 |\n",
      "|    clip_fraction        | 0.0537      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.84       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.9e+05     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.003      |\n",
      "|    std                  | 0.875       |\n",
      "|    value_loss           | 6.69e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 44        |\n",
      "|    time_elapsed    | 2774      |\n",
      "|    total_timesteps | 90112     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 2815        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005123888 |\n",
      "|    clip_fraction        | 0.0439      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.86       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.61e+05    |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00281    |\n",
      "|    std                  | 0.881       |\n",
      "|    value_loss           | 6.65e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 2856         |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055309064 |\n",
      "|    clip_fraction        | 0.0382       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.87        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.17e+05     |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    std                  | 0.878        |\n",
      "|    value_loss           | 6.61e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-50206.47 +/- 9.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 95000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042765243 |\n",
      "|    clip_fraction        | 0.02         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.85        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.35e+05     |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.000469    |\n",
      "|    std                  | 0.873        |\n",
      "|    value_loss           | 6.56e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 47        |\n",
      "|    time_elapsed    | 2951      |\n",
      "|    total_timesteps | 96256     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 2991         |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046468657 |\n",
      "|    clip_fraction        | 0.0159       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.83        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.28e+05     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | 0.000607     |\n",
      "|    std                  | 0.864        |\n",
      "|    value_loss           | 6.52e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-50206.77 +/- 10.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035653713 |\n",
      "|    clip_fraction        | 0.0232       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.82        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.65e+05     |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.000496    |\n",
      "|    std                  | 0.869        |\n",
      "|    value_loss           | 6.48e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 49        |\n",
      "|    time_elapsed    | 3087      |\n",
      "|    total_timesteps | 100352    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 3129         |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045063403 |\n",
      "|    clip_fraction        | 0.0389       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.81        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.43e+05     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00261     |\n",
      "|    std                  | 0.861        |\n",
      "|    value_loss           | 6.44e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 3169        |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005472139 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.8        |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.06e+05    |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.000807   |\n",
      "|    std                  | 0.86        |\n",
      "|    value_loss           | 6.4e+05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-50213.64 +/- 9.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 105000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048526945 |\n",
      "|    clip_fraction        | 0.0273       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.78        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.12e+05     |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.000766    |\n",
      "|    std                  | 0.854        |\n",
      "|    value_loss           | 6.37e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 52        |\n",
      "|    time_elapsed    | 3265      |\n",
      "|    total_timesteps | 106496    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 3305        |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002996068 |\n",
      "|    clip_fraction        | 0.0253      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.74       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.63e+05    |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.000826   |\n",
      "|    std                  | 0.839       |\n",
      "|    value_loss           | 6.34e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-50206.50 +/- 10.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 110000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005035983 |\n",
      "|    clip_fraction        | 0.0235      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.71       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.11e+05    |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.000502   |\n",
      "|    std                  | 0.833       |\n",
      "|    value_loss           | 6.3e+05     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 54        |\n",
      "|    time_elapsed    | 3401      |\n",
      "|    total_timesteps | 110592    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 3442        |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003454103 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.71       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.65e+05    |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00137    |\n",
      "|    std                  | 0.838       |\n",
      "|    value_loss           | 6.27e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 3482         |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036001643 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.73        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.83e+05     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.00075     |\n",
      "|    std                  | 0.845        |\n",
      "|    value_loss           | 6.24e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-50205.07 +/- 15.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 115000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030244382 |\n",
      "|    clip_fraction        | 0.00786      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.75        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.98e+05     |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | 0.000711     |\n",
      "|    std                  | 0.847        |\n",
      "|    value_loss           | 6.21e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 57        |\n",
      "|    time_elapsed    | 3578      |\n",
      "|    total_timesteps | 116736    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 3619         |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053925673 |\n",
      "|    clip_fraction        | 0.0393       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.76        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.33e+05     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    std                  | 0.851        |\n",
      "|    value_loss           | 6.19e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-50202.83 +/- 7.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002308815 |\n",
      "|    clip_fraction        | 0.0237      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.77       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.68e+05    |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00135    |\n",
      "|    std                  | 0.853       |\n",
      "|    value_loss           | 6.17e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 59        |\n",
      "|    time_elapsed    | 3713      |\n",
      "|    total_timesteps | 120832    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 3754         |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040922947 |\n",
      "|    clip_fraction        | 0.034        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.77        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.24e+05     |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    std                  | 0.854        |\n",
      "|    value_loss           | 6.15e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 61           |\n",
      "|    time_elapsed         | 3795         |\n",
      "|    total_timesteps      | 124928       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041097053 |\n",
      "|    clip_fraction        | 0.0258       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.75        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.64e+05     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.000926    |\n",
      "|    std                  | 0.841        |\n",
      "|    value_loss           | 6.13e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-50208.80 +/- 10.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 125000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067175375 |\n",
      "|    clip_fraction        | 0.0445       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.7         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.05e+05     |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.00283     |\n",
      "|    std                  | 0.828        |\n",
      "|    value_loss           | 6.11e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 62        |\n",
      "|    time_elapsed    | 3892      |\n",
      "|    total_timesteps | 126976    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 3933         |\n",
      "|    total_timesteps      | 129024       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044888062 |\n",
      "|    clip_fraction        | 0.0318       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.68        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.33e+05     |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    std                  | 0.828        |\n",
      "|    value_loss           | 6.09e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-50206.43 +/- 14.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 130000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038238412 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.68        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.45e+05     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.000609    |\n",
      "|    std                  | 0.829        |\n",
      "|    value_loss           | 5.89e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 64        |\n",
      "|    time_elapsed    | 4027      |\n",
      "|    total_timesteps | 131072    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 4068        |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004211833 |\n",
      "|    clip_fraction        | 0.0498      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.67       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.44e+05    |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.00225    |\n",
      "|    std                  | 0.827       |\n",
      "|    value_loss           | 5.79e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=-50210.10 +/- 10.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 135000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005115783 |\n",
      "|    clip_fraction        | 0.039       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.65       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.57e+05    |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00212    |\n",
      "|    std                  | 0.819       |\n",
      "|    value_loss           | 5.75e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 66        |\n",
      "|    time_elapsed    | 4162      |\n",
      "|    total_timesteps | 135168    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 4203         |\n",
      "|    total_timesteps      | 137216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046936455 |\n",
      "|    clip_fraction        | 0.043        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.63        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.87e+05     |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    std                  | 0.819        |\n",
      "|    value_loss           | 5.7e+05      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 4243         |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064134086 |\n",
      "|    clip_fraction        | 0.0572       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.62        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.51e+05     |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.00337     |\n",
      "|    std                  | 0.815        |\n",
      "|    value_loss           | 5.66e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-50202.34 +/- 9.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062732855 |\n",
      "|    clip_fraction        | 0.0502       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.6         |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.03e+05     |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    std                  | 0.805        |\n",
      "|    value_loss           | 5.62e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 69        |\n",
      "|    time_elapsed    | 4338      |\n",
      "|    total_timesteps | 141312    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 4378         |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039853584 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.61        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.87e+05     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -2.39e-06    |\n",
      "|    std                  | 0.819        |\n",
      "|    value_loss           | 5.59e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=-50197.13 +/- 12.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 145000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00492079 |\n",
      "|    clip_fraction        | 0.0525     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.64      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 2.84e+05   |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.00267   |\n",
      "|    std                  | 0.822      |\n",
      "|    value_loss           | 5.55e+05   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 71        |\n",
      "|    time_elapsed    | 4474      |\n",
      "|    total_timesteps | 145408    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 4516         |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058230264 |\n",
      "|    clip_fraction        | 0.0516       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.63        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2e+05        |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    std                  | 0.817        |\n",
      "|    value_loss           | 5.52e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 4556        |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004325344 |\n",
      "|    clip_fraction        | 0.0279      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.6        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.95e+05    |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.000215   |\n",
      "|    std                  | 0.809       |\n",
      "|    value_loss           | 5.49e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-50204.92 +/- 14.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004037902 |\n",
      "|    clip_fraction        | 0.0279      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.56       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.61e+05    |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.000934   |\n",
      "|    std                  | 0.796       |\n",
      "|    value_loss           | 5.46e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 74        |\n",
      "|    time_elapsed    | 4652      |\n",
      "|    total_timesteps | 151552    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 75           |\n",
      "|    time_elapsed         | 4694         |\n",
      "|    total_timesteps      | 153600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036211885 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.52        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.76e+05     |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.000291    |\n",
      "|    std                  | 0.786        |\n",
      "|    value_loss           | 5.43e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=-50212.14 +/- 7.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 155000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042840363 |\n",
      "|    clip_fraction        | 0.0366       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.49        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.06e+05     |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.00175     |\n",
      "|    std                  | 0.779        |\n",
      "|    value_loss           | 5.4e+05      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 76        |\n",
      "|    time_elapsed    | 4789      |\n",
      "|    total_timesteps | 155648    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 4830        |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007373413 |\n",
      "|    clip_fraction        | 0.0507      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.43       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.68e+05    |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.00387    |\n",
      "|    std                  | 0.76        |\n",
      "|    value_loss           | 5.37e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 4871         |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037197233 |\n",
      "|    clip_fraction        | 0.0313       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.38e+05     |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.000328    |\n",
      "|    std                  | 0.763        |\n",
      "|    value_loss           | 5.35e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-50210.45 +/- 3.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038356609 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.43        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3e+05        |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.000609    |\n",
      "|    std                  | 0.769        |\n",
      "|    value_loss           | 5.33e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 79        |\n",
      "|    time_elapsed    | 4967      |\n",
      "|    total_timesteps | 161792    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 5008         |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059884824 |\n",
      "|    clip_fraction        | 0.0423       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.45        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.36e+05     |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    std                  | 0.776        |\n",
      "|    value_loss           | 5.31e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=-50202.98 +/- 12.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 165000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072309296 |\n",
      "|    clip_fraction        | 0.0542       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.47        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.1e+05      |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.00265     |\n",
      "|    std                  | 0.78         |\n",
      "|    value_loss           | 5.29e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 81        |\n",
      "|    time_elapsed    | 5104      |\n",
      "|    total_timesteps | 165888    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 5145         |\n",
      "|    total_timesteps      | 167936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030593337 |\n",
      "|    clip_fraction        | 0.023        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.46        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.52e+05     |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    std                  | 0.776        |\n",
      "|    value_loss           | 5.28e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 83           |\n",
      "|    time_elapsed         | 5186         |\n",
      "|    total_timesteps      | 169984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025094047 |\n",
      "|    clip_fraction        | 0.0341       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.45        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.87e+05     |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    std                  | 0.778        |\n",
      "|    value_loss           | 5.26e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-50206.76 +/- 3.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 170000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038181609 |\n",
      "|    clip_fraction        | 0.0444       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.45        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.98e+05     |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    std                  | 0.778        |\n",
      "|    value_loss           | 5.25e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 84        |\n",
      "|    time_elapsed    | 5282      |\n",
      "|    total_timesteps | 172032    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 5323        |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002391261 |\n",
      "|    clip_fraction        | 0.0257      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.43       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.16e+05    |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.00132    |\n",
      "|    std                  | 0.765       |\n",
      "|    value_loss           | 5.04e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=-50208.03 +/- 13.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 175000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005420194 |\n",
      "|    clip_fraction        | 0.0522      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.37       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.33e+05    |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00341    |\n",
      "|    std                  | 0.754       |\n",
      "|    value_loss           | 4.97e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 86        |\n",
      "|    time_elapsed    | 5418      |\n",
      "|    total_timesteps | 176128    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 87           |\n",
      "|    time_elapsed         | 5459         |\n",
      "|    total_timesteps      | 178176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019267958 |\n",
      "|    clip_fraction        | 0.0127       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.34        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.5e+05      |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | 0.00107      |\n",
      "|    std                  | 0.751        |\n",
      "|    value_loss           | 4.93e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-50202.70 +/- 12.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006234116 |\n",
      "|    clip_fraction        | 0.0513      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.35       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.17e+05    |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00291    |\n",
      "|    std                  | 0.754       |\n",
      "|    value_loss           | 4.9e+05     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 88        |\n",
      "|    time_elapsed    | 5554      |\n",
      "|    total_timesteps | 180224    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 5596         |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046365885 |\n",
      "|    clip_fraction        | 0.034        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.34        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.99e+05     |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    std                  | 0.744        |\n",
      "|    value_loss           | 4.86e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 5637         |\n",
      "|    total_timesteps      | 184320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061358544 |\n",
      "|    clip_fraction        | 0.0399       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.28        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.11e+05     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.00222     |\n",
      "|    std                  | 0.726        |\n",
      "|    value_loss           | 4.82e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-50204.68 +/- 12.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 185000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004505831 |\n",
      "|    clip_fraction        | 0.0268      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.26       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.38e+05    |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    std                  | 0.728       |\n",
      "|    value_loss           | 4.79e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 91        |\n",
      "|    time_elapsed    | 5733      |\n",
      "|    total_timesteps | 186368    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 5773        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004065321 |\n",
      "|    clip_fraction        | 0.031       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.29       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.32e+05    |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.000782   |\n",
      "|    std                  | 0.738       |\n",
      "|    value_loss           | 4.76e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-50212.09 +/- 10.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 190000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004944363 |\n",
      "|    clip_fraction        | 0.0296      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.29       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.08e+05    |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -4.08e-05   |\n",
      "|    std                  | 0.731       |\n",
      "|    value_loss           | 4.74e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 93        |\n",
      "|    time_elapsed    | 5868      |\n",
      "|    total_timesteps | 190464    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 5910        |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004985301 |\n",
      "|    clip_fraction        | 0.0343      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.27       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.13e+05    |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00197    |\n",
      "|    std                  | 0.728       |\n",
      "|    value_loss           | 4.71e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 5951         |\n",
      "|    total_timesteps      | 194560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041493555 |\n",
      "|    clip_fraction        | 0.0483       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.26        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.03e+05     |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.00294     |\n",
      "|    std                  | 0.729        |\n",
      "|    value_loss           | 4.68e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-50204.81 +/- 7.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 195000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063999826 |\n",
      "|    clip_fraction        | 0.0325       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.27        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.87e+05     |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | -0.000861    |\n",
      "|    std                  | 0.729        |\n",
      "|    value_loss           | 4.65e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 96        |\n",
      "|    time_elapsed    | 6047      |\n",
      "|    total_timesteps | 196608    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 6089         |\n",
      "|    total_timesteps      | 198656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055314964 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.25        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.36e+05     |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.000739    |\n",
      "|    std                  | 0.72         |\n",
      "|    value_loss           | 4.63e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-50208.52 +/- 11.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 200000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043704547 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.23        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.82e+05     |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.000725    |\n",
      "|    std                  | 0.715        |\n",
      "|    value_loss           | 4.61e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 98        |\n",
      "|    time_elapsed    | 6185      |\n",
      "|    total_timesteps | 200704    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 6227        |\n",
      "|    total_timesteps      | 202752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005623165 |\n",
      "|    clip_fraction        | 0.0413      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.21       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.71e+05    |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.00182    |\n",
      "|    std                  | 0.717       |\n",
      "|    value_loss           | 4.59e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 100          |\n",
      "|    time_elapsed         | 6268         |\n",
      "|    total_timesteps      | 204800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062211957 |\n",
      "|    clip_fraction        | 0.0485       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.2         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.38e+05     |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | -0.00175     |\n",
      "|    std                  | 0.712        |\n",
      "|    value_loss           | 4.57e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=-50203.08 +/- 12.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 205000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036938689 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.24        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.17e+05     |\n",
      "|    n_updates            | 1000         |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    std                  | 0.728        |\n",
      "|    value_loss           | 4.56e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 101       |\n",
      "|    time_elapsed    | 6366      |\n",
      "|    total_timesteps | 206848    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 102         |\n",
      "|    time_elapsed         | 6408        |\n",
      "|    total_timesteps      | 208896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006268311 |\n",
      "|    clip_fraction        | 0.042       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.23       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.37e+05    |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    std                  | 0.718       |\n",
      "|    value_loss           | 4.54e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-50197.36 +/- 13.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 210000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059842886 |\n",
      "|    clip_fraction        | 0.0589       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.23        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.33e+05     |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.00356     |\n",
      "|    std                  | 0.724        |\n",
      "|    value_loss           | 4.53e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 103       |\n",
      "|    time_elapsed    | 6504      |\n",
      "|    total_timesteps | 210944    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 104          |\n",
      "|    time_elapsed         | 6546         |\n",
      "|    total_timesteps      | 212992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039617494 |\n",
      "|    clip_fraction        | 0.0314       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.24        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.65e+05     |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | -0.000802    |\n",
      "|    std                  | 0.723        |\n",
      "|    value_loss           | 4.52e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=-50191.65 +/- 17.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 215000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055840327 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.24        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.6e+05      |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | 1.69e-05     |\n",
      "|    std                  | 0.721        |\n",
      "|    value_loss           | 4.52e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 105       |\n",
      "|    time_elapsed    | 6643      |\n",
      "|    total_timesteps | 215040    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 106         |\n",
      "|    time_elapsed         | 6684        |\n",
      "|    total_timesteps      | 217088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009357236 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.23       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.79e+05    |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | 0.00421     |\n",
      "|    std                  | 0.716       |\n",
      "|    value_loss           | 4.27e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 107          |\n",
      "|    time_elapsed         | 6727         |\n",
      "|    total_timesteps      | 219136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066315723 |\n",
      "|    clip_fraction        | 0.0929       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.21        |\n",
      "|    explained_variance   | 0.0869       |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.96e+05     |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | 4.16e-05     |\n",
      "|    std                  | 0.711        |\n",
      "|    value_loss           | 4.23e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-50210.84 +/- 12.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 220000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00854824 |\n",
      "|    clip_fraction        | 0.0917     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.14      |\n",
      "|    explained_variance   | 0.122      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.48e+05   |\n",
      "|    n_updates            | 1070       |\n",
      "|    policy_gradient_loss | -0.00128   |\n",
      "|    std                  | 0.69       |\n",
      "|    value_loss           | 3.43e+05   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 108       |\n",
      "|    time_elapsed    | 6824      |\n",
      "|    total_timesteps | 221184    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 109         |\n",
      "|    time_elapsed         | 6866        |\n",
      "|    total_timesteps      | 223232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005092926 |\n",
      "|    clip_fraction        | 0.0465      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.08       |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.01e+05    |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | 0.00107     |\n",
      "|    std                  | 0.677       |\n",
      "|    value_loss           | 3.19e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=-50206.28 +/- 7.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 225000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004815542 |\n",
      "|    clip_fraction        | 0.0631      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.02       |\n",
      "|    explained_variance   | 0.214       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.88e+05    |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | -0.00136    |\n",
      "|    std                  | 0.665       |\n",
      "|    value_loss           | 3.86e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 110       |\n",
      "|    time_elapsed    | 6962      |\n",
      "|    total_timesteps | 225280    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 32         |\n",
      "|    iterations           | 111        |\n",
      "|    time_elapsed         | 7005       |\n",
      "|    total_timesteps      | 227328     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00350831 |\n",
      "|    clip_fraction        | 0.0292     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.96      |\n",
      "|    explained_variance   | 0.284      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.69e+05   |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.000766  |\n",
      "|    std                  | 0.652      |\n",
      "|    value_loss           | 4.08e+05   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 112          |\n",
      "|    time_elapsed         | 7053         |\n",
      "|    total_timesteps      | 229376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050846767 |\n",
      "|    clip_fraction        | 0.0594       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.94        |\n",
      "|    explained_variance   | 0.315        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.09e+05     |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | -0.00331     |\n",
      "|    std                  | 0.652        |\n",
      "|    value_loss           | 4.05e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-50211.89 +/- 9.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 230000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057523362 |\n",
      "|    clip_fraction        | 0.0474       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | 0.351        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.66e+05     |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | 0.0012       |\n",
      "|    std                  | 0.648        |\n",
      "|    value_loss           | 4.03e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 113       |\n",
      "|    time_elapsed    | 7148      |\n",
      "|    total_timesteps | 231424    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 32        |\n",
      "|    iterations           | 114       |\n",
      "|    time_elapsed         | 7190      |\n",
      "|    total_timesteps      | 233472    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0065782 |\n",
      "|    clip_fraction        | 0.0634    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.93     |\n",
      "|    explained_variance   | 0.362     |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.91e+05  |\n",
      "|    n_updates            | 1130      |\n",
      "|    policy_gradient_loss | -0.00107  |\n",
      "|    std                  | 0.649     |\n",
      "|    value_loss           | 4e+05     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=-50200.80 +/- 10.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 235000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062017827 |\n",
      "|    clip_fraction        | 0.0679       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.9         |\n",
      "|    explained_variance   | 0.379        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.79e+05     |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.00237     |\n",
      "|    std                  | 0.637        |\n",
      "|    value_loss           | 3.97e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 115       |\n",
      "|    time_elapsed    | 7286      |\n",
      "|    total_timesteps | 235520    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 116         |\n",
      "|    time_elapsed         | 7329        |\n",
      "|    total_timesteps      | 237568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004602245 |\n",
      "|    clip_fraction        | 0.0532      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.368       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.8e+05     |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.000686   |\n",
      "|    std                  | 0.626       |\n",
      "|    value_loss           | 3.95e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 117          |\n",
      "|    time_elapsed         | 7371         |\n",
      "|    total_timesteps      | 239616       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051150788 |\n",
      "|    clip_fraction        | 0.0588       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0.369        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.77e+05     |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | 0.000324     |\n",
      "|    std                  | 0.623        |\n",
      "|    value_loss           | 3.92e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-50202.83 +/- 12.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 240000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00984245 |\n",
      "|    clip_fraction        | 0.0688     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.8       |\n",
      "|    explained_variance   | 0.38       |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.94e+05   |\n",
      "|    n_updates            | 1170       |\n",
      "|    policy_gradient_loss | -0.00157   |\n",
      "|    std                  | 0.618      |\n",
      "|    value_loss           | 3.9e+05    |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 118       |\n",
      "|    time_elapsed    | 7467      |\n",
      "|    total_timesteps | 241664    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 32         |\n",
      "|    iterations           | 119        |\n",
      "|    time_elapsed         | 7508       |\n",
      "|    total_timesteps      | 243712     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00559403 |\n",
      "|    clip_fraction        | 0.0639     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.81      |\n",
      "|    explained_variance   | 0.397      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.68e+05   |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.00171   |\n",
      "|    std                  | 0.621      |\n",
      "|    value_loss           | 3.88e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=-50200.85 +/- 7.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 245000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004331373 |\n",
      "|    clip_fraction        | 0.0687      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.423       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.33e+05    |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | -0.00252    |\n",
      "|    std                  | 0.615       |\n",
      "|    value_loss           | 3.87e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 120       |\n",
      "|    time_elapsed    | 7605      |\n",
      "|    total_timesteps | 245760    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 121         |\n",
      "|    time_elapsed         | 7646        |\n",
      "|    total_timesteps      | 247808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004705649 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.394       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.8e+05     |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | 1.54e-06    |\n",
      "|    std                  | 0.62        |\n",
      "|    value_loss           | 3.85e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 122         |\n",
      "|    time_elapsed         | 7688        |\n",
      "|    total_timesteps      | 249856      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010615952 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.497       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.86e+05    |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | 0.00243     |\n",
      "|    std                  | 0.623       |\n",
      "|    value_loss           | 3.85e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=-50208.11 +/- 8.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 250000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00461411 |\n",
      "|    clip_fraction        | 0.033      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.82      |\n",
      "|    explained_variance   | 0.521      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.8e+05    |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | -0.00102   |\n",
      "|    std                  | 0.623      |\n",
      "|    value_loss           | 3.85e+05   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 123       |\n",
      "|    time_elapsed    | 7783      |\n",
      "|    total_timesteps | 251904    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 124          |\n",
      "|    time_elapsed         | 7824         |\n",
      "|    total_timesteps      | 253952       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043495195 |\n",
      "|    clip_fraction        | 0.0311       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.507        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.16e+05     |\n",
      "|    n_updates            | 1230         |\n",
      "|    policy_gradient_loss | 0.00179      |\n",
      "|    std                  | 0.626        |\n",
      "|    value_loss           | 3.84e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=-50208.44 +/- 11.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 255000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075945864 |\n",
      "|    clip_fraction        | 0.069        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.539        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.47e+05     |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.00342     |\n",
      "|    std                  | 0.625        |\n",
      "|    value_loss           | 3.81e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 125       |\n",
      "|    time_elapsed    | 7920      |\n",
      "|    total_timesteps | 256000    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 32         |\n",
      "|    iterations           | 126        |\n",
      "|    time_elapsed         | 7961       |\n",
      "|    total_timesteps      | 258048     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00498654 |\n",
      "|    clip_fraction        | 0.0262     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.83      |\n",
      "|    explained_variance   | 0.58       |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.92e+05   |\n",
      "|    n_updates            | 1250       |\n",
      "|    policy_gradient_loss | -0.00111   |\n",
      "|    std                  | 0.625      |\n",
      "|    value_loss           | 3.84e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-50210.33 +/- 12.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005540885 |\n",
      "|    clip_fraction        | 0.0413      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.65        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.15e+05    |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.00251    |\n",
      "|    std                  | 0.631       |\n",
      "|    value_loss           | 3.62e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 127       |\n",
      "|    time_elapsed    | 8058      |\n",
      "|    total_timesteps | 260096    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 128         |\n",
      "|    time_elapsed         | 8099        |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003160643 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.51e+05    |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | -0.00254    |\n",
      "|    std                  | 0.633       |\n",
      "|    value_loss           | 3.6e+05     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 32         |\n",
      "|    iterations           | 129        |\n",
      "|    time_elapsed         | 8141       |\n",
      "|    total_timesteps      | 264192     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00849997 |\n",
      "|    clip_fraction        | 0.0757     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.87      |\n",
      "|    explained_variance   | 0.607      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.2e+05    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.00625   |\n",
      "|    std                  | 0.633      |\n",
      "|    value_loss           | 3.59e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=-50198.87 +/- 11.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 265000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060526864 |\n",
      "|    clip_fraction        | 0.0421       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.636        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.43e+05     |\n",
      "|    n_updates            | 1290         |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    std                  | 0.625        |\n",
      "|    value_loss           | 3.54e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 130       |\n",
      "|    time_elapsed    | 8235      |\n",
      "|    total_timesteps | 266240    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 131         |\n",
      "|    time_elapsed         | 8276        |\n",
      "|    total_timesteps      | 268288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007934116 |\n",
      "|    clip_fraction        | 0.089       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.5e+05     |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.00478    |\n",
      "|    std                  | 0.621       |\n",
      "|    value_loss           | 3.52e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=-50208.77 +/- 6.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 270000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004657656 |\n",
      "|    clip_fraction        | 0.0355      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.621       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.22e+05    |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | -0.00146    |\n",
      "|    std                  | 0.608       |\n",
      "|    value_loss           | 3.49e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 132       |\n",
      "|    time_elapsed    | 8373      |\n",
      "|    total_timesteps | 270336    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 133          |\n",
      "|    time_elapsed         | 8414         |\n",
      "|    total_timesteps      | 272384       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041723573 |\n",
      "|    clip_fraction        | 0.0311       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.655        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.63e+05     |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.000131    |\n",
      "|    std                  | 0.596        |\n",
      "|    value_loss           | 3.46e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 134          |\n",
      "|    time_elapsed         | 8455         |\n",
      "|    total_timesteps      | 274432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059942165 |\n",
      "|    clip_fraction        | 0.0473       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.666        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.3e+05      |\n",
      "|    n_updates            | 1330         |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    std                  | 0.589        |\n",
      "|    value_loss           | 3.44e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=-50206.31 +/- 9.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 275000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033388578 |\n",
      "|    clip_fraction        | 0.044        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.64        |\n",
      "|    explained_variance   | 0.671        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.57e+05     |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | 0.000283     |\n",
      "|    std                  | 0.585        |\n",
      "|    value_loss           | 3.43e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 135       |\n",
      "|    time_elapsed    | 8552      |\n",
      "|    total_timesteps | 276480    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 136          |\n",
      "|    time_elapsed         | 8593         |\n",
      "|    total_timesteps      | 278528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067918543 |\n",
      "|    clip_fraction        | 0.0701       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.63        |\n",
      "|    explained_variance   | 0.66         |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.21e+05     |\n",
      "|    n_updates            | 1350         |\n",
      "|    policy_gradient_loss | -0.0043      |\n",
      "|    std                  | 0.585        |\n",
      "|    value_loss           | 3.39e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-50201.32 +/- 12.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 280000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043280907 |\n",
      "|    clip_fraction        | 0.0501       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.657        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.11e+05     |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.0005      |\n",
      "|    std                  | 0.581        |\n",
      "|    value_loss           | 3.36e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 137       |\n",
      "|    time_elapsed    | 8689      |\n",
      "|    total_timesteps | 280576    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 138          |\n",
      "|    time_elapsed         | 8730         |\n",
      "|    total_timesteps      | 282624       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053574406 |\n",
      "|    clip_fraction        | 0.0528       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.64        |\n",
      "|    explained_variance   | 0.675        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.42e+05     |\n",
      "|    n_updates            | 1370         |\n",
      "|    policy_gradient_loss | -0.00263     |\n",
      "|    std                  | 0.592        |\n",
      "|    value_loss           | 3.34e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 139         |\n",
      "|    time_elapsed         | 8771        |\n",
      "|    total_timesteps      | 284672      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006001947 |\n",
      "|    clip_fraction        | 0.0572      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.65       |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.92e+05    |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0026     |\n",
      "|    std                  | 0.588       |\n",
      "|    value_loss           | 3.32e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=-50195.32 +/- 14.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 285000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005510697 |\n",
      "|    clip_fraction        | 0.0552      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.64       |\n",
      "|    explained_variance   | 0.683       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.37e+05    |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | -0.00154    |\n",
      "|    std                  | 0.586       |\n",
      "|    value_loss           | 3.3e+05     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 140       |\n",
      "|    time_elapsed    | 8867      |\n",
      "|    total_timesteps | 286720    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 141          |\n",
      "|    time_elapsed         | 8907         |\n",
      "|    total_timesteps      | 288768       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066848043 |\n",
      "|    clip_fraction        | 0.0498       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.61        |\n",
      "|    explained_variance   | 0.692        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.86e+05     |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.000716    |\n",
      "|    std                  | 0.58         |\n",
      "|    value_loss           | 3.32e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-50215.80 +/- 7.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 290000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037071826 |\n",
      "|    clip_fraction        | 0.0473       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.6         |\n",
      "|    explained_variance   | 0.706        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.8e+05      |\n",
      "|    n_updates            | 1410         |\n",
      "|    policy_gradient_loss | 0.000687     |\n",
      "|    std                  | 0.581        |\n",
      "|    value_loss           | 3.3e+05      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 142       |\n",
      "|    time_elapsed    | 9003      |\n",
      "|    total_timesteps | 290816    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 143         |\n",
      "|    time_elapsed         | 9045        |\n",
      "|    total_timesteps      | 292864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004993193 |\n",
      "|    clip_fraction        | 0.0747      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.684       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.71e+05    |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.00221    |\n",
      "|    std                  | 0.585       |\n",
      "|    value_loss           | 3.27e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 144          |\n",
      "|    time_elapsed         | 9086         |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067262743 |\n",
      "|    clip_fraction        | 0.0784       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.686        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.35e+05     |\n",
      "|    n_updates            | 1430         |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    std                  | 0.587        |\n",
      "|    value_loss           | 3.27e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=-50203.18 +/- 13.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 295000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063389987 |\n",
      "|    clip_fraction        | 0.0853       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.702        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.83e+05     |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    std                  | 0.585        |\n",
      "|    value_loss           | 3.26e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 145       |\n",
      "|    time_elapsed    | 9182      |\n",
      "|    total_timesteps | 296960    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 146          |\n",
      "|    time_elapsed         | 9224         |\n",
      "|    total_timesteps      | 299008       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063799047 |\n",
      "|    clip_fraction        | 0.0819       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.6         |\n",
      "|    explained_variance   | 0.7          |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.8e+05      |\n",
      "|    n_updates            | 1450         |\n",
      "|    policy_gradient_loss | -0.00313     |\n",
      "|    std                  | 0.58         |\n",
      "|    value_loss           | 3.25e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-50215.81 +/- 18.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 300000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012858777 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.702       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.17e+05    |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | 0.0117      |\n",
      "|    std                  | 0.586       |\n",
      "|    value_loss           | 2.7e+05     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 147       |\n",
      "|    time_elapsed    | 9321      |\n",
      "|    total_timesteps | 301056    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 148         |\n",
      "|    time_elapsed         | 9362        |\n",
      "|    total_timesteps      | 303104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006204268 |\n",
      "|    clip_fraction        | 0.0866      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.732       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.9e+05     |\n",
      "|    n_updates            | 1470        |\n",
      "|    policy_gradient_loss | -0.00458    |\n",
      "|    std                  | 0.582       |\n",
      "|    value_loss           | 3.07e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=305000, episode_reward=-50210.57 +/- 9.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 305000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005102874 |\n",
      "|    clip_fraction        | 0.0428      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.63       |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.23e+05    |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.000196   |\n",
      "|    std                  | 0.589       |\n",
      "|    value_loss           | 3.04e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 149       |\n",
      "|    time_elapsed    | 9458      |\n",
      "|    total_timesteps | 305152    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 150          |\n",
      "|    time_elapsed         | 9500         |\n",
      "|    total_timesteps      | 307200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063467836 |\n",
      "|    clip_fraction        | 0.0709       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.63        |\n",
      "|    explained_variance   | 0.715        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.57e+05     |\n",
      "|    n_updates            | 1490         |\n",
      "|    policy_gradient_loss | -0.00352     |\n",
      "|    std                  | 0.589        |\n",
      "|    value_loss           | 3.01e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 151         |\n",
      "|    time_elapsed         | 9542        |\n",
      "|    total_timesteps      | 309248      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006293649 |\n",
      "|    clip_fraction        | 0.0767      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.63       |\n",
      "|    explained_variance   | 0.706       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.01e+05    |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.00173    |\n",
      "|    std                  | 0.589       |\n",
      "|    value_loss           | 3e+05       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=-50215.47 +/- 3.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 310000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083584385 |\n",
      "|    clip_fraction        | 0.0846       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.61        |\n",
      "|    explained_variance   | 0.696        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.47e+05     |\n",
      "|    n_updates            | 1510         |\n",
      "|    policy_gradient_loss | -0.0035      |\n",
      "|    std                  | 0.585        |\n",
      "|    value_loss           | 2.97e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 152       |\n",
      "|    time_elapsed    | 9638      |\n",
      "|    total_timesteps | 311296    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 153         |\n",
      "|    time_elapsed         | 9679        |\n",
      "|    total_timesteps      | 313344      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006562836 |\n",
      "|    clip_fraction        | 0.07        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.6        |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.47e+05    |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | 0.000359    |\n",
      "|    std                  | 0.582       |\n",
      "|    value_loss           | 3.02e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=315000, episode_reward=-50200.97 +/- 17.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 315000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010987399 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | 0.613       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 9.14e+04    |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | -0.000529   |\n",
      "|    std                  | 0.59        |\n",
      "|    value_loss           | 1.96e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 154       |\n",
      "|    time_elapsed    | 9775      |\n",
      "|    total_timesteps | 315392    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 155         |\n",
      "|    time_elapsed         | 9816        |\n",
      "|    total_timesteps      | 317440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007273874 |\n",
      "|    clip_fraction        | 0.0916      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.64       |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.24e+05    |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.00504    |\n",
      "|    std                  | 0.592       |\n",
      "|    value_loss           | 2.89e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 32         |\n",
      "|    iterations           | 156        |\n",
      "|    time_elapsed         | 9858       |\n",
      "|    total_timesteps      | 319488     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00513783 |\n",
      "|    clip_fraction        | 0.0513     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.64      |\n",
      "|    explained_variance   | 0.717      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.2e+05    |\n",
      "|    n_updates            | 1550       |\n",
      "|    policy_gradient_loss | -0.00145   |\n",
      "|    std                  | 0.591      |\n",
      "|    value_loss           | 2.87e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-50206.51 +/- 7.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 320000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059791002 |\n",
      "|    clip_fraction        | 0.0487       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.64        |\n",
      "|    explained_variance   | 0.773        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.4e+05      |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    std                  | 0.592        |\n",
      "|    value_loss           | 2.87e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 157       |\n",
      "|    time_elapsed    | 9954      |\n",
      "|    total_timesteps | 321536    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 158         |\n",
      "|    time_elapsed         | 9995        |\n",
      "|    total_timesteps      | 323584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007501998 |\n",
      "|    clip_fraction        | 0.0678      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.59e+05    |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | -0.00492    |\n",
      "|    std                  | 0.584       |\n",
      "|    value_loss           | 2.84e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=325000, episode_reward=-50213.60 +/- 11.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 325000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063429954 |\n",
      "|    clip_fraction        | 0.0692       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.722        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.45e+05     |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.00264     |\n",
      "|    std                  | 0.581        |\n",
      "|    value_loss           | 2.82e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 159       |\n",
      "|    time_elapsed    | 10091     |\n",
      "|    total_timesteps | 325632    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 160          |\n",
      "|    time_elapsed         | 10132        |\n",
      "|    total_timesteps      | 327680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063921763 |\n",
      "|    clip_fraction        | 0.0686       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.736        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.19e+05     |\n",
      "|    n_updates            | 1590         |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    std                  | 0.577        |\n",
      "|    value_loss           | 2.8e+05      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 161         |\n",
      "|    time_elapsed         | 10173       |\n",
      "|    total_timesteps      | 329728      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008145827 |\n",
      "|    clip_fraction        | 0.0707      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.57       |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.07e+05    |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0046     |\n",
      "|    std                  | 0.583       |\n",
      "|    value_loss           | 2.79e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=-50212.08 +/- 7.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 330000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00844099 |\n",
      "|    clip_fraction        | 0.0903     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.6       |\n",
      "|    explained_variance   | 0.754      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.63e+05   |\n",
      "|    n_updates            | 1610       |\n",
      "|    policy_gradient_loss | -0.00401   |\n",
      "|    std                  | 0.59       |\n",
      "|    value_loss           | 2.77e+05   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 162       |\n",
      "|    time_elapsed    | 10269     |\n",
      "|    total_timesteps | 331776    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 163         |\n",
      "|    time_elapsed         | 10310       |\n",
      "|    total_timesteps      | 333824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012409475 |\n",
      "|    clip_fraction        | 0.089       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.6        |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.18e+05    |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.00228    |\n",
      "|    std                  | 0.586       |\n",
      "|    value_loss           | 2.76e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=335000, episode_reward=-50210.24 +/- 10.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 335000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062111923 |\n",
      "|    clip_fraction        | 0.0596       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.58        |\n",
      "|    explained_variance   | 0.752        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.02e+05     |\n",
      "|    n_updates            | 1630         |\n",
      "|    policy_gradient_loss | -0.0025      |\n",
      "|    std                  | 0.579        |\n",
      "|    value_loss           | 2.75e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 164       |\n",
      "|    time_elapsed    | 10406     |\n",
      "|    total_timesteps | 335872    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 165          |\n",
      "|    time_elapsed         | 10447        |\n",
      "|    total_timesteps      | 337920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064789946 |\n",
      "|    clip_fraction        | 0.0585       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.55        |\n",
      "|    explained_variance   | 0.755        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 9.53e+04     |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.000373    |\n",
      "|    std                  | 0.575        |\n",
      "|    value_loss           | 2.74e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 166          |\n",
      "|    time_elapsed         | 10488        |\n",
      "|    total_timesteps      | 339968       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057013235 |\n",
      "|    clip_fraction        | 0.0791       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.52        |\n",
      "|    explained_variance   | 0.76         |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 8.75e+04     |\n",
      "|    n_updates            | 1650         |\n",
      "|    policy_gradient_loss | -0.00496     |\n",
      "|    std                  | 0.571        |\n",
      "|    value_loss           | 2.73e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-50195.27 +/- 9.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 340000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061255116 |\n",
      "|    clip_fraction        | 0.0768       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.5         |\n",
      "|    explained_variance   | 0.764        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.72e+05     |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.00334     |\n",
      "|    std                  | 0.569        |\n",
      "|    value_loss           | 2.73e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 167       |\n",
      "|    time_elapsed    | 10583     |\n",
      "|    total_timesteps | 342016    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 168          |\n",
      "|    time_elapsed         | 10625        |\n",
      "|    total_timesteps      | 344064       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068314653 |\n",
      "|    clip_fraction        | 0.0757       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.798        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.09e+05     |\n",
      "|    n_updates            | 1670         |\n",
      "|    policy_gradient_loss | -0.0037      |\n",
      "|    std                  | 0.56         |\n",
      "|    value_loss           | 2.66e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=345000, episode_reward=-50199.23 +/- 11.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 345000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009043653 |\n",
      "|    clip_fraction        | 0.088       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.45       |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.26e+05    |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.00336    |\n",
      "|    std                  | 0.559       |\n",
      "|    value_loss           | 2.59e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 169       |\n",
      "|    time_elapsed    | 10721     |\n",
      "|    total_timesteps | 346112    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 170         |\n",
      "|    time_elapsed         | 10763       |\n",
      "|    total_timesteps      | 348160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009980445 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.43       |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.82e+05    |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | -0.00322    |\n",
      "|    std                  | 0.558       |\n",
      "|    value_loss           | 2.55e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=-50204.84 +/- 7.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 350000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008149625 |\n",
      "|    clip_fraction        | 0.0911      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.43       |\n",
      "|    explained_variance   | 0.768       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.81e+04    |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.00354    |\n",
      "|    std                  | 0.56        |\n",
      "|    value_loss           | 2.52e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 171       |\n",
      "|    time_elapsed    | 10859     |\n",
      "|    total_timesteps | 350208    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 172         |\n",
      "|    time_elapsed         | 10900       |\n",
      "|    total_timesteps      | 352256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008313205 |\n",
      "|    clip_fraction        | 0.0961      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.39       |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.13e+05    |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | -0.00273    |\n",
      "|    std                  | 0.55        |\n",
      "|    value_loss           | 2.52e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 173         |\n",
      "|    time_elapsed         | 10942       |\n",
      "|    total_timesteps      | 354304      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007951643 |\n",
      "|    clip_fraction        | 0.0794      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.35       |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.04e+05    |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.00264    |\n",
      "|    std                  | 0.542       |\n",
      "|    value_loss           | 2.49e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=355000, episode_reward=-50208.27 +/- 6.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 355000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005977057 |\n",
      "|    clip_fraction        | 0.0627      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.34       |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.27e+05    |\n",
      "|    n_updates            | 1730        |\n",
      "|    policy_gradient_loss | -0.0026     |\n",
      "|    std                  | 0.541       |\n",
      "|    value_loss           | 2.54e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 174       |\n",
      "|    time_elapsed    | 11037     |\n",
      "|    total_timesteps | 356352    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 175         |\n",
      "|    time_elapsed         | 11078       |\n",
      "|    total_timesteps      | 358400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011524498 |\n",
      "|    clip_fraction        | 0.0963      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.32       |\n",
      "|    explained_variance   | 0.771       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.1e+05     |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.00387    |\n",
      "|    std                  | 0.539       |\n",
      "|    value_loss           | 2.44e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-50200.65 +/- 13.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 360000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007833243 |\n",
      "|    clip_fraction        | 0.0777      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.29       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.57e+05    |\n",
      "|    n_updates            | 1750        |\n",
      "|    policy_gradient_loss | -0.00118    |\n",
      "|    std                  | 0.539       |\n",
      "|    value_loss           | 2.42e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 176       |\n",
      "|    time_elapsed    | 11173     |\n",
      "|    total_timesteps | 360448    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 177          |\n",
      "|    time_elapsed         | 11215        |\n",
      "|    total_timesteps      | 362496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064682746 |\n",
      "|    clip_fraction        | 0.0826       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.27        |\n",
      "|    explained_variance   | 0.783        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 9.9e+04      |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    std                  | 0.532        |\n",
      "|    value_loss           | 2.4e+05      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 178         |\n",
      "|    time_elapsed         | 11256       |\n",
      "|    total_timesteps      | 364544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009360736 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.26       |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.11e+05    |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | -0.00616    |\n",
      "|    std                  | 0.535       |\n",
      "|    value_loss           | 2.38e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=365000, episode_reward=-50198.99 +/- 19.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 365000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005370749 |\n",
      "|    clip_fraction        | 0.0844      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.28       |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.17e+05    |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.00171    |\n",
      "|    std                  | 0.539       |\n",
      "|    value_loss           | 2.36e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 179       |\n",
      "|    time_elapsed    | 11352     |\n",
      "|    total_timesteps | 366592    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 180          |\n",
      "|    time_elapsed         | 11393        |\n",
      "|    total_timesteps      | 368640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071651246 |\n",
      "|    clip_fraction        | 0.104        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.29        |\n",
      "|    explained_variance   | 0.782        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.1e+05      |\n",
      "|    n_updates            | 1790         |\n",
      "|    policy_gradient_loss | 0.000345     |\n",
      "|    std                  | 0.54         |\n",
      "|    value_loss           | 2.36e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=-50215.65 +/- 3.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 370000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010053923 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.28       |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.76e+04    |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.00707    |\n",
      "|    std                  | 0.534       |\n",
      "|    value_loss           | 2.33e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 181       |\n",
      "|    time_elapsed    | 11488     |\n",
      "|    total_timesteps | 370688    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 182         |\n",
      "|    time_elapsed         | 11530       |\n",
      "|    total_timesteps      | 372736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010779517 |\n",
      "|    clip_fraction        | 0.0935      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.21       |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.27e+05    |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | -0.00317    |\n",
      "|    std                  | 0.52        |\n",
      "|    value_loss           | 2.32e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 183          |\n",
      "|    time_elapsed         | 11571        |\n",
      "|    total_timesteps      | 374784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0094077885 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.18        |\n",
      "|    explained_variance   | 0.799        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.42e+05     |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | 0.000939     |\n",
      "|    std                  | 0.519        |\n",
      "|    value_loss           | 2.41e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=375000, episode_reward=-50208.32 +/- 17.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 375000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007993836 |\n",
      "|    clip_fraction        | 0.0958      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.16       |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.22e+05    |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | -0.00409    |\n",
      "|    std                  | 0.516       |\n",
      "|    value_loss           | 2.3e+05     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 184       |\n",
      "|    time_elapsed    | 11668     |\n",
      "|    total_timesteps | 376832    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 32         |\n",
      "|    iterations           | 185        |\n",
      "|    time_elapsed         | 11710      |\n",
      "|    total_timesteps      | 378880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00867426 |\n",
      "|    clip_fraction        | 0.0757     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.16      |\n",
      "|    explained_variance   | 0.78       |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.02e+05   |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.00194   |\n",
      "|    std                  | 0.519      |\n",
      "|    value_loss           | 2.33e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-50208.75 +/- 13.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 380000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008484349 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.17       |\n",
      "|    explained_variance   | 0.755       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.36e+05    |\n",
      "|    n_updates            | 1850        |\n",
      "|    policy_gradient_loss | -0.000183   |\n",
      "|    std                  | 0.519       |\n",
      "|    value_loss           | 2.31e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 186       |\n",
      "|    time_elapsed    | 11807     |\n",
      "|    total_timesteps | 380928    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 187         |\n",
      "|    time_elapsed         | 11848       |\n",
      "|    total_timesteps      | 382976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015272861 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.15       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.62e+04    |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | 0.0107      |\n",
      "|    std                  | 0.514       |\n",
      "|    value_loss           | 2.15e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=385000, episode_reward=-50208.55 +/- 10.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 385000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009946579 |\n",
      "|    clip_fraction        | 0.079       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.16       |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.06e+05    |\n",
      "|    n_updates            | 1870        |\n",
      "|    policy_gradient_loss | -0.00204    |\n",
      "|    std                  | 0.52        |\n",
      "|    value_loss           | 2.32e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 188       |\n",
      "|    time_elapsed    | 11945     |\n",
      "|    total_timesteps | 385024    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 189         |\n",
      "|    time_elapsed         | 11987       |\n",
      "|    total_timesteps      | 387072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008658767 |\n",
      "|    clip_fraction        | 0.0976      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.15       |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.84e+04    |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.00393    |\n",
      "|    std                  | 0.512       |\n",
      "|    value_loss           | 2.23e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 190          |\n",
      "|    time_elapsed         | 12029        |\n",
      "|    total_timesteps      | 389120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073701655 |\n",
      "|    clip_fraction        | 0.116        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.13        |\n",
      "|    explained_variance   | 0.772        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.15e+05     |\n",
      "|    n_updates            | 1890         |\n",
      "|    policy_gradient_loss | 0.00191      |\n",
      "|    std                  | 0.511        |\n",
      "|    value_loss           | 2.18e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=-50210.36 +/- 15.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 390000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008024133 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.12       |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.04e+05    |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0016     |\n",
      "|    std                  | 0.507       |\n",
      "|    value_loss           | 2.17e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 191       |\n",
      "|    time_elapsed    | 12125     |\n",
      "|    total_timesteps | 391168    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 192         |\n",
      "|    time_elapsed         | 12167       |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006686468 |\n",
      "|    clip_fraction        | 0.0744      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.14       |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.22e+05    |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    std                  | 0.515       |\n",
      "|    value_loss           | 2.17e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=395000, episode_reward=-50204.94 +/- 17.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 395000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008234125 |\n",
      "|    clip_fraction        | 0.0866      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.14       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.02e+05    |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0014     |\n",
      "|    std                  | 0.513       |\n",
      "|    value_loss           | 2.16e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 193       |\n",
      "|    time_elapsed    | 12264     |\n",
      "|    total_timesteps | 395264    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 194          |\n",
      "|    time_elapsed         | 12306        |\n",
      "|    total_timesteps      | 397312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073905527 |\n",
      "|    clip_fraction        | 0.0835       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.13        |\n",
      "|    explained_variance   | 0.814        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1e+05        |\n",
      "|    n_updates            | 1930         |\n",
      "|    policy_gradient_loss | -0.00321     |\n",
      "|    std                  | 0.508        |\n",
      "|    value_loss           | 2.14e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 195         |\n",
      "|    time_elapsed         | 12347       |\n",
      "|    total_timesteps      | 399360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005575249 |\n",
      "|    clip_fraction        | 0.0534      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.12       |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.88e+04    |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.00244    |\n",
      "|    std                  | 0.51        |\n",
      "|    value_loss           | 2.21e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-50203.18 +/- 16.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 400000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072118016 |\n",
      "|    clip_fraction        | 0.0936       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.11        |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 9.33e+04     |\n",
      "|    n_updates            | 1950         |\n",
      "|    policy_gradient_loss | -0.00255     |\n",
      "|    std                  | 0.505        |\n",
      "|    value_loss           | 2.09e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 196       |\n",
      "|    time_elapsed    | 12445     |\n",
      "|    total_timesteps | 401408    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 32         |\n",
      "|    iterations           | 197        |\n",
      "|    time_elapsed         | 12487      |\n",
      "|    total_timesteps      | 403456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00908093 |\n",
      "|    clip_fraction        | 0.102      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.07      |\n",
      "|    explained_variance   | 0.821      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 7.51e+04   |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.00428   |\n",
      "|    std                  | 0.497      |\n",
      "|    value_loss           | 2.07e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=405000, episode_reward=-50212.49 +/- 10.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 405000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009261074 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.06       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.15e+05    |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | -0.00385    |\n",
      "|    std                  | 0.498       |\n",
      "|    value_loss           | 2.06e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 198       |\n",
      "|    time_elapsed    | 12585     |\n",
      "|    total_timesteps | 405504    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 199         |\n",
      "|    time_elapsed         | 12628       |\n",
      "|    total_timesteps      | 407552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009217624 |\n",
      "|    clip_fraction        | 0.099       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.06       |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.05e+05    |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0038     |\n",
      "|    std                  | 0.494       |\n",
      "|    value_loss           | 2.05e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 200         |\n",
      "|    time_elapsed         | 12670       |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008034512 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.04       |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.01e+05    |\n",
      "|    n_updates            | 1990        |\n",
      "|    policy_gradient_loss | -0.00325    |\n",
      "|    std                  | 0.491       |\n",
      "|    value_loss           | 2.03e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=-50212.05 +/- 10.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 410000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010750765 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.04       |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.26e+05    |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.00119    |\n",
      "|    std                  | 0.491       |\n",
      "|    value_loss           | 2.06e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 201       |\n",
      "|    time_elapsed    | 12766     |\n",
      "|    total_timesteps | 411648    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 202         |\n",
      "|    time_elapsed         | 12808       |\n",
      "|    total_timesteps      | 413696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022649229 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.04       |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.53e+04    |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | 0.0116      |\n",
      "|    std                  | 0.491       |\n",
      "|    value_loss           | 1.64e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=415000, episode_reward=-50219.43 +/- 6.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 415000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013213655 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.01       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.3e+04     |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.004      |\n",
      "|    std                  | 0.48        |\n",
      "|    value_loss           | 2.03e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 203       |\n",
      "|    time_elapsed    | 12905     |\n",
      "|    total_timesteps | 415744    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 204         |\n",
      "|    time_elapsed         | 12948       |\n",
      "|    total_timesteps      | 417792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011575615 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.95       |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.88e+04    |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | -0.00159    |\n",
      "|    std                  | 0.471       |\n",
      "|    value_loss           | 2.01e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 205          |\n",
      "|    time_elapsed         | 12990        |\n",
      "|    total_timesteps      | 419840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069216117 |\n",
      "|    clip_fraction        | 0.106        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.92        |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.01e+05     |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.00384     |\n",
      "|    std                  | 0.472        |\n",
      "|    value_loss           | 2.02e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-50204.97 +/- 7.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 420000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090736635 |\n",
      "|    clip_fraction        | 0.085        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.93        |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.16e+05     |\n",
      "|    n_updates            | 2050         |\n",
      "|    policy_gradient_loss | -0.00165     |\n",
      "|    std                  | 0.472        |\n",
      "|    value_loss           | 2.04e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 206       |\n",
      "|    time_elapsed    | 13088     |\n",
      "|    total_timesteps | 421888    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 207         |\n",
      "|    time_elapsed         | 13131       |\n",
      "|    total_timesteps      | 423936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007923761 |\n",
      "|    clip_fraction        | 0.0958      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 9.27e+04    |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.00474    |\n",
      "|    std                  | 0.474       |\n",
      "|    value_loss           | 2.01e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=425000, episode_reward=-50209.74 +/- 7.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 425000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011150313 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.96       |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.06e+05    |\n",
      "|    n_updates            | 2070        |\n",
      "|    policy_gradient_loss | 0.00154     |\n",
      "|    std                  | 0.477       |\n",
      "|    value_loss           | 1.95e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 208       |\n",
      "|    time_elapsed    | 13227     |\n",
      "|    total_timesteps | 425984    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 209         |\n",
      "|    time_elapsed         | 13270       |\n",
      "|    total_timesteps      | 428032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009407477 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.95       |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.09e+04    |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0033     |\n",
      "|    std                  | 0.471       |\n",
      "|    value_loss           | 2.04e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=-50213.88 +/- 7.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 430000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008933788 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.95       |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.44e+05    |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | -0.00243    |\n",
      "|    std                  | 0.475       |\n",
      "|    value_loss           | 1.94e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 210       |\n",
      "|    time_elapsed    | 13368     |\n",
      "|    total_timesteps | 430080    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 211         |\n",
      "|    time_elapsed         | 13410       |\n",
      "|    total_timesteps      | 432128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008081406 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.16e+05    |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.00292    |\n",
      "|    std                  | 0.47        |\n",
      "|    value_loss           | 1.92e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 212         |\n",
      "|    time_elapsed         | 13453       |\n",
      "|    total_timesteps      | 434176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007055591 |\n",
      "|    clip_fraction        | 0.0932      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.9        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.98e+04    |\n",
      "|    n_updates            | 2110        |\n",
      "|    policy_gradient_loss | -0.0043     |\n",
      "|    std                  | 0.465       |\n",
      "|    value_loss           | 1.89e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=435000, episode_reward=-50213.71 +/- 12.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 435000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010480416 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.86       |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 9.25e+04    |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.00476    |\n",
      "|    std                  | 0.457       |\n",
      "|    value_loss           | 1.85e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 213       |\n",
      "|    time_elapsed    | 13549     |\n",
      "|    total_timesteps | 436224    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 214          |\n",
      "|    time_elapsed         | 13593        |\n",
      "|    total_timesteps      | 438272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0079808645 |\n",
      "|    clip_fraction        | 0.0936       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.84        |\n",
      "|    explained_variance   | 0.844        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 8.58e+04     |\n",
      "|    n_updates            | 2130         |\n",
      "|    policy_gradient_loss | -2.2e-06     |\n",
      "|    std                  | 0.457        |\n",
      "|    value_loss           | 1.88e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-50215.90 +/- 6.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 440000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008706508 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.85       |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.93e+04    |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.00453    |\n",
      "|    std                  | 0.461       |\n",
      "|    value_loss           | 1.89e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 215       |\n",
      "|    time_elapsed    | 13690     |\n",
      "|    total_timesteps | 440320    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 216         |\n",
      "|    time_elapsed         | 13732       |\n",
      "|    total_timesteps      | 442368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007822694 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.84       |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 9.75e+04    |\n",
      "|    n_updates            | 2150        |\n",
      "|    policy_gradient_loss | -0.00474    |\n",
      "|    std                  | 0.455       |\n",
      "|    value_loss           | 1.84e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 217         |\n",
      "|    time_elapsed         | 13776       |\n",
      "|    total_timesteps      | 444416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012095815 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.82       |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.76e+04    |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.00332    |\n",
      "|    std                  | 0.454       |\n",
      "|    value_loss           | 1.81e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=445000, episode_reward=-50210.56 +/- 12.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 445000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011038378 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.82       |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.87e+04    |\n",
      "|    n_updates            | 2170        |\n",
      "|    policy_gradient_loss | -0.00348    |\n",
      "|    std                  | 0.453       |\n",
      "|    value_loss           | 1.8e+05     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 218       |\n",
      "|    time_elapsed    | 13873     |\n",
      "|    total_timesteps | 446464    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 219         |\n",
      "|    time_elapsed         | 13916       |\n",
      "|    total_timesteps      | 448512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011824038 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.8        |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.98e+04    |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0049     |\n",
      "|    std                  | 0.452       |\n",
      "|    value_loss           | 1.79e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=-50217.88 +/- 14.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 450000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009688577 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.81       |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.09e+05    |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | -0.00179    |\n",
      "|    std                  | 0.457       |\n",
      "|    value_loss           | 1.8e+05     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 220       |\n",
      "|    time_elapsed    | 14013     |\n",
      "|    total_timesteps | 450560    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 221         |\n",
      "|    time_elapsed         | 14056       |\n",
      "|    total_timesteps      | 452608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009353217 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.84       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.27e+04    |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.000886   |\n",
      "|    std                  | 0.461       |\n",
      "|    value_loss           | 1.77e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 222         |\n",
      "|    time_elapsed         | 14099       |\n",
      "|    total_timesteps      | 454656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010054689 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.83       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.59e+04    |\n",
      "|    n_updates            | 2210        |\n",
      "|    policy_gradient_loss | -0.00306    |\n",
      "|    std                  | 0.457       |\n",
      "|    value_loss           | 1.77e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=455000, episode_reward=-50206.61 +/- 3.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 455000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01264411 |\n",
      "|    clip_fraction        | 0.147      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.8       |\n",
      "|    explained_variance   | 0.864      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 9.74e+04   |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.005     |\n",
      "|    std                  | 0.451      |\n",
      "|    value_loss           | 1.76e+05   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 223       |\n",
      "|    time_elapsed    | 14196     |\n",
      "|    total_timesteps | 456704    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 224         |\n",
      "|    time_elapsed         | 14239       |\n",
      "|    total_timesteps      | 458752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012256077 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.79       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.58e+04    |\n",
      "|    n_updates            | 2230        |\n",
      "|    policy_gradient_loss | -0.00459    |\n",
      "|    std                  | 0.451       |\n",
      "|    value_loss           | 1.76e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-50210.04 +/- 10.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 460000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01027376 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.81      |\n",
      "|    explained_variance   | 0.867      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 8.43e+04   |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.00368   |\n",
      "|    std                  | 0.455      |\n",
      "|    value_loss           | 1.75e+05   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 225       |\n",
      "|    time_elapsed    | 14336     |\n",
      "|    total_timesteps | 460800    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 226         |\n",
      "|    time_elapsed         | 14379       |\n",
      "|    total_timesteps      | 462848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009137537 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.96e+04    |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | -0.00371    |\n",
      "|    std                  | 0.451       |\n",
      "|    value_loss           | 1.75e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 227         |\n",
      "|    time_elapsed         | 14421       |\n",
      "|    total_timesteps      | 464896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010555098 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.37e+05    |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.00504    |\n",
      "|    std                  | 0.448       |\n",
      "|    value_loss           | 1.75e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=465000, episode_reward=-50199.03 +/- 13.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 465000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008847916 |\n",
      "|    clip_fraction        | 0.0976      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.75       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 9.26e+04    |\n",
      "|    n_updates            | 2270        |\n",
      "|    policy_gradient_loss | -0.00146    |\n",
      "|    std                  | 0.443       |\n",
      "|    value_loss           | 1.76e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 228       |\n",
      "|    time_elapsed    | 14518     |\n",
      "|    total_timesteps | 466944    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 229         |\n",
      "|    time_elapsed         | 14561       |\n",
      "|    total_timesteps      | 468992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009362459 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.74       |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.25e+04    |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.00274    |\n",
      "|    std                  | 0.444       |\n",
      "|    value_loss           | 1.77e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=-50191.95 +/- 12.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 470000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009206517 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.62e+04    |\n",
      "|    n_updates            | 2290        |\n",
      "|    policy_gradient_loss | -0.00251    |\n",
      "|    std                  | 0.441       |\n",
      "|    value_loss           | 1.77e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 230       |\n",
      "|    time_elapsed    | 14659     |\n",
      "|    total_timesteps | 471040    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 231         |\n",
      "|    time_elapsed         | 14702       |\n",
      "|    total_timesteps      | 473088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010132934 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.26e+04    |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | 0.00274     |\n",
      "|    std                  | 0.44        |\n",
      "|    value_loss           | 1.65e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=475000, episode_reward=-50213.37 +/- 9.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 475000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010727951 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.72       |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.85e+04    |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | -0.00287    |\n",
      "|    std                  | 0.44        |\n",
      "|    value_loss           | 1.67e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 232       |\n",
      "|    time_elapsed    | 14798     |\n",
      "|    total_timesteps | 475136    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 233         |\n",
      "|    time_elapsed         | 14841       |\n",
      "|    total_timesteps      | 477184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011026319 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.71       |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.88e+04    |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.00242    |\n",
      "|    std                  | 0.436       |\n",
      "|    value_loss           | 1.67e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 234         |\n",
      "|    time_elapsed         | 14883       |\n",
      "|    total_timesteps      | 479232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009689901 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.69       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.16e+04    |\n",
      "|    n_updates            | 2330        |\n",
      "|    policy_gradient_loss | -0.00105    |\n",
      "|    std                  | 0.433       |\n",
      "|    value_loss           | 1.66e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-50208.26 +/- 10.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 480000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009874884 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.64       |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.09e+05    |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.00416    |\n",
      "|    std                  | 0.423       |\n",
      "|    value_loss           | 1.63e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 235       |\n",
      "|    time_elapsed    | 14981     |\n",
      "|    total_timesteps | 481280    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 236         |\n",
      "|    time_elapsed         | 15024       |\n",
      "|    total_timesteps      | 483328      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011733988 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 9.64e+04    |\n",
      "|    n_updates            | 2350        |\n",
      "|    policy_gradient_loss | -0.00237    |\n",
      "|    std                  | 0.423       |\n",
      "|    value_loss           | 1.69e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=485000, episode_reward=-50210.31 +/- 6.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 485000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013819177 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.77e+04    |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.00492    |\n",
      "|    std                  | 0.417       |\n",
      "|    value_loss           | 1.62e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 237       |\n",
      "|    time_elapsed    | 15121     |\n",
      "|    total_timesteps | 485376    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 238         |\n",
      "|    time_elapsed         | 15164       |\n",
      "|    total_timesteps      | 487424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007554779 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.81e+04    |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | -0.00219    |\n",
      "|    std                  | 0.419       |\n",
      "|    value_loss           | 1.61e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 239         |\n",
      "|    time_elapsed         | 15206       |\n",
      "|    total_timesteps      | 489472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008914919 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.22e+05    |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.000935   |\n",
      "|    std                  | 0.417       |\n",
      "|    value_loss           | 1.61e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=-50202.67 +/- 9.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 490000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013945809 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.88e+04    |\n",
      "|    n_updates            | 2390        |\n",
      "|    policy_gradient_loss | -0.00402    |\n",
      "|    std                  | 0.41        |\n",
      "|    value_loss           | 1.6e+05     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 240       |\n",
      "|    time_elapsed    | 15303     |\n",
      "|    total_timesteps | 491520    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 241         |\n",
      "|    time_elapsed         | 15345       |\n",
      "|    total_timesteps      | 493568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008830186 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.52e+04    |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.00327    |\n",
      "|    std                  | 0.41        |\n",
      "|    value_loss           | 1.59e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=495000, episode_reward=-50215.74 +/- 10.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 495000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013087208 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.07e+04    |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | -0.00496    |\n",
      "|    std                  | 0.411       |\n",
      "|    value_loss           | 1.58e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 242       |\n",
      "|    time_elapsed    | 15443     |\n",
      "|    total_timesteps | 495616    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 243         |\n",
      "|    time_elapsed         | 15485       |\n",
      "|    total_timesteps      | 497664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012211252 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.04e+04    |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.00254    |\n",
      "|    std                  | 0.406       |\n",
      "|    value_loss           | 1.58e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 244         |\n",
      "|    time_elapsed         | 15528       |\n",
      "|    total_timesteps      | 499712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013223389 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.44e+04    |\n",
      "|    n_updates            | 2430        |\n",
      "|    policy_gradient_loss | -0.00245    |\n",
      "|    std                  | 0.404       |\n",
      "|    value_loss           | 1.58e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-50210.02 +/- 3.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 500000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012089543 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.21e+04    |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.00403    |\n",
      "|    std                  | 0.41        |\n",
      "|    value_loss           | 1.57e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 245       |\n",
      "|    time_elapsed    | 15624     |\n",
      "|    total_timesteps | 501760    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 246         |\n",
      "|    time_elapsed         | 15666       |\n",
      "|    total_timesteps      | 503808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012515817 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 9.48e+04    |\n",
      "|    n_updates            | 2450        |\n",
      "|    policy_gradient_loss | -0.00148    |\n",
      "|    std                  | 0.414       |\n",
      "|    value_loss           | 1.57e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=505000, episode_reward=-50204.37 +/- 9.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 505000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012555782 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.76e+04    |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.000918   |\n",
      "|    std                  | 0.414       |\n",
      "|    value_loss           | 1.57e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 247       |\n",
      "|    time_elapsed    | 15763     |\n",
      "|    total_timesteps | 505856    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 248         |\n",
      "|    time_elapsed         | 15805       |\n",
      "|    total_timesteps      | 507904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008789505 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.24e+04    |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | -0.000942   |\n",
      "|    std                  | 0.416       |\n",
      "|    value_loss           | 1.58e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 249         |\n",
      "|    time_elapsed         | 15848       |\n",
      "|    total_timesteps      | 509952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019863669 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.84e+04    |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | 0.00425     |\n",
      "|    std                  | 0.413       |\n",
      "|    value_loss           | 1.38e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=-50204.69 +/- 9.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 510000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010543769 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.3e+04     |\n",
      "|    n_updates            | 2490        |\n",
      "|    policy_gradient_loss | -0.000847   |\n",
      "|    std                  | 0.413       |\n",
      "|    value_loss           | 1.26e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 250       |\n",
      "|    time_elapsed    | 15945     |\n",
      "|    total_timesteps | 512000    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 251         |\n",
      "|    time_elapsed         | 15988       |\n",
      "|    total_timesteps      | 514048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012416378 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.26e+04    |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.00456    |\n",
      "|    std                  | 0.413       |\n",
      "|    value_loss           | 1.26e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=515000, episode_reward=-50213.86 +/- 15.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 515000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009588056 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.57e+04    |\n",
      "|    n_updates            | 2510        |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    std                  | 0.412       |\n",
      "|    value_loss           | 1.15e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 252       |\n",
      "|    time_elapsed    | 16084     |\n",
      "|    total_timesteps | 516096    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 253         |\n",
      "|    time_elapsed         | 16127       |\n",
      "|    total_timesteps      | 518144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012244896 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.9e+04     |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.000534   |\n",
      "|    std                  | 0.408       |\n",
      "|    value_loss           | 1.15e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-50204.71 +/- 16.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 520000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012292283 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.24e+04    |\n",
      "|    n_updates            | 2530        |\n",
      "|    policy_gradient_loss | -0.00621    |\n",
      "|    std                  | 0.407       |\n",
      "|    value_loss           | 1.13e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 254       |\n",
      "|    time_elapsed    | 16225     |\n",
      "|    total_timesteps | 520192    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 255         |\n",
      "|    time_elapsed         | 16267       |\n",
      "|    total_timesteps      | 522240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014892388 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.84e+04    |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.00663    |\n",
      "|    std                  | 0.408       |\n",
      "|    value_loss           | 1.12e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 256          |\n",
      "|    time_elapsed         | 16310        |\n",
      "|    total_timesteps      | 524288       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0126811545 |\n",
      "|    clip_fraction        | 0.13         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | 0.948        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 4.92e+04     |\n",
      "|    n_updates            | 2550         |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    std                  | 0.411        |\n",
      "|    value_loss           | 1.12e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=525000, episode_reward=-50208.41 +/- 8.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 525000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013534103 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.61e+04    |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.00608    |\n",
      "|    std                  | 0.41        |\n",
      "|    value_loss           | 1.11e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 257       |\n",
      "|    time_elapsed    | 16407     |\n",
      "|    total_timesteps | 526336    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 258         |\n",
      "|    time_elapsed         | 16450       |\n",
      "|    total_timesteps      | 528384      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011132027 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.87e+04    |\n",
      "|    n_updates            | 2570        |\n",
      "|    policy_gradient_loss | -0.0066     |\n",
      "|    std                  | 0.411       |\n",
      "|    value_loss           | 1.07e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=-50200.82 +/- 6.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 530000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012804812 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.67e+04    |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.00709    |\n",
      "|    std                  | 0.407       |\n",
      "|    value_loss           | 1.05e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 259       |\n",
      "|    time_elapsed    | 16547     |\n",
      "|    total_timesteps | 530432    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 260         |\n",
      "|    time_elapsed         | 16591       |\n",
      "|    total_timesteps      | 532480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009175954 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.96e+04    |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | -0.00413    |\n",
      "|    std                  | 0.406       |\n",
      "|    value_loss           | 1.14e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 261          |\n",
      "|    time_elapsed         | 16634        |\n",
      "|    total_timesteps      | 534528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102777835 |\n",
      "|    clip_fraction        | 0.127        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.13e+04     |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | 0.00599      |\n",
      "|    std                  | 0.407        |\n",
      "|    value_loss           | 7.03e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=535000, episode_reward=-50200.98 +/- 6.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 535000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015194483 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.98e+04    |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | -0.00196    |\n",
      "|    std                  | 0.4         |\n",
      "|    value_loss           | 1.03e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 262       |\n",
      "|    time_elapsed    | 16732     |\n",
      "|    total_timesteps | 536576    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 32         |\n",
      "|    iterations           | 263        |\n",
      "|    time_elapsed         | 16774      |\n",
      "|    total_timesteps      | 538624     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01212916 |\n",
      "|    clip_fraction        | 0.163      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.44      |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 5.16e+04   |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.00483   |\n",
      "|    std                  | 0.402      |\n",
      "|    value_loss           | 1.02e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-50210.42 +/- 13.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 540000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011704458 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.9e+04     |\n",
      "|    n_updates            | 2630        |\n",
      "|    policy_gradient_loss | -0.00333    |\n",
      "|    std                  | 0.4         |\n",
      "|    value_loss           | 1.05e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 264       |\n",
      "|    time_elapsed    | 16871     |\n",
      "|    total_timesteps | 540672    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 265         |\n",
      "|    time_elapsed         | 16915       |\n",
      "|    total_timesteps      | 542720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012193395 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.37e+04    |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    std                  | 0.403       |\n",
      "|    value_loss           | 1.01e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 266         |\n",
      "|    time_elapsed         | 16958       |\n",
      "|    total_timesteps      | 544768      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010550132 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.47e+04    |\n",
      "|    n_updates            | 2650        |\n",
      "|    policy_gradient_loss | -0.00471    |\n",
      "|    std                  | 0.396       |\n",
      "|    value_loss           | 9.96e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=545000, episode_reward=-50200.24 +/- 14.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 545000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010294501 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.95e+04    |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.00537    |\n",
      "|    std                  | 0.393       |\n",
      "|    value_loss           | 9.87e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 267       |\n",
      "|    time_elapsed    | 17056     |\n",
      "|    total_timesteps | 546816    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 268         |\n",
      "|    time_elapsed         | 17099       |\n",
      "|    total_timesteps      | 548864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010931824 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.31e+04    |\n",
      "|    n_updates            | 2670        |\n",
      "|    policy_gradient_loss | -0.00555    |\n",
      "|    std                  | 0.393       |\n",
      "|    value_loss           | 1.01e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=-50198.92 +/- 18.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 550000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015355364 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.87e+04    |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    std                  | 0.392       |\n",
      "|    value_loss           | 9.86e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 269       |\n",
      "|    time_elapsed    | 17197     |\n",
      "|    total_timesteps | 550912    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 32         |\n",
      "|    iterations           | 270        |\n",
      "|    time_elapsed         | 17240      |\n",
      "|    total_timesteps      | 552960     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01209229 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.36      |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 3.26e+04   |\n",
      "|    n_updates            | 2690       |\n",
      "|    policy_gradient_loss | -0.00434   |\n",
      "|    std                  | 0.393      |\n",
      "|    value_loss           | 9.83e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=555000, episode_reward=-50212.26 +/- 4.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 555000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010535765 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.39e+04    |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.00239    |\n",
      "|    std                  | 0.385       |\n",
      "|    value_loss           | 9.79e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 271       |\n",
      "|    time_elapsed    | 17337     |\n",
      "|    total_timesteps | 555008    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 272         |\n",
      "|    time_elapsed         | 17380       |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014821881 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.6e+04     |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | -0.00363    |\n",
      "|    std                  | 0.385       |\n",
      "|    value_loss           | 9.93e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 273         |\n",
      "|    time_elapsed         | 17422       |\n",
      "|    total_timesteps      | 559104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012767874 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.85e+04    |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | 0.000367    |\n",
      "|    std                  | 0.384       |\n",
      "|    value_loss           | 9.37e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-50214.07 +/- 7.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 560000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014930654 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.46e+04    |\n",
      "|    n_updates            | 2730        |\n",
      "|    policy_gradient_loss | -0.00157    |\n",
      "|    std                  | 0.383       |\n",
      "|    value_loss           | 9e+04       |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 274       |\n",
      "|    time_elapsed    | 17520     |\n",
      "|    total_timesteps | 561152    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 275         |\n",
      "|    time_elapsed         | 17563       |\n",
      "|    total_timesteps      | 563200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015902577 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.69e+04    |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.00889    |\n",
      "|    std                  | 0.388       |\n",
      "|    value_loss           | 8.9e+04     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=565000, episode_reward=-50208.35 +/- 10.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 565000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0114662815 |\n",
      "|    clip_fraction        | 0.15         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 0.943        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 4.41e+04     |\n",
      "|    n_updates            | 2750         |\n",
      "|    policy_gradient_loss | 0.0054       |\n",
      "|    std                  | 0.387        |\n",
      "|    value_loss           | 1.05e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 276       |\n",
      "|    time_elapsed    | 17661     |\n",
      "|    total_timesteps | 565248    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 277         |\n",
      "|    time_elapsed         | 17704       |\n",
      "|    total_timesteps      | 567296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009894473 |\n",
      "|    clip_fraction        | 0.0797      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.34e+04    |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | 0.000624    |\n",
      "|    std                  | 0.387       |\n",
      "|    value_loss           | 9.83e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 278          |\n",
      "|    time_elapsed         | 17746        |\n",
      "|    total_timesteps      | 569344       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057588825 |\n",
      "|    clip_fraction        | 0.0707       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.945        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.27e+04     |\n",
      "|    n_updates            | 2770         |\n",
      "|    policy_gradient_loss | -0.000481    |\n",
      "|    std                  | 0.387        |\n",
      "|    value_loss           | 8.06e+04     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=-50211.87 +/- 7.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 570000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014218783 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.36e+04    |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.00382    |\n",
      "|    std                  | 0.383       |\n",
      "|    value_loss           | 8.53e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 279       |\n",
      "|    time_elapsed    | 17844     |\n",
      "|    total_timesteps | 571392    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 280         |\n",
      "|    time_elapsed         | 17888       |\n",
      "|    total_timesteps      | 573440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010170196 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.58e+04    |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | -0.00307    |\n",
      "|    std                  | 0.383       |\n",
      "|    value_loss           | 8.48e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=575000, episode_reward=-50199.06 +/- 10.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 575000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011821708 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.29e+04    |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0027     |\n",
      "|    std                  | 0.383       |\n",
      "|    value_loss           | 8.42e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 281       |\n",
      "|    time_elapsed    | 17986     |\n",
      "|    total_timesteps | 575488    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 282          |\n",
      "|    time_elapsed         | 18028        |\n",
      "|    total_timesteps      | 577536       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0135448305 |\n",
      "|    clip_fraction        | 0.174        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 0.975        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 4.51e+04     |\n",
      "|    n_updates            | 2810         |\n",
      "|    policy_gradient_loss | -0.00305     |\n",
      "|    std                  | 0.38         |\n",
      "|    value_loss           | 8.32e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 283         |\n",
      "|    time_elapsed         | 18071       |\n",
      "|    total_timesteps      | 579584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012727374 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.44e+04    |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    std                  | 0.377       |\n",
      "|    value_loss           | 8.19e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-50201.22 +/- 7.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 580000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014979268 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.52e+04    |\n",
      "|    n_updates            | 2830        |\n",
      "|    policy_gradient_loss | -0.00073    |\n",
      "|    std                  | 0.375       |\n",
      "|    value_loss           | 8.35e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 284       |\n",
      "|    time_elapsed    | 18170     |\n",
      "|    total_timesteps | 581632    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 285         |\n",
      "|    time_elapsed         | 18213       |\n",
      "|    total_timesteps      | 583680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014154441 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.15e+04    |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.00349    |\n",
      "|    std                  | 0.377       |\n",
      "|    value_loss           | 8.02e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=585000, episode_reward=-50215.89 +/- 10.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 585000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012542648 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.74e+04    |\n",
      "|    n_updates            | 2850        |\n",
      "|    policy_gradient_loss | -0.00327    |\n",
      "|    std                  | 0.374       |\n",
      "|    value_loss           | 8.06e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 286       |\n",
      "|    time_elapsed    | 18311     |\n",
      "|    total_timesteps | 585728    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 32         |\n",
      "|    iterations           | 287        |\n",
      "|    time_elapsed         | 18353      |\n",
      "|    total_timesteps      | 587776     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01335671 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.18      |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 3.55e+04   |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.00653   |\n",
      "|    std                  | 0.366      |\n",
      "|    value_loss           | 7.85e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 288         |\n",
      "|    time_elapsed         | 18397       |\n",
      "|    total_timesteps      | 589824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018994445 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.63e+04    |\n",
      "|    n_updates            | 2870        |\n",
      "|    policy_gradient_loss | -0.006      |\n",
      "|    std                  | 0.369       |\n",
      "|    value_loss           | 7.89e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=-50213.68 +/- 14.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 590000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016773382 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.09e+04    |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.00948    |\n",
      "|    std                  | 0.366       |\n",
      "|    value_loss           | 7.82e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 32        |\n",
      "|    iterations      | 289       |\n",
      "|    time_elapsed    | 18494     |\n",
      "|    total_timesteps | 591872    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 290         |\n",
      "|    time_elapsed         | 18537       |\n",
      "|    total_timesteps      | 593920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012796211 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.8e+04     |\n",
      "|    n_updates            | 2890        |\n",
      "|    policy_gradient_loss | -0.00227    |\n",
      "|    std                  | 0.368       |\n",
      "|    value_loss           | 7.78e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=595000, episode_reward=-50197.71 +/- 14.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 595000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016564514 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.47e+04    |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.00455    |\n",
      "|    std                  | 0.368       |\n",
      "|    value_loss           | 7.77e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 291       |\n",
      "|    time_elapsed    | 18637     |\n",
      "|    total_timesteps | 595968    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 292         |\n",
      "|    time_elapsed         | 18679       |\n",
      "|    total_timesteps      | 598016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014695032 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.22e+04    |\n",
      "|    n_updates            | 2910        |\n",
      "|    policy_gradient_loss | -0.00632    |\n",
      "|    std                  | 0.37        |\n",
      "|    value_loss           | 7.77e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-50213.91 +/- 8.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 600000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03329388 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.16      |\n",
      "|    explained_variance   | 0.837      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 2.79e+04   |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | 0.0175     |\n",
      "|    std                  | 0.37       |\n",
      "|    value_loss           | 1.39e+05   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 293       |\n",
      "|    time_elapsed    | 18777     |\n",
      "|    total_timesteps | 600064    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 294         |\n",
      "|    time_elapsed         | 18820       |\n",
      "|    total_timesteps      | 602112      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015025197 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.54e+04    |\n",
      "|    n_updates            | 2930        |\n",
      "|    policy_gradient_loss | -0.00613    |\n",
      "|    std                  | 0.371       |\n",
      "|    value_loss           | 7.29e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 295         |\n",
      "|    time_elapsed         | 18863       |\n",
      "|    total_timesteps      | 604160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015806831 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.3e+04     |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.00642    |\n",
      "|    std                  | 0.373       |\n",
      "|    value_loss           | 7.05e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=605000, episode_reward=-50206.57 +/- 6.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 605000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013851853 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.86e+04    |\n",
      "|    n_updates            | 2950        |\n",
      "|    policy_gradient_loss | -0.00824    |\n",
      "|    std                  | 0.371       |\n",
      "|    value_loss           | 6.97e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 296       |\n",
      "|    time_elapsed    | 18961     |\n",
      "|    total_timesteps | 606208    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 297         |\n",
      "|    time_elapsed         | 19004       |\n",
      "|    total_timesteps      | 608256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017676437 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.55e+04    |\n",
      "|    n_updates            | 2960        |\n",
      "|    policy_gradient_loss | -0.0033     |\n",
      "|    std                  | 0.372       |\n",
      "|    value_loss           | 6.97e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=-50213.84 +/- 9.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 610000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009446306 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.05e+04    |\n",
      "|    n_updates            | 2970        |\n",
      "|    policy_gradient_loss | 0.00519     |\n",
      "|    std                  | 0.371       |\n",
      "|    value_loss           | 9.04e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 298       |\n",
      "|    time_elapsed    | 19101     |\n",
      "|    total_timesteps | 610304    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 299         |\n",
      "|    time_elapsed         | 19144       |\n",
      "|    total_timesteps      | 612352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014462147 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.88e+04    |\n",
      "|    n_updates            | 2980        |\n",
      "|    policy_gradient_loss | -0.00439    |\n",
      "|    std                  | 0.375       |\n",
      "|    value_loss           | 6.76e+04    |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 32        |\n",
      "|    iterations           | 300       |\n",
      "|    time_elapsed         | 19187     |\n",
      "|    total_timesteps      | 614400    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.017468  |\n",
      "|    clip_fraction        | 0.185     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.17     |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 2.29e+04  |\n",
      "|    n_updates            | 2990      |\n",
      "|    policy_gradient_loss | -0.00657  |\n",
      "|    std                  | 0.368     |\n",
      "|    value_loss           | 6.65e+04  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=615000, episode_reward=-50202.49 +/- 9.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 615000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016555432 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.77e+04    |\n",
      "|    n_updates            | 3000        |\n",
      "|    policy_gradient_loss | 0.000994    |\n",
      "|    std                  | 0.369       |\n",
      "|    value_loss           | 6.97e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 301       |\n",
      "|    time_elapsed    | 19284     |\n",
      "|    total_timesteps | 616448    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 302         |\n",
      "|    time_elapsed         | 19327       |\n",
      "|    total_timesteps      | 618496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015774045 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.7e+04     |\n",
      "|    n_updates            | 3010        |\n",
      "|    policy_gradient_loss | -0.00266    |\n",
      "|    std                  | 0.371       |\n",
      "|    value_loss           | 6.55e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-50206.62 +/- 12.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 620000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014012664 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.79e+04    |\n",
      "|    n_updates            | 3020        |\n",
      "|    policy_gradient_loss | -0.00634    |\n",
      "|    std                  | 0.372       |\n",
      "|    value_loss           | 6.41e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 303       |\n",
      "|    time_elapsed    | 19424     |\n",
      "|    total_timesteps | 620544    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 304         |\n",
      "|    time_elapsed         | 19467       |\n",
      "|    total_timesteps      | 622592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015694069 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.82e+04    |\n",
      "|    n_updates            | 3030        |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    std                  | 0.372       |\n",
      "|    value_loss           | 6.33e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 305         |\n",
      "|    time_elapsed         | 19510       |\n",
      "|    total_timesteps      | 624640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015481159 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.4e+04     |\n",
      "|    n_updates            | 3040        |\n",
      "|    policy_gradient_loss | -0.00315    |\n",
      "|    std                  | 0.373       |\n",
      "|    value_loss           | 6.29e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=625000, episode_reward=-50208.03 +/- 17.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 625000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016243175 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.94e+04    |\n",
      "|    n_updates            | 3050        |\n",
      "|    policy_gradient_loss | -0.00171    |\n",
      "|    std                  | 0.373       |\n",
      "|    value_loss           | 6.21e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 306       |\n",
      "|    time_elapsed    | 19607     |\n",
      "|    total_timesteps | 626688    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 307         |\n",
      "|    time_elapsed         | 19650       |\n",
      "|    total_timesteps      | 628736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019528883 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.53e+04    |\n",
      "|    n_updates            | 3060        |\n",
      "|    policy_gradient_loss | -0.00867    |\n",
      "|    std                  | 0.371       |\n",
      "|    value_loss           | 6.09e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=-50203.22 +/- 12.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 630000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017549077 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.47e+04    |\n",
      "|    n_updates            | 3070        |\n",
      "|    policy_gradient_loss | -0.0071     |\n",
      "|    std                  | 0.374       |\n",
      "|    value_loss           | 6.03e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 308       |\n",
      "|    time_elapsed    | 19748     |\n",
      "|    total_timesteps | 630784    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 309         |\n",
      "|    time_elapsed         | 19791       |\n",
      "|    total_timesteps      | 632832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018691426 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.88e+04    |\n",
      "|    n_updates            | 3080        |\n",
      "|    policy_gradient_loss | -0.00602    |\n",
      "|    std                  | 0.38        |\n",
      "|    value_loss           | 6.01e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 310         |\n",
      "|    time_elapsed         | 19834       |\n",
      "|    total_timesteps      | 634880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014325168 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.98e+04    |\n",
      "|    n_updates            | 3090        |\n",
      "|    policy_gradient_loss | -0.0097     |\n",
      "|    std                  | 0.379       |\n",
      "|    value_loss           | 5.96e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=635000, episode_reward=-50198.97 +/- 8.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 635000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015721433 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.21e+04    |\n",
      "|    n_updates            | 3100        |\n",
      "|    policy_gradient_loss | -0.00177    |\n",
      "|    std                  | 0.375       |\n",
      "|    value_loss           | 5.96e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 311       |\n",
      "|    time_elapsed    | 19931     |\n",
      "|    total_timesteps | 636928    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 31         |\n",
      "|    iterations           | 312        |\n",
      "|    time_elapsed         | 19974      |\n",
      "|    total_timesteps      | 638976     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01086322 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 3.58e+04   |\n",
      "|    n_updates            | 3110       |\n",
      "|    policy_gradient_loss | -0.00221   |\n",
      "|    std                  | 0.376      |\n",
      "|    value_loss           | 5.99e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=-50208.50 +/- 8.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 640000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015333623 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.42e+04    |\n",
      "|    n_updates            | 3120        |\n",
      "|    policy_gradient_loss | -0.00865    |\n",
      "|    std                  | 0.375       |\n",
      "|    value_loss           | 6.01e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 313       |\n",
      "|    time_elapsed    | 20072     |\n",
      "|    total_timesteps | 641024    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 314         |\n",
      "|    time_elapsed         | 20115       |\n",
      "|    total_timesteps      | 643072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017560847 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.29e+04    |\n",
      "|    n_updates            | 3130        |\n",
      "|    policy_gradient_loss | -0.00908    |\n",
      "|    std                  | 0.377       |\n",
      "|    value_loss           | 5.63e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=645000, episode_reward=-50206.75 +/- 15.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 645000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016882483 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.78e+04    |\n",
      "|    n_updates            | 3140        |\n",
      "|    policy_gradient_loss | -0.00682    |\n",
      "|    std                  | 0.377       |\n",
      "|    value_loss           | 5.47e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 315       |\n",
      "|    time_elapsed    | 20214     |\n",
      "|    total_timesteps | 645120    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 316         |\n",
      "|    time_elapsed         | 20257       |\n",
      "|    total_timesteps      | 647168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015320134 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.58e+04    |\n",
      "|    n_updates            | 3150        |\n",
      "|    policy_gradient_loss | -0.00664    |\n",
      "|    std                  | 0.377       |\n",
      "|    value_loss           | 5.35e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 317         |\n",
      "|    time_elapsed         | 20301       |\n",
      "|    total_timesteps      | 649216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018262323 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.62e+04    |\n",
      "|    n_updates            | 3160        |\n",
      "|    policy_gradient_loss | -0.0076     |\n",
      "|    std                  | 0.379       |\n",
      "|    value_loss           | 5.29e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=-50204.72 +/- 14.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 650000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016524378 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.83e+04    |\n",
      "|    n_updates            | 3170        |\n",
      "|    policy_gradient_loss | -0.00567    |\n",
      "|    std                  | 0.377       |\n",
      "|    value_loss           | 5.11e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 318       |\n",
      "|    time_elapsed    | 20400     |\n",
      "|    total_timesteps | 651264    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 319         |\n",
      "|    time_elapsed         | 20443       |\n",
      "|    total_timesteps      | 653312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015150903 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.49e+04    |\n",
      "|    n_updates            | 3180        |\n",
      "|    policy_gradient_loss | -0.00557    |\n",
      "|    std                  | 0.372       |\n",
      "|    value_loss           | 5.14e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=655000, episode_reward=-50211.78 +/- 7.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 655000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014740724 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.12e+04    |\n",
      "|    n_updates            | 3190        |\n",
      "|    policy_gradient_loss | -0.00524    |\n",
      "|    std                  | 0.37        |\n",
      "|    value_loss           | 5.06e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 320       |\n",
      "|    time_elapsed    | 20540     |\n",
      "|    total_timesteps | 655360    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 321         |\n",
      "|    time_elapsed         | 20583       |\n",
      "|    total_timesteps      | 657408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017497865 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.72e+04    |\n",
      "|    n_updates            | 3200        |\n",
      "|    policy_gradient_loss | 0.00464     |\n",
      "|    std                  | 0.367       |\n",
      "|    value_loss           | 5.42e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 31         |\n",
      "|    iterations           | 322        |\n",
      "|    time_elapsed         | 20626      |\n",
      "|    total_timesteps      | 659456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03710674 |\n",
      "|    clip_fraction        | 0.347      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 0.94       |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.93e+04   |\n",
      "|    n_updates            | 3210       |\n",
      "|    policy_gradient_loss | 0.0312     |\n",
      "|    std                  | 0.367      |\n",
      "|    value_loss           | 3.98e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-50216.07 +/- 10.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 660000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017057443 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.32e+04    |\n",
      "|    n_updates            | 3220        |\n",
      "|    policy_gradient_loss | 0.00275     |\n",
      "|    std                  | 0.368       |\n",
      "|    value_loss           | 4.92e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 323       |\n",
      "|    time_elapsed    | 20724     |\n",
      "|    total_timesteps | 661504    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 324         |\n",
      "|    time_elapsed         | 20767       |\n",
      "|    total_timesteps      | 663552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015134819 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.61e+04    |\n",
      "|    n_updates            | 3230        |\n",
      "|    policy_gradient_loss | -0.00762    |\n",
      "|    std                  | 0.367       |\n",
      "|    value_loss           | 4.81e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=665000, episode_reward=-50214.22 +/- 4.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 665000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015047091 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.38e+04    |\n",
      "|    n_updates            | 3240        |\n",
      "|    policy_gradient_loss | -0.00594    |\n",
      "|    std                  | 0.366       |\n",
      "|    value_loss           | 4.81e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 325       |\n",
      "|    time_elapsed    | 20865     |\n",
      "|    total_timesteps | 665600    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 326         |\n",
      "|    time_elapsed         | 20908       |\n",
      "|    total_timesteps      | 667648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016270317 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.51e+04    |\n",
      "|    n_updates            | 3250        |\n",
      "|    policy_gradient_loss | -0.00831    |\n",
      "|    std                  | 0.364       |\n",
      "|    value_loss           | 4.7e+04     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 327         |\n",
      "|    time_elapsed         | 20951       |\n",
      "|    total_timesteps      | 669696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014413226 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.68e+04    |\n",
      "|    n_updates            | 3260        |\n",
      "|    policy_gradient_loss | -0.00754    |\n",
      "|    std                  | 0.37        |\n",
      "|    value_loss           | 4.61e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=-50211.94 +/- 9.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 670000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023398641 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.44e+04    |\n",
      "|    n_updates            | 3270        |\n",
      "|    policy_gradient_loss | -0.00409    |\n",
      "|    std                  | 0.369       |\n",
      "|    value_loss           | 4.7e+04     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 328       |\n",
      "|    time_elapsed    | 21048     |\n",
      "|    total_timesteps | 671744    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 329         |\n",
      "|    time_elapsed         | 21091       |\n",
      "|    total_timesteps      | 673792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019388035 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.33e+04    |\n",
      "|    n_updates            | 3280        |\n",
      "|    policy_gradient_loss | 0.00113     |\n",
      "|    std                  | 0.37        |\n",
      "|    value_loss           | 4.8e+04     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=675000, episode_reward=-50206.45 +/- 17.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 675000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016153406 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.41e+04    |\n",
      "|    n_updates            | 3290        |\n",
      "|    policy_gradient_loss | -0.00481    |\n",
      "|    std                  | 0.365       |\n",
      "|    value_loss           | 4.49e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 330       |\n",
      "|    time_elapsed    | 21187     |\n",
      "|    total_timesteps | 675840    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 331         |\n",
      "|    time_elapsed         | 21230       |\n",
      "|    total_timesteps      | 677888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025559105 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.87e+04    |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | 0.000218    |\n",
      "|    std                  | 0.366       |\n",
      "|    value_loss           | 4.85e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 332         |\n",
      "|    time_elapsed         | 21273       |\n",
      "|    total_timesteps      | 679936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011713404 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.14e+04    |\n",
      "|    n_updates            | 3310        |\n",
      "|    policy_gradient_loss | -0.00422    |\n",
      "|    std                  | 0.371       |\n",
      "|    value_loss           | 4.48e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-50219.58 +/- 10.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 680000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016213093 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.01e+04    |\n",
      "|    n_updates            | 3320        |\n",
      "|    policy_gradient_loss | -0.00652    |\n",
      "|    std                  | 0.373       |\n",
      "|    value_loss           | 4.5e+04     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 333       |\n",
      "|    time_elapsed    | 21370     |\n",
      "|    total_timesteps | 681984    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 334         |\n",
      "|    time_elapsed         | 21414       |\n",
      "|    total_timesteps      | 684032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017612154 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.44e+04    |\n",
      "|    n_updates            | 3330        |\n",
      "|    policy_gradient_loss | -0.00505    |\n",
      "|    std                  | 0.374       |\n",
      "|    value_loss           | 4.56e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=685000, episode_reward=-50206.06 +/- 11.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 685000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015018545 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1e+04       |\n",
      "|    n_updates            | 3340        |\n",
      "|    policy_gradient_loss | -0.00524    |\n",
      "|    std                  | 0.371       |\n",
      "|    value_loss           | 4.18e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 335       |\n",
      "|    time_elapsed    | 21510     |\n",
      "|    total_timesteps | 686080    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 336         |\n",
      "|    time_elapsed         | 21553       |\n",
      "|    total_timesteps      | 688128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017572355 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.33e+04    |\n",
      "|    n_updates            | 3350        |\n",
      "|    policy_gradient_loss | -0.00883    |\n",
      "|    std                  | 0.368       |\n",
      "|    value_loss           | 4.09e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=-50203.30 +/- 17.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 690000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016408343 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.69e+04    |\n",
      "|    n_updates            | 3360        |\n",
      "|    policy_gradient_loss | -0.00555    |\n",
      "|    std                  | 0.363       |\n",
      "|    value_loss           | 4.03e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 337       |\n",
      "|    time_elapsed    | 21652     |\n",
      "|    total_timesteps | 690176    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 338         |\n",
      "|    time_elapsed         | 21695       |\n",
      "|    total_timesteps      | 692224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013802533 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.56e+04    |\n",
      "|    n_updates            | 3370        |\n",
      "|    policy_gradient_loss | -0.00612    |\n",
      "|    std                  | 0.362       |\n",
      "|    value_loss           | 3.96e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 339         |\n",
      "|    time_elapsed         | 21738       |\n",
      "|    total_timesteps      | 694272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015189595 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.89e+04    |\n",
      "|    n_updates            | 3380        |\n",
      "|    policy_gradient_loss | -0.00497    |\n",
      "|    std                  | 0.358       |\n",
      "|    value_loss           | 3.91e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=695000, episode_reward=-50210.06 +/- 9.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 695000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018194757 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.82e+04    |\n",
      "|    n_updates            | 3390        |\n",
      "|    policy_gradient_loss | -0.00524    |\n",
      "|    std                  | 0.363       |\n",
      "|    value_loss           | 3.85e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 340       |\n",
      "|    time_elapsed    | 21835     |\n",
      "|    total_timesteps | 696320    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 31        |\n",
      "|    iterations           | 341       |\n",
      "|    time_elapsed         | 21877     |\n",
      "|    total_timesteps      | 698368    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0149228 |\n",
      "|    clip_fraction        | 0.165     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.1      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.92e+04  |\n",
      "|    n_updates            | 3400      |\n",
      "|    policy_gradient_loss | -0.0101   |\n",
      "|    std                  | 0.366     |\n",
      "|    value_loss           | 3.77e+04  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=-50218.17 +/- 13.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 700000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01744625 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.08      |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 2.39e+04   |\n",
      "|    n_updates            | 3410       |\n",
      "|    policy_gradient_loss | -0.00212   |\n",
      "|    std                  | 0.359      |\n",
      "|    value_loss           | 3.76e+04   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 342       |\n",
      "|    time_elapsed    | 21976     |\n",
      "|    total_timesteps | 700416    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 343         |\n",
      "|    time_elapsed         | 22019       |\n",
      "|    total_timesteps      | 702464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016364066 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.89e+04    |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | -0.00247    |\n",
      "|    std                  | 0.357       |\n",
      "|    value_loss           | 3.66e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 31         |\n",
      "|    iterations           | 344        |\n",
      "|    time_elapsed         | 22062      |\n",
      "|    total_timesteps      | 704512     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01790401 |\n",
      "|    clip_fraction        | 0.193      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.43e+04   |\n",
      "|    n_updates            | 3430       |\n",
      "|    policy_gradient_loss | -0.00473   |\n",
      "|    std                  | 0.354      |\n",
      "|    value_loss           | 3.6e+04    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=705000, episode_reward=-50206.60 +/- 19.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 705000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03788537 |\n",
      "|    clip_fraction        | 0.298      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.34e+04   |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | 0.0153     |\n",
      "|    std                  | 0.353      |\n",
      "|    value_loss           | 3.72e+04   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 345       |\n",
      "|    time_elapsed    | 22159     |\n",
      "|    total_timesteps | 706560    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 346         |\n",
      "|    time_elapsed         | 22201       |\n",
      "|    total_timesteps      | 708608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015119575 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.998      |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.58e+04    |\n",
      "|    n_updates            | 3450        |\n",
      "|    policy_gradient_loss | 0.00153     |\n",
      "|    std                  | 0.353       |\n",
      "|    value_loss           | 5.25e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=-50199.27 +/- 19.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 710000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019193716 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.77e+04    |\n",
      "|    n_updates            | 3460        |\n",
      "|    policy_gradient_loss | -0.00238    |\n",
      "|    std                  | 0.355       |\n",
      "|    value_loss           | 3.65e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 347       |\n",
      "|    time_elapsed    | 22299     |\n",
      "|    total_timesteps | 710656    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 348         |\n",
      "|    time_elapsed         | 22343       |\n",
      "|    total_timesteps      | 712704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016755395 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.17e+04    |\n",
      "|    n_updates            | 3470        |\n",
      "|    policy_gradient_loss | -0.000238   |\n",
      "|    std                  | 0.353       |\n",
      "|    value_loss           | 3.37e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 31         |\n",
      "|    iterations           | 349        |\n",
      "|    time_elapsed         | 22387      |\n",
      "|    total_timesteps      | 714752     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03411482 |\n",
      "|    clip_fraction        | 0.248      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.997     |\n",
      "|    explained_variance   | 0.933      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.08e+04   |\n",
      "|    n_updates            | 3480       |\n",
      "|    policy_gradient_loss | 0.00964    |\n",
      "|    std                  | 0.353      |\n",
      "|    value_loss           | 1.19e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=715000, episode_reward=-50195.38 +/- 17.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 715000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017450664 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.24e+04    |\n",
      "|    n_updates            | 3490        |\n",
      "|    policy_gradient_loss | -0.000859   |\n",
      "|    std                  | 0.354       |\n",
      "|    value_loss           | 3.33e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 350       |\n",
      "|    time_elapsed    | 22485     |\n",
      "|    total_timesteps | 716800    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 351         |\n",
      "|    time_elapsed         | 22528       |\n",
      "|    total_timesteps      | 718848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018414084 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.96e+04    |\n",
      "|    n_updates            | 3500        |\n",
      "|    policy_gradient_loss | -0.00931    |\n",
      "|    std                  | 0.356       |\n",
      "|    value_loss           | 3.3e+04     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-50194.79 +/- 4.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 720000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019203657 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.43e+04    |\n",
      "|    n_updates            | 3510        |\n",
      "|    policy_gradient_loss | 0.00144     |\n",
      "|    std                  | 0.355       |\n",
      "|    value_loss           | 3.29e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 352       |\n",
      "|    time_elapsed    | 22625     |\n",
      "|    total_timesteps | 720896    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 353         |\n",
      "|    time_elapsed         | 22668       |\n",
      "|    total_timesteps      | 722944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014922429 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.8e+03     |\n",
      "|    n_updates            | 3520        |\n",
      "|    policy_gradient_loss | -0.00783    |\n",
      "|    std                  | 0.356       |\n",
      "|    value_loss           | 3.25e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 354         |\n",
      "|    time_elapsed         | 22711       |\n",
      "|    total_timesteps      | 724992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015251917 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.4e+04     |\n",
      "|    n_updates            | 3530        |\n",
      "|    policy_gradient_loss | -0.00193    |\n",
      "|    std                  | 0.356       |\n",
      "|    value_loss           | 3.3e+04     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=725000, episode_reward=-50215.59 +/- 10.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 725000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050727636 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.988      |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.14e+04    |\n",
      "|    n_updates            | 3540        |\n",
      "|    policy_gradient_loss | 0.0171      |\n",
      "|    std                  | 0.353       |\n",
      "|    value_loss           | 3.52e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 355       |\n",
      "|    time_elapsed    | 22807     |\n",
      "|    total_timesteps | 727040    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 356         |\n",
      "|    time_elapsed         | 22850       |\n",
      "|    total_timesteps      | 729088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017359583 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.973      |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.2e+04     |\n",
      "|    n_updates            | 3550        |\n",
      "|    policy_gradient_loss | -0.00276    |\n",
      "|    std                  | 0.353       |\n",
      "|    value_loss           | 3.02e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=-50206.67 +/- 12.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 730000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02688978 |\n",
      "|    clip_fraction        | 0.241      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.978     |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.34e+04   |\n",
      "|    n_updates            | 3560       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.354      |\n",
      "|    value_loss           | 2.96e+04   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 357       |\n",
      "|    time_elapsed    | 22948     |\n",
      "|    total_timesteps | 731136    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 358         |\n",
      "|    time_elapsed         | 22991       |\n",
      "|    total_timesteps      | 733184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017916054 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.993      |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.47e+04    |\n",
      "|    n_updates            | 3570        |\n",
      "|    policy_gradient_loss | -0.00543    |\n",
      "|    std                  | 0.356       |\n",
      "|    value_loss           | 2.92e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=735000, episode_reward=-50197.29 +/- 6.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 735000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014797932 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.996      |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.64e+04    |\n",
      "|    n_updates            | 3580        |\n",
      "|    policy_gradient_loss | -0.000484   |\n",
      "|    std                  | 0.356       |\n",
      "|    value_loss           | 1.07e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 359       |\n",
      "|    time_elapsed    | 23090     |\n",
      "|    total_timesteps | 735232    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 360         |\n",
      "|    time_elapsed         | 23133       |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018940551 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.16e+04    |\n",
      "|    n_updates            | 3590        |\n",
      "|    policy_gradient_loss | -0.00932    |\n",
      "|    std                  | 0.358       |\n",
      "|    value_loss           | 2.83e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 361         |\n",
      "|    time_elapsed         | 23175       |\n",
      "|    total_timesteps      | 739328      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016130326 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.26e+04    |\n",
      "|    n_updates            | 3600        |\n",
      "|    policy_gradient_loss | -0.00692    |\n",
      "|    std                  | 0.36        |\n",
      "|    value_loss           | 2.78e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=-50214.18 +/- 7.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 740000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018437024 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.994      |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.15e+04    |\n",
      "|    n_updates            | 3610        |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    std                  | 0.356       |\n",
      "|    value_loss           | 2.75e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 362       |\n",
      "|    time_elapsed    | 23275     |\n",
      "|    total_timesteps | 741376    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 363         |\n",
      "|    time_elapsed         | 23319       |\n",
      "|    total_timesteps      | 743424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014667559 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.965      |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.68e+04    |\n",
      "|    n_updates            | 3620        |\n",
      "|    policy_gradient_loss | -0.00603    |\n",
      "|    std                  | 0.353       |\n",
      "|    value_loss           | 2.69e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=745000, episode_reward=-50214.37 +/- 7.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 745000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019435026 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.959      |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.63e+03    |\n",
      "|    n_updates            | 3630        |\n",
      "|    policy_gradient_loss | -0.00833    |\n",
      "|    std                  | 0.353       |\n",
      "|    value_loss           | 2.66e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 364       |\n",
      "|    time_elapsed    | 23419     |\n",
      "|    total_timesteps | 745472    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 365         |\n",
      "|    time_elapsed         | 23463       |\n",
      "|    total_timesteps      | 747520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015442708 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.959      |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.09e+03    |\n",
      "|    n_updates            | 3640        |\n",
      "|    policy_gradient_loss | -0.00361    |\n",
      "|    std                  | 0.352       |\n",
      "|    value_loss           | 2.49e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 366         |\n",
      "|    time_elapsed         | 23507       |\n",
      "|    total_timesteps      | 749568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018254839 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.933      |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.2e+04     |\n",
      "|    n_updates            | 3650        |\n",
      "|    policy_gradient_loss | -0.00684    |\n",
      "|    std                  | 0.347       |\n",
      "|    value_loss           | 2.38e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=-50218.05 +/- 13.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 750000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014201394 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.913      |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.05e+04    |\n",
      "|    n_updates            | 3660        |\n",
      "|    policy_gradient_loss | -0.00941    |\n",
      "|    std                  | 0.344       |\n",
      "|    value_loss           | 2.38e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 367       |\n",
      "|    time_elapsed    | 23606     |\n",
      "|    total_timesteps | 751616    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 368         |\n",
      "|    time_elapsed         | 23649       |\n",
      "|    total_timesteps      | 753664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020355357 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.911      |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.27e+04    |\n",
      "|    n_updates            | 3670        |\n",
      "|    policy_gradient_loss | -0.0045     |\n",
      "|    std                  | 0.344       |\n",
      "|    value_loss           | 2.44e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=755000, episode_reward=-50199.20 +/- 8.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 755000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018323189 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.918      |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 9.05e+03    |\n",
      "|    n_updates            | 3680        |\n",
      "|    policy_gradient_loss | -0.00928    |\n",
      "|    std                  | 0.346       |\n",
      "|    value_loss           | 2.42e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 369       |\n",
      "|    time_elapsed    | 23747     |\n",
      "|    total_timesteps | 755712    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 370         |\n",
      "|    time_elapsed         | 23790       |\n",
      "|    total_timesteps      | 757760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017243173 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.929      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.03e+03    |\n",
      "|    n_updates            | 3690        |\n",
      "|    policy_gradient_loss | -0.00503    |\n",
      "|    std                  | 0.348       |\n",
      "|    value_loss           | 2.37e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 371         |\n",
      "|    time_elapsed         | 23833       |\n",
      "|    total_timesteps      | 759808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018877745 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.935      |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 9.64e+03    |\n",
      "|    n_updates            | 3700        |\n",
      "|    policy_gradient_loss | -0.00197    |\n",
      "|    std                  | 0.347       |\n",
      "|    value_loss           | 2.35e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=-50202.71 +/- 14.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 760000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016244398 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.935      |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.34e+03    |\n",
      "|    n_updates            | 3710        |\n",
      "|    policy_gradient_loss | -0.00705    |\n",
      "|    std                  | 0.348       |\n",
      "|    value_loss           | 2.3e+04     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 372       |\n",
      "|    time_elapsed    | 23930     |\n",
      "|    total_timesteps | 761856    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 373         |\n",
      "|    time_elapsed         | 23974       |\n",
      "|    total_timesteps      | 763904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013932652 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.911      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.27e+04    |\n",
      "|    n_updates            | 3720        |\n",
      "|    policy_gradient_loss | -0.00382    |\n",
      "|    std                  | 0.342       |\n",
      "|    value_loss           | 2.3e+04     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=765000, episode_reward=-50219.43 +/- 10.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 765000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054506674 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.891      |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.6e+04     |\n",
      "|    n_updates            | 3730        |\n",
      "|    policy_gradient_loss | 0.0295      |\n",
      "|    std                  | 0.343       |\n",
      "|    value_loss           | 7.97e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 374       |\n",
      "|    time_elapsed    | 24072     |\n",
      "|    total_timesteps | 765952    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 375         |\n",
      "|    time_elapsed         | 24115       |\n",
      "|    total_timesteps      | 768000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023461245 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.889      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 9.48e+03    |\n",
      "|    n_updates            | 3740        |\n",
      "|    policy_gradient_loss | 0.00666     |\n",
      "|    std                  | 0.342       |\n",
      "|    value_loss           | 2.43e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=-50197.03 +/- 3.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 770000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017505523 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.891      |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.91e+03    |\n",
      "|    n_updates            | 3750        |\n",
      "|    policy_gradient_loss | -0.00374    |\n",
      "|    std                  | 0.342       |\n",
      "|    value_loss           | 2.43e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 376       |\n",
      "|    time_elapsed    | 24213     |\n",
      "|    total_timesteps | 770048    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 377         |\n",
      "|    time_elapsed         | 24256       |\n",
      "|    total_timesteps      | 772096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019111834 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.906      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 9.49e+03    |\n",
      "|    n_updates            | 3760        |\n",
      "|    policy_gradient_loss | 0.00609     |\n",
      "|    std                  | 0.345       |\n",
      "|    value_loss           | 2.25e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 378         |\n",
      "|    time_elapsed         | 24299       |\n",
      "|    total_timesteps      | 774144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018806051 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.922      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.35e+04    |\n",
      "|    n_updates            | 3770        |\n",
      "|    policy_gradient_loss | -0.000924   |\n",
      "|    std                  | 0.344       |\n",
      "|    value_loss           | 2.08e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=775000, episode_reward=-50199.01 +/- 22.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 775000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018022612 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.922      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.11e+04    |\n",
      "|    n_updates            | 3780        |\n",
      "|    policy_gradient_loss | -0.00525    |\n",
      "|    std                  | 0.347       |\n",
      "|    value_loss           | 2.04e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 379       |\n",
      "|    time_elapsed    | 24397     |\n",
      "|    total_timesteps | 776192    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 380         |\n",
      "|    time_elapsed         | 24440       |\n",
      "|    total_timesteps      | 778240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016986012 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.956      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 9.57e+03    |\n",
      "|    n_updates            | 3790        |\n",
      "|    policy_gradient_loss | -0.00703    |\n",
      "|    std                  | 0.352       |\n",
      "|    value_loss           | 2e+04       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-50202.59 +/- 12.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 780000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01993825 |\n",
      "|    clip_fraction        | 0.222      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.978     |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.49e+04   |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.00027   |\n",
      "|    std                  | 0.356      |\n",
      "|    value_loss           | 1.98e+04   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 381       |\n",
      "|    time_elapsed    | 24538     |\n",
      "|    total_timesteps | 780288    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 382         |\n",
      "|    time_elapsed         | 24581       |\n",
      "|    total_timesteps      | 782336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014333131 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.992      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.55e+04    |\n",
      "|    n_updates            | 3810        |\n",
      "|    policy_gradient_loss | -0.00683    |\n",
      "|    std                  | 0.356       |\n",
      "|    value_loss           | 1.93e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 383         |\n",
      "|    time_elapsed         | 24625       |\n",
      "|    total_timesteps      | 784384      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015258379 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.999      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.4e+04     |\n",
      "|    n_updates            | 3820        |\n",
      "|    policy_gradient_loss | -0.00389    |\n",
      "|    std                  | 0.357       |\n",
      "|    value_loss           | 1.89e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=785000, episode_reward=-50210.33 +/- 6.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 785000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01435836 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.32e+04   |\n",
      "|    n_updates            | 3830       |\n",
      "|    policy_gradient_loss | -0.00758   |\n",
      "|    std                  | 0.359      |\n",
      "|    value_loss           | 1.86e+04   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 384       |\n",
      "|    time_elapsed    | 24723     |\n",
      "|    total_timesteps | 786432    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 385         |\n",
      "|    time_elapsed         | 24766       |\n",
      "|    total_timesteps      | 788480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030380014 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.85e+03    |\n",
      "|    n_updates            | 3840        |\n",
      "|    policy_gradient_loss | 0.0102      |\n",
      "|    std                  | 0.358       |\n",
      "|    value_loss           | 1.84e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=-50199.09 +/- 17.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 790000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020456241 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.63e+03    |\n",
      "|    n_updates            | 3850        |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    std                  | 0.363       |\n",
      "|    value_loss           | 1.84e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 386       |\n",
      "|    time_elapsed    | 24863     |\n",
      "|    total_timesteps | 790528    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 387         |\n",
      "|    time_elapsed         | 24906       |\n",
      "|    total_timesteps      | 792576      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016636917 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.07e+04    |\n",
      "|    n_updates            | 3860        |\n",
      "|    policy_gradient_loss | -0.00693    |\n",
      "|    std                  | 0.362       |\n",
      "|    value_loss           | 1.77e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 388         |\n",
      "|    time_elapsed         | 24949       |\n",
      "|    total_timesteps      | 794624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037457693 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.1e+04     |\n",
      "|    n_updates            | 3870        |\n",
      "|    policy_gradient_loss | 0.0167      |\n",
      "|    std                  | 0.362       |\n",
      "|    value_loss           | 6.88e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=795000, episode_reward=-50206.58 +/- 10.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 795000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014192334 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.56e+04    |\n",
      "|    n_updates            | 3880        |\n",
      "|    policy_gradient_loss | 0.00851     |\n",
      "|    std                  | 0.362       |\n",
      "|    value_loss           | 8.71e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 389       |\n",
      "|    time_elapsed    | 25047     |\n",
      "|    total_timesteps | 796672    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 390         |\n",
      "|    time_elapsed         | 25090       |\n",
      "|    total_timesteps      | 798720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020250827 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1e+04       |\n",
      "|    n_updates            | 3890        |\n",
      "|    policy_gradient_loss | -0.00446    |\n",
      "|    std                  | 0.362       |\n",
      "|    value_loss           | 1.7e+04     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=-50206.34 +/- 6.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020188546 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.21e+03    |\n",
      "|    n_updates            | 3900        |\n",
      "|    policy_gradient_loss | -0.00683    |\n",
      "|    std                  | 0.368       |\n",
      "|    value_loss           | 1.66e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 391       |\n",
      "|    time_elapsed    | 25188     |\n",
      "|    total_timesteps | 800768    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 392         |\n",
      "|    time_elapsed         | 25231       |\n",
      "|    total_timesteps      | 802816      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021341136 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 9.34e+03    |\n",
      "|    n_updates            | 3910        |\n",
      "|    policy_gradient_loss | -0.000922   |\n",
      "|    std                  | 0.367       |\n",
      "|    value_loss           | 1.68e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 393         |\n",
      "|    time_elapsed         | 25274       |\n",
      "|    total_timesteps      | 804864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018052712 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 9.74e+03    |\n",
      "|    n_updates            | 3920        |\n",
      "|    policy_gradient_loss | -0.002      |\n",
      "|    std                  | 0.365       |\n",
      "|    value_loss           | 1.71e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=805000, episode_reward=-50202.43 +/- 12.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 805000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012361094 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.07e+04    |\n",
      "|    n_updates            | 3930        |\n",
      "|    policy_gradient_loss | 0.00252     |\n",
      "|    std                  | 0.365       |\n",
      "|    value_loss           | 3.82e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 394       |\n",
      "|    time_elapsed    | 25371     |\n",
      "|    total_timesteps | 806912    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 395         |\n",
      "|    time_elapsed         | 25415       |\n",
      "|    total_timesteps      | 808960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022984851 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.37e+03    |\n",
      "|    n_updates            | 3940        |\n",
      "|    policy_gradient_loss | 0.00783     |\n",
      "|    std                  | 0.364       |\n",
      "|    value_loss           | 1.92e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=-50203.09 +/- 9.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 810000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018631872 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.35e+03    |\n",
      "|    n_updates            | 3950        |\n",
      "|    policy_gradient_loss | -0.00288    |\n",
      "|    std                  | 0.367       |\n",
      "|    value_loss           | 1.66e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 396       |\n",
      "|    time_elapsed    | 25514     |\n",
      "|    total_timesteps | 811008    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 31         |\n",
      "|    iterations           | 397        |\n",
      "|    time_elapsed         | 25557      |\n",
      "|    total_timesteps      | 813056     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01638769 |\n",
      "|    clip_fraction        | 0.205      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.997      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 8.61e+03   |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | -0.00771   |\n",
      "|    std                  | 0.364      |\n",
      "|    value_loss           | 1.64e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=815000, episode_reward=-50215.86 +/- 11.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 815000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01695954 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | 0.997      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 6.05e+03   |\n",
      "|    n_updates            | 3970       |\n",
      "|    policy_gradient_loss | -0.00314   |\n",
      "|    std                  | 0.363      |\n",
      "|    value_loss           | 1.5e+04    |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 398       |\n",
      "|    time_elapsed    | 25656     |\n",
      "|    total_timesteps | 815104    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 399         |\n",
      "|    time_elapsed         | 25699       |\n",
      "|    total_timesteps      | 817152      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014069205 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.54e+03    |\n",
      "|    n_updates            | 3980        |\n",
      "|    policy_gradient_loss | -0.0077     |\n",
      "|    std                  | 0.365       |\n",
      "|    value_loss           | 1.47e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 400         |\n",
      "|    time_elapsed         | 25743       |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020733913 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.87e+03    |\n",
      "|    n_updates            | 3990        |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    std                  | 0.363       |\n",
      "|    value_loss           | 1.44e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=-50208.57 +/- 10.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 820000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028325051 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.52e+04    |\n",
      "|    n_updates            | 4000        |\n",
      "|    policy_gradient_loss | 0.0281      |\n",
      "|    std                  | 0.363       |\n",
      "|    value_loss           | 4.96e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 401       |\n",
      "|    time_elapsed    | 25842     |\n",
      "|    total_timesteps | 821248    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 402         |\n",
      "|    time_elapsed         | 25885       |\n",
      "|    total_timesteps      | 823296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019473832 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.21e+03    |\n",
      "|    n_updates            | 4010        |\n",
      "|    policy_gradient_loss | -0.00312    |\n",
      "|    std                  | 0.364       |\n",
      "|    value_loss           | 1.42e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=825000, episode_reward=-50218.10 +/- 11.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 825000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014588646 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.71e+03    |\n",
      "|    n_updates            | 4020        |\n",
      "|    policy_gradient_loss | -0.00499    |\n",
      "|    std                  | 0.363       |\n",
      "|    value_loss           | 1.36e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 403       |\n",
      "|    time_elapsed    | 25984     |\n",
      "|    total_timesteps | 825344    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 404         |\n",
      "|    time_elapsed         | 26028       |\n",
      "|    total_timesteps      | 827392      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017535262 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.13e+03    |\n",
      "|    n_updates            | 4030        |\n",
      "|    policy_gradient_loss | -0.00924    |\n",
      "|    std                  | 0.364       |\n",
      "|    value_loss           | 1.34e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 31         |\n",
      "|    iterations           | 405        |\n",
      "|    time_elapsed         | 26072      |\n",
      "|    total_timesteps      | 829440     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01973148 |\n",
      "|    clip_fraction        | 0.205      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | 0.997      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 3.83e+03   |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.00619   |\n",
      "|    std                  | 0.365      |\n",
      "|    value_loss           | 1.33e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=-50217.27 +/- 15.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 830000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016425733 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.77e+03    |\n",
      "|    n_updates            | 4050        |\n",
      "|    policy_gradient_loss | -0.00517    |\n",
      "|    std                  | 0.368       |\n",
      "|    value_loss           | 1.3e+04     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 406       |\n",
      "|    time_elapsed    | 26173     |\n",
      "|    total_timesteps | 831488    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 407         |\n",
      "|    time_elapsed         | 26216       |\n",
      "|    total_timesteps      | 833536      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021761866 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.97e+03    |\n",
      "|    n_updates            | 4060        |\n",
      "|    policy_gradient_loss | -0.00694    |\n",
      "|    std                  | 0.364       |\n",
      "|    value_loss           | 1.28e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=835000, episode_reward=-50199.04 +/- 15.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 835000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020600008 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.57e+03    |\n",
      "|    n_updates            | 4070        |\n",
      "|    policy_gradient_loss | -0.00788    |\n",
      "|    std                  | 0.369       |\n",
      "|    value_loss           | 1.24e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 408       |\n",
      "|    time_elapsed    | 26315     |\n",
      "|    total_timesteps | 835584    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 409         |\n",
      "|    time_elapsed         | 26358       |\n",
      "|    total_timesteps      | 837632      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015449384 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.19e+03    |\n",
      "|    n_updates            | 4080        |\n",
      "|    policy_gradient_loss | -0.00445    |\n",
      "|    std                  | 0.363       |\n",
      "|    value_loss           | 1.22e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 410         |\n",
      "|    time_elapsed         | 26402       |\n",
      "|    total_timesteps      | 839680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040994026 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.47e+03    |\n",
      "|    n_updates            | 4090        |\n",
      "|    policy_gradient_loss | 0.0143      |\n",
      "|    std                  | 0.363       |\n",
      "|    value_loss           | 1.44e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=-50202.88 +/- 18.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 840000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017810049 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.92e+03    |\n",
      "|    n_updates            | 4100        |\n",
      "|    policy_gradient_loss | -0.00937    |\n",
      "|    std                  | 0.37        |\n",
      "|    value_loss           | 1.18e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 411       |\n",
      "|    time_elapsed    | 26500     |\n",
      "|    total_timesteps | 841728    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 412         |\n",
      "|    time_elapsed         | 26544       |\n",
      "|    total_timesteps      | 843776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014204926 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.89e+03    |\n",
      "|    n_updates            | 4110        |\n",
      "|    policy_gradient_loss | -0.00706    |\n",
      "|    std                  | 0.373       |\n",
      "|    value_loss           | 1.17e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=845000, episode_reward=-50208.22 +/- 15.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 845000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01731537 |\n",
      "|    clip_fraction        | 0.185      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.13      |\n",
      "|    explained_variance   | 0.998      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 9.83e+03   |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.00373   |\n",
      "|    std                  | 0.378      |\n",
      "|    value_loss           | 1.14e+04   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 413       |\n",
      "|    time_elapsed    | 26642     |\n",
      "|    total_timesteps | 845824    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 31         |\n",
      "|    iterations           | 414        |\n",
      "|    time_elapsed         | 26685      |\n",
      "|    total_timesteps      | 847872     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03001406 |\n",
      "|    clip_fraction        | 0.313      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.14      |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.12e+04   |\n",
      "|    n_updates            | 4130       |\n",
      "|    policy_gradient_loss | 0.0208     |\n",
      "|    std                  | 0.379      |\n",
      "|    value_loss           | 1.76e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 415         |\n",
      "|    time_elapsed         | 26729       |\n",
      "|    total_timesteps      | 849920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021787472 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.26e+03    |\n",
      "|    n_updates            | 4140        |\n",
      "|    policy_gradient_loss | 0.00319     |\n",
      "|    std                  | 0.374       |\n",
      "|    value_loss           | 9.18e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=-50208.70 +/- 14.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 850000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016002549 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.64e+03    |\n",
      "|    n_updates            | 4150        |\n",
      "|    policy_gradient_loss | -0.00685    |\n",
      "|    std                  | 0.373       |\n",
      "|    value_loss           | 1.12e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 416       |\n",
      "|    time_elapsed    | 26828     |\n",
      "|    total_timesteps | 851968    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 417         |\n",
      "|    time_elapsed         | 26872       |\n",
      "|    total_timesteps      | 854016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015269786 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.63e+03    |\n",
      "|    n_updates            | 4160        |\n",
      "|    policy_gradient_loss | -0.00872    |\n",
      "|    std                  | 0.374       |\n",
      "|    value_loss           | 1.17e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=855000, episode_reward=-50209.65 +/- 16.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 855000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017593196 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.2e+03     |\n",
      "|    n_updates            | 4170        |\n",
      "|    policy_gradient_loss | -0.00611    |\n",
      "|    std                  | 0.377       |\n",
      "|    value_loss           | 1.14e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 418       |\n",
      "|    time_elapsed    | 26968     |\n",
      "|    total_timesteps | 856064    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 419         |\n",
      "|    time_elapsed         | 27012       |\n",
      "|    total_timesteps      | 858112      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020255191 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.69e+03    |\n",
      "|    n_updates            | 4180        |\n",
      "|    policy_gradient_loss | 0.00128     |\n",
      "|    std                  | 0.377       |\n",
      "|    value_loss           | 1.09e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=-50200.93 +/- 15.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 860000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016995143 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.89e+03    |\n",
      "|    n_updates            | 4190        |\n",
      "|    policy_gradient_loss | -0.00546    |\n",
      "|    std                  | 0.373       |\n",
      "|    value_loss           | 1.04e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 420       |\n",
      "|    time_elapsed    | 27110     |\n",
      "|    total_timesteps | 860160    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 421         |\n",
      "|    time_elapsed         | 27154       |\n",
      "|    total_timesteps      | 862208      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015160963 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.58e+03    |\n",
      "|    n_updates            | 4200        |\n",
      "|    policy_gradient_loss | -0.00436    |\n",
      "|    std                  | 0.375       |\n",
      "|    value_loss           | 1.02e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 422         |\n",
      "|    time_elapsed         | 27197       |\n",
      "|    total_timesteps      | 864256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021822408 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.5e+03     |\n",
      "|    n_updates            | 4210        |\n",
      "|    policy_gradient_loss | -0.00568    |\n",
      "|    std                  | 0.369       |\n",
      "|    value_loss           | 1.01e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=865000, episode_reward=-50204.48 +/- 7.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 865000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014431513 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.49e+03    |\n",
      "|    n_updates            | 4220        |\n",
      "|    policy_gradient_loss | -0.00533    |\n",
      "|    std                  | 0.367       |\n",
      "|    value_loss           | 9.59e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 423       |\n",
      "|    time_elapsed    | 27296     |\n",
      "|    total_timesteps | 866304    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 424         |\n",
      "|    time_elapsed         | 27339       |\n",
      "|    total_timesteps      | 868352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020822857 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.66e+03    |\n",
      "|    n_updates            | 4230        |\n",
      "|    policy_gradient_loss | -0.00781    |\n",
      "|    std                  | 0.363       |\n",
      "|    value_loss           | 9.77e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=-50206.47 +/- 10.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 870000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018162452 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.52e+03    |\n",
      "|    n_updates            | 4240        |\n",
      "|    policy_gradient_loss | -0.00198    |\n",
      "|    std                  | 0.362       |\n",
      "|    value_loss           | 9.64e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 425       |\n",
      "|    time_elapsed    | 27437     |\n",
      "|    total_timesteps | 870400    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 426         |\n",
      "|    time_elapsed         | 27480       |\n",
      "|    total_timesteps      | 872448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019839862 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.35e+03    |\n",
      "|    n_updates            | 4250        |\n",
      "|    policy_gradient_loss | -0.00894    |\n",
      "|    std                  | 0.36        |\n",
      "|    value_loss           | 9.4e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 427         |\n",
      "|    time_elapsed         | 27523       |\n",
      "|    total_timesteps      | 874496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013445029 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.9e+03     |\n",
      "|    n_updates            | 4260        |\n",
      "|    policy_gradient_loss | -0.00679    |\n",
      "|    std                  | 0.36        |\n",
      "|    value_loss           | 9.26e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=875000, episode_reward=-50213.82 +/- 9.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 875000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032094743 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.32e+03    |\n",
      "|    n_updates            | 4270        |\n",
      "|    policy_gradient_loss | 0.00376     |\n",
      "|    std                  | 0.361       |\n",
      "|    value_loss           | 8.36e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 428       |\n",
      "|    time_elapsed    | 27621     |\n",
      "|    total_timesteps | 876544    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 429         |\n",
      "|    time_elapsed         | 27665       |\n",
      "|    total_timesteps      | 878592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017557537 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.78e+03    |\n",
      "|    n_updates            | 4280        |\n",
      "|    policy_gradient_loss | 0.00569     |\n",
      "|    std                  | 0.358       |\n",
      "|    value_loss           | 6.78e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=-50210.22 +/- 16.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 880000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018889433 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.77e+03    |\n",
      "|    n_updates            | 4290        |\n",
      "|    policy_gradient_loss | -0.00509    |\n",
      "|    std                  | 0.357       |\n",
      "|    value_loss           | 6.51e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 430       |\n",
      "|    time_elapsed    | 27762     |\n",
      "|    total_timesteps | 880640    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 31         |\n",
      "|    iterations           | 431        |\n",
      "|    time_elapsed         | 27806      |\n",
      "|    total_timesteps      | 882688     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01851771 |\n",
      "|    clip_fraction        | 0.208      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.984     |\n",
      "|    explained_variance   | 0.999      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 3.96e+03   |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.00251   |\n",
      "|    std                  | 0.353      |\n",
      "|    value_loss           | 6.37e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 432         |\n",
      "|    time_elapsed         | 27849       |\n",
      "|    total_timesteps      | 884736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027334973 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.968      |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.62e+03    |\n",
      "|    n_updates            | 4310        |\n",
      "|    policy_gradient_loss | 0.0127      |\n",
      "|    std                  | 0.353       |\n",
      "|    value_loss           | 1.57e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=885000, episode_reward=-50208.03 +/- 15.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 885000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028880231 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.969      |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.35e+03    |\n",
      "|    n_updates            | 4320        |\n",
      "|    policy_gradient_loss | 0.0193      |\n",
      "|    std                  | 0.353       |\n",
      "|    value_loss           | 6.13e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 433       |\n",
      "|    time_elapsed    | 27947     |\n",
      "|    total_timesteps | 886784    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 434         |\n",
      "|    time_elapsed         | 27990       |\n",
      "|    total_timesteps      | 888832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024132209 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.955      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.42e+03    |\n",
      "|    n_updates            | 4330        |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    std                  | 0.352       |\n",
      "|    value_loss           | 6.89e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=-50209.98 +/- 10.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 890000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016788766 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.934      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.01e+03    |\n",
      "|    n_updates            | 4340        |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    std                  | 0.35        |\n",
      "|    value_loss           | 6.64e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 435       |\n",
      "|    time_elapsed    | 28087     |\n",
      "|    total_timesteps | 890880    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 31         |\n",
      "|    iterations           | 436        |\n",
      "|    time_elapsed         | 28130      |\n",
      "|    total_timesteps      | 892928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02435828 |\n",
      "|    clip_fraction        | 0.211      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.898     |\n",
      "|    explained_variance   | 0.999      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 4.23e+03   |\n",
      "|    n_updates            | 4350       |\n",
      "|    policy_gradient_loss | -0.0082    |\n",
      "|    std                  | 0.344      |\n",
      "|    value_loss           | 6.6e+03    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 437         |\n",
      "|    time_elapsed         | 28174       |\n",
      "|    total_timesteps      | 894976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019234315 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.888      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.35e+03    |\n",
      "|    n_updates            | 4360        |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    std                  | 0.344       |\n",
      "|    value_loss           | 6.63e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=895000, episode_reward=-50200.70 +/- 6.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 895000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017728914 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.886      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.88e+03    |\n",
      "|    n_updates            | 4370        |\n",
      "|    policy_gradient_loss | -0.00514    |\n",
      "|    std                  | 0.343       |\n",
      "|    value_loss           | 7e+03       |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 438       |\n",
      "|    time_elapsed    | 28272     |\n",
      "|    total_timesteps | 897024    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 439         |\n",
      "|    time_elapsed         | 28316       |\n",
      "|    total_timesteps      | 899072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018951979 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.863      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.07e+03    |\n",
      "|    n_updates            | 4380        |\n",
      "|    policy_gradient_loss | -0.0033     |\n",
      "|    std                  | 0.339       |\n",
      "|    value_loss           | 6.68e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=-50212.36 +/- 9.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 900000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018537598 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.836      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 529         |\n",
      "|    n_updates            | 4390        |\n",
      "|    policy_gradient_loss | -0.00599    |\n",
      "|    std                  | 0.337       |\n",
      "|    value_loss           | 5.37e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 440       |\n",
      "|    time_elapsed    | 28414     |\n",
      "|    total_timesteps | 901120    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 441         |\n",
      "|    time_elapsed         | 28458       |\n",
      "|    total_timesteps      | 903168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021486681 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.846      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.09e+03    |\n",
      "|    n_updates            | 4400        |\n",
      "|    policy_gradient_loss | -0.00604    |\n",
      "|    std                  | 0.34        |\n",
      "|    value_loss           | 5.74e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=905000, episode_reward=-50206.99 +/- 10.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 905000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02226841 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.873     |\n",
      "|    explained_variance   | 0.999      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.21e+03   |\n",
      "|    n_updates            | 4410       |\n",
      "|    policy_gradient_loss | -0.00916   |\n",
      "|    std                  | 0.344      |\n",
      "|    value_loss           | 5.23e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 442       |\n",
      "|    time_elapsed    | 28557     |\n",
      "|    total_timesteps | 905216    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 443         |\n",
      "|    time_elapsed         | 28600       |\n",
      "|    total_timesteps      | 907264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027506512 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.871      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.35e+03    |\n",
      "|    n_updates            | 4420        |\n",
      "|    policy_gradient_loss | -0.00915    |\n",
      "|    std                  | 0.34        |\n",
      "|    value_loss           | 5.36e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 444         |\n",
      "|    time_elapsed         | 28643       |\n",
      "|    total_timesteps      | 909312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022760112 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.855      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.7e+03     |\n",
      "|    n_updates            | 4430        |\n",
      "|    policy_gradient_loss | -0.00608    |\n",
      "|    std                  | 0.34        |\n",
      "|    value_loss           | 5.04e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=-50217.69 +/- 8.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 910000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021468963 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.861      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.67e+03    |\n",
      "|    n_updates            | 4440        |\n",
      "|    policy_gradient_loss | -0.00844    |\n",
      "|    std                  | 0.343       |\n",
      "|    value_loss           | 5.25e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 445       |\n",
      "|    time_elapsed    | 28742     |\n",
      "|    total_timesteps | 911360    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 446         |\n",
      "|    time_elapsed         | 28785       |\n",
      "|    total_timesteps      | 913408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025419667 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.865      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.49e+03    |\n",
      "|    n_updates            | 4450        |\n",
      "|    policy_gradient_loss | -0.00766    |\n",
      "|    std                  | 0.342       |\n",
      "|    value_loss           | 4.96e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=915000, episode_reward=-50202.80 +/- 9.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 915000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023870725 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.861      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.05e+03    |\n",
      "|    n_updates            | 4460        |\n",
      "|    policy_gradient_loss | -0.00484    |\n",
      "|    std                  | 0.342       |\n",
      "|    value_loss           | 4.94e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 447       |\n",
      "|    time_elapsed    | 28883     |\n",
      "|    total_timesteps | 915456    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 448         |\n",
      "|    time_elapsed         | 28926       |\n",
      "|    total_timesteps      | 917504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027418979 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.865      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.3e+03     |\n",
      "|    n_updates            | 4470        |\n",
      "|    policy_gradient_loss | 1.14e-07    |\n",
      "|    std                  | 0.344       |\n",
      "|    value_loss           | 5.05e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 449         |\n",
      "|    time_elapsed         | 28971       |\n",
      "|    total_timesteps      | 919552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026164243 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.879      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.88e+03    |\n",
      "|    n_updates            | 4480        |\n",
      "|    policy_gradient_loss | 0.000391    |\n",
      "|    std                  | 0.344       |\n",
      "|    value_loss           | 4.9e+03     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=-50202.42 +/- 15.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 920000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028531577 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.902      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.35e+03    |\n",
      "|    n_updates            | 4490        |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    std                  | 0.347       |\n",
      "|    value_loss           | 4.69e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 450       |\n",
      "|    time_elapsed    | 29068     |\n",
      "|    total_timesteps | 921600    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 451         |\n",
      "|    time_elapsed         | 29112       |\n",
      "|    total_timesteps      | 923648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019203307 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.929      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.51e+03    |\n",
      "|    n_updates            | 4500        |\n",
      "|    policy_gradient_loss | -0.00846    |\n",
      "|    std                  | 0.351       |\n",
      "|    value_loss           | 4.5e+03     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=925000, episode_reward=-50210.34 +/- 7.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 925000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027112475 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.956      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 515         |\n",
      "|    n_updates            | 4510        |\n",
      "|    policy_gradient_loss | -0.00934    |\n",
      "|    std                  | 0.356       |\n",
      "|    value_loss           | 4.41e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 452       |\n",
      "|    time_elapsed    | 29210     |\n",
      "|    total_timesteps | 925696    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 31         |\n",
      "|    iterations           | 453        |\n",
      "|    time_elapsed         | 29253      |\n",
      "|    total_timesteps      | 927744     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05794379 |\n",
      "|    clip_fraction        | 0.384      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.975     |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 2.92e+03   |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | 0.0321     |\n",
      "|    std                  | 0.355      |\n",
      "|    value_loss           | 9.07e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 31         |\n",
      "|    iterations           | 454        |\n",
      "|    time_elapsed         | 29296      |\n",
      "|    total_timesteps      | 929792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03359952 |\n",
      "|    clip_fraction        | 0.251      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.966     |\n",
      "|    explained_variance   | 0.999      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.67e+03   |\n",
      "|    n_updates            | 4530       |\n",
      "|    policy_gradient_loss | -0.00404   |\n",
      "|    std                  | 0.354      |\n",
      "|    value_loss           | 4.61e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=-50191.14 +/- 13.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 930000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016754095 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.983      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.62e+03    |\n",
      "|    n_updates            | 4540        |\n",
      "|    policy_gradient_loss | -0.00783    |\n",
      "|    std                  | 0.358       |\n",
      "|    value_loss           | 4.05e+03    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 455       |\n",
      "|    time_elapsed    | 29393     |\n",
      "|    total_timesteps | 931840    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 31         |\n",
      "|    iterations           | 456        |\n",
      "|    time_elapsed         | 29437      |\n",
      "|    total_timesteps      | 933888     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01882019 |\n",
      "|    clip_fraction        | 0.224      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.995     |\n",
      "|    explained_variance   | 0.999      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.89e+03   |\n",
      "|    n_updates            | 4550       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.358      |\n",
      "|    value_loss           | 3.92e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=935000, episode_reward=-50215.78 +/- 8.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 935000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020430366 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.29e+03    |\n",
      "|    n_updates            | 4560        |\n",
      "|    policy_gradient_loss | -0.00575    |\n",
      "|    std                  | 0.359       |\n",
      "|    value_loss           | 3.86e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 457       |\n",
      "|    time_elapsed    | 29536     |\n",
      "|    total_timesteps | 935936    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 458         |\n",
      "|    time_elapsed         | 29580       |\n",
      "|    total_timesteps      | 937984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022450069 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 575         |\n",
      "|    n_updates            | 4570        |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    std                  | 0.359       |\n",
      "|    value_loss           | 3.89e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=-50217.29 +/- 13.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 940000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029335218 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.98e+03    |\n",
      "|    n_updates            | 4580        |\n",
      "|    policy_gradient_loss | 0.00654     |\n",
      "|    std                  | 0.36        |\n",
      "|    value_loss           | 4.61e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 459       |\n",
      "|    time_elapsed    | 29677     |\n",
      "|    total_timesteps | 940032    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 460         |\n",
      "|    time_elapsed         | 29721       |\n",
      "|    total_timesteps      | 942080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025474388 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 845         |\n",
      "|    n_updates            | 4590        |\n",
      "|    policy_gradient_loss | -0.00823    |\n",
      "|    std                  | 0.363       |\n",
      "|    value_loss           | 3.7e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 461         |\n",
      "|    time_elapsed         | 29766       |\n",
      "|    total_timesteps      | 944128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027578589 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.72e+03    |\n",
      "|    n_updates            | 4600        |\n",
      "|    policy_gradient_loss | -0.00727    |\n",
      "|    std                  | 0.368       |\n",
      "|    value_loss           | 3.53e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=945000, episode_reward=-50212.22 +/- 7.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 945000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022337254 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.43e+03    |\n",
      "|    n_updates            | 4610        |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    std                  | 0.365       |\n",
      "|    value_loss           | 3.6e+03     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 462       |\n",
      "|    time_elapsed    | 29864     |\n",
      "|    total_timesteps | 946176    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 463         |\n",
      "|    time_elapsed         | 29906       |\n",
      "|    total_timesteps      | 948224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026216485 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.39e+03    |\n",
      "|    n_updates            | 4620        |\n",
      "|    policy_gradient_loss | 0.0126      |\n",
      "|    std                  | 0.366       |\n",
      "|    value_loss           | 1.84e+04    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=-50193.57 +/- 7.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 950000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022010967 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 855         |\n",
      "|    n_updates            | 4630        |\n",
      "|    policy_gradient_loss | -0.0088     |\n",
      "|    std                  | 0.364       |\n",
      "|    value_loss           | 3.25e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 464       |\n",
      "|    time_elapsed    | 30004     |\n",
      "|    total_timesteps | 950272    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 465         |\n",
      "|    time_elapsed         | 30048       |\n",
      "|    total_timesteps      | 952320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020876437 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.54e+03    |\n",
      "|    n_updates            | 4640        |\n",
      "|    policy_gradient_loss | -0.00738    |\n",
      "|    std                  | 0.365       |\n",
      "|    value_loss           | 3.18e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 466         |\n",
      "|    time_elapsed         | 30091       |\n",
      "|    total_timesteps      | 954368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026646007 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.2e+03     |\n",
      "|    n_updates            | 4650        |\n",
      "|    policy_gradient_loss | -0.00393    |\n",
      "|    std                  | 0.362       |\n",
      "|    value_loss           | 3.09e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=955000, episode_reward=-50202.84 +/- 4.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 955000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02396217 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.85e+03   |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.00813   |\n",
      "|    std                  | 0.358      |\n",
      "|    value_loss           | 2.97e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 467       |\n",
      "|    time_elapsed    | 30189     |\n",
      "|    total_timesteps | 956416    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 468         |\n",
      "|    time_elapsed         | 30232       |\n",
      "|    total_timesteps      | 958464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022313317 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 921         |\n",
      "|    n_updates            | 4670        |\n",
      "|    policy_gradient_loss | -0.00593    |\n",
      "|    std                  | 0.363       |\n",
      "|    value_loss           | 2.9e+03     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=-50206.47 +/- 13.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032066606 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.38e+03    |\n",
      "|    n_updates            | 4680        |\n",
      "|    policy_gradient_loss | -0.00194    |\n",
      "|    std                  | 0.363       |\n",
      "|    value_loss           | 2.8e+03     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 469       |\n",
      "|    time_elapsed    | 30331     |\n",
      "|    total_timesteps | 960512    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 470         |\n",
      "|    time_elapsed         | 30375       |\n",
      "|    total_timesteps      | 962560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022307176 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 986         |\n",
      "|    n_updates            | 4690        |\n",
      "|    policy_gradient_loss | -0.00523    |\n",
      "|    std                  | 0.366       |\n",
      "|    value_loss           | 2.77e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 471         |\n",
      "|    time_elapsed         | 30419       |\n",
      "|    total_timesteps      | 964608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042799354 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.4e+03     |\n",
      "|    n_updates            | 4700        |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    std                  | 0.367       |\n",
      "|    value_loss           | 3.19e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=965000, episode_reward=-50210.49 +/- 9.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 965000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025989003 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.57e+03    |\n",
      "|    n_updates            | 4710        |\n",
      "|    policy_gradient_loss | 0.000656    |\n",
      "|    std                  | 0.367       |\n",
      "|    value_loss           | 2.9e+03     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 472       |\n",
      "|    time_elapsed    | 30518     |\n",
      "|    total_timesteps | 966656    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 473         |\n",
      "|    time_elapsed         | 30562       |\n",
      "|    total_timesteps      | 968704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026050137 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.41e+03    |\n",
      "|    n_updates            | 4720        |\n",
      "|    policy_gradient_loss | -0.00402    |\n",
      "|    std                  | 0.368       |\n",
      "|    value_loss           | 2.66e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=-50210.80 +/- 15.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 970000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03119827 |\n",
      "|    clip_fraction        | 0.269      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.38e+03   |\n",
      "|    n_updates            | 4730       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.367      |\n",
      "|    value_loss           | 2.47e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 474       |\n",
      "|    time_elapsed    | 30662     |\n",
      "|    total_timesteps | 970752    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 475         |\n",
      "|    time_elapsed         | 30705       |\n",
      "|    total_timesteps      | 972800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019472355 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 789         |\n",
      "|    n_updates            | 4740        |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 0.366       |\n",
      "|    value_loss           | 2.36e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 31         |\n",
      "|    iterations           | 476        |\n",
      "|    time_elapsed         | 30748      |\n",
      "|    total_timesteps      | 974848     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02423846 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 507        |\n",
      "|    n_updates            | 4750       |\n",
      "|    policy_gradient_loss | -0.00644   |\n",
      "|    std                  | 0.36       |\n",
      "|    value_loss           | 2.3e+03    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=975000, episode_reward=-50208.58 +/- 11.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 975000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050232902 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.26e+03    |\n",
      "|    n_updates            | 4760        |\n",
      "|    policy_gradient_loss | 0.0138      |\n",
      "|    std                  | 0.361       |\n",
      "|    value_loss           | 2.64e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 477       |\n",
      "|    time_elapsed    | 30847     |\n",
      "|    total_timesteps | 976896    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 31         |\n",
      "|    iterations           | 478        |\n",
      "|    time_elapsed         | 30890      |\n",
      "|    total_timesteps      | 978944     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17709708 |\n",
      "|    clip_fraction        | 0.514      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.941      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 8.32e+04   |\n",
      "|    n_updates            | 4770       |\n",
      "|    policy_gradient_loss | 0.16       |\n",
      "|    std                  | 0.36       |\n",
      "|    value_loss           | 9.44e+04   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=-50202.72 +/- 9.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 980000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05503515 |\n",
      "|    clip_fraction        | 0.374      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 840        |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | 0.018      |\n",
      "|    std                  | 0.361      |\n",
      "|    value_loss           | 4.38e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 479       |\n",
      "|    time_elapsed    | 30988     |\n",
      "|    total_timesteps | 980992    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 31         |\n",
      "|    iterations           | 480        |\n",
      "|    time_elapsed         | 31031      |\n",
      "|    total_timesteps      | 983040     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02290222 |\n",
      "|    clip_fraction        | 0.25       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.19e+03   |\n",
      "|    n_updates            | 4790       |\n",
      "|    policy_gradient_loss | -0.00874   |\n",
      "|    std                  | 0.363      |\n",
      "|    value_loss           | 2.38e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=985000, episode_reward=-50210.23 +/- 15.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 985000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031644695 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.26e+03    |\n",
      "|    n_updates            | 4800        |\n",
      "|    policy_gradient_loss | 0.00113     |\n",
      "|    std                  | 0.368       |\n",
      "|    value_loss           | 2.12e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 481       |\n",
      "|    time_elapsed    | 31130     |\n",
      "|    total_timesteps | 985088    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 482         |\n",
      "|    time_elapsed         | 31174       |\n",
      "|    total_timesteps      | 987136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021240892 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 754         |\n",
      "|    n_updates            | 4810        |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    std                  | 0.372       |\n",
      "|    value_loss           | 2.05e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 483         |\n",
      "|    time_elapsed         | 31217       |\n",
      "|    total_timesteps      | 989184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027481226 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 454         |\n",
      "|    n_updates            | 4820        |\n",
      "|    policy_gradient_loss | -0.00662    |\n",
      "|    std                  | 0.372       |\n",
      "|    value_loss           | 2.13e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=-50210.84 +/- 15.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 990000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026608031 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.23e+03    |\n",
      "|    n_updates            | 4830        |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    std                  | 0.373       |\n",
      "|    value_loss           | 2.02e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 484       |\n",
      "|    time_elapsed    | 31316     |\n",
      "|    total_timesteps | 991232    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 485         |\n",
      "|    time_elapsed         | 31359       |\n",
      "|    total_timesteps      | 993280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025136441 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.4e+03     |\n",
      "|    n_updates            | 4840        |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    std                  | 0.372       |\n",
      "|    value_loss           | 1.91e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=995000, episode_reward=-50204.83 +/- 12.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 995000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020503638 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.15e+03    |\n",
      "|    n_updates            | 4850        |\n",
      "|    policy_gradient_loss | -0.00995    |\n",
      "|    std                  | 0.374       |\n",
      "|    value_loss           | 1.88e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 486       |\n",
      "|    time_elapsed    | 31459     |\n",
      "|    total_timesteps | 995328    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 31         |\n",
      "|    iterations           | 487        |\n",
      "|    time_elapsed         | 31503      |\n",
      "|    total_timesteps      | 997376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01963307 |\n",
      "|    clip_fraction        | 0.218      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.14      |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.04e+03   |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | -0.00658   |\n",
      "|    std                  | 0.37       |\n",
      "|    value_loss           | 1.88e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 488         |\n",
      "|    time_elapsed         | 31546       |\n",
      "|    total_timesteps      | 999424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018704627 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.29e+03    |\n",
      "|    n_updates            | 4870        |\n",
      "|    policy_gradient_loss | -0.00676    |\n",
      "|    std                  | 0.367       |\n",
      "|    value_loss           | 1.77e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=-50191.85 +/- 7.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022009078 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 812         |\n",
      "|    n_updates            | 4880        |\n",
      "|    policy_gradient_loss | -0.00938    |\n",
      "|    std                  | 0.363       |\n",
      "|    value_loss           | 1.71e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 31        |\n",
      "|    iterations      | 489       |\n",
      "|    time_elapsed    | 31644     |\n",
      "|    total_timesteps | 1001472   |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simple DQN architecture\n",
    "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
    "\n",
    "# Set up the model with simpler hyperparameters\n",
    "model = PPO('CnnPolicy', custom_env, policy_kwargs=policy_kwargs, \n",
    "            verbose=1, learning_rate=0.0005, batch_size=32)\n",
    "\n",
    "# Setup evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(custom_env, best_model_save_path=model_dir,\n",
    "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
    "                                         name_prefix='ppo_model_checkpoint')\n",
    "\n",
    "\n",
    "\n",
    "# Start training the model with callbacks for evaluation and checkpoints\n",
    "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the final model after training\n",
    "model.save(\"ppo_custom_env_model\")\n",
    "custom_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "del env\n",
    "foo = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust number of episodes based on the environment's characteristics\n",
    "if hasattr(env, \"max_episode_steps\"):\n",
    "    # If the environment has predefined max steps, use a higher number for evaluation\n",
    "    num_episodes = 50  \n",
    "else:\n",
    "    # For simpler environments, use fewer episodes\n",
    "    num_episodes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = foo #load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "episode_rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = model.predict(obs)  # Use trained policy\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        record_agent_dynamics(env)  # Record smoothness metrics\n",
    "        if done:\n",
    "            break\n",
    "    episode_rewards.append(total_reward)\n",
    "    obs = env.reset()\n",
    "\n",
    "print(f\"Average Reward: {np.mean(episode_rewards)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gravar os video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(env, model, save_path):\n",
    "    # Video recorder wrapper\n",
    "    trigger = lambda t: t == 0\n",
    "    env = RecordVideo(\n",
    "        env,\n",
    "        video_folder=save_path,\n",
    "        episode_trigger=trigger,\n",
    "        video_length=0,\n",
    "        disable_logger=True,\n",
    "    )\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    trunc = False\n",
    "    while not trunc:\n",
    "        # pass observation to model to get predicted action\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "\n",
    "        # pass action to env and get info back\n",
    "        obs, rewards, trunc, done, info = env.step(action)\n",
    "\n",
    "        # show the environment on the screen\n",
    "        env.render()\n",
    "\n",
    "    # Close the Environment\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psuper/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.77GB > 0.74GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running episode 0\n"
     ]
    }
   ],
   "source": [
    "# DQN original\n",
    "\n",
    "# Define a Environment\n",
    "env = gym.make(\"CarRacing-v3\", continuous=False, render_mode='rgb_array') \n",
    "\n",
    "model = DQN.load(path=\"models/baseline/DQN/best_model.zip\", env=env)\n",
    "\n",
    "record_video(env, model, \"./recordings/original/DQN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psuper/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/psuper/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/reinforcement-learning-with-gymnasium/recordings/original/DQN folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EnhancedCarRacing' object has no attribute 'scroll'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# env = TimeLimit(env, max_episode_steps=1000)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m DQN\u001b[38;5;241m.\u001b[39mload(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/custom/DQN/best_model.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, env\u001b[38;5;241m=\u001b[39menv)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mrecord_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./recordings/original/DQN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mrecord_video\u001b[0;34m(env, model, save_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m trigger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m t: t \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m env \u001b[38;5;241m=\u001b[39m RecordVideo(\n\u001b[1;32m      5\u001b[0m     env,\n\u001b[1;32m      6\u001b[0m     video_folder\u001b[38;5;241m=\u001b[39msave_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     disable_logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m obs, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m trunc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trunc:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# pass observation to model to get predicted action\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:340\u001b[0m, in \u001b[0;36mRecordVideo.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_recording(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-episode-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_length:\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_recording()\n",
      "File \u001b[0;32m~/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:312\u001b[0m, in \u001b[0;36mRecordVideo._capture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_capture_frame\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot capture a frame, recording wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt started.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 312\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(frame, List):\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frame) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# render was called\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/gymnasium/core.py:332\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/reinforcement-learning-with-gymnasium/custom_cr.py:225\u001b[0m, in \u001b[0;36mEnhancedCarRacing.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_obstacles_to_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/reinforcement-learning-with-gymnasium/custom_cr.py:213\u001b[0m, in \u001b[0;36mEnhancedCarRacing.add_obstacles_to_observation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_obstacles_to_observation\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobstacles:\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;66;03m# Convert world coordinates to pixel coordinates\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m         pixel_x, pixel_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworld_to_pixel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m pixel_x \u001b[38;5;241m<\u001b[39m observation\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m pixel_y \u001b[38;5;241m<\u001b[39m observation\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    215\u001b[0m             observation[pixel_y\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:pixel_y\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m, pixel_x\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:pixel_x\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/reinforcement-learning-with-gymnasium/custom_cr.py:219\u001b[0m, in \u001b[0;36mEnhancedCarRacing.world_to_pixel\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mworld_to_pixel\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[0;32m--> 219\u001b[0m     pixel_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((x \u001b[38;5;241m-\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscroll\u001b[49m[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale)\n\u001b[1;32m    220\u001b[0m     pixel_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((y \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscroll[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale)\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pixel_x, pixel_y\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EnhancedCarRacing' object has no attribute 'scroll'"
     ]
    }
   ],
   "source": [
    "# DQN custom\n",
    "\n",
    "# Define a Environment\n",
    "env = EnhancedCarRacing(render_mode=\"rgb_array\")\n",
    "env = GrayscaleObservation(env, keep_dim=True)\n",
    "# env = TimeLimit(env, max_episode_steps=1000)\n",
    "\n",
    "model = DQN.load(path=\"models/custom/DQN/best_model.zip\", env=env)\n",
    "\n",
    "record_video(env, model, \"./recordings/original/DQN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:416: UserWarning: \u001b[33mWARN: Unable to save last video! Did you call close()?\u001b[0m\n",
      "  logger.warn(\"Unable to save last video! Did you call close()?\")\n",
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/recordings/custom/PPO folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n",
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(5)\n",
      "Box([-1.  0.  0.], 1.0, (3,), float32)\n",
      "Running episode 0\n"
     ]
    },
    {
     "ename": "InvalidAction",
     "evalue": "you passed the invalid action `(array([0, 0, 0]), <class 'numpy.ndarray'>)`. The supported action_space is `Discrete(5)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidAction\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# pass action to env and get info back\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m obs, rewards, trunc, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# show the environment on the screen\u001b[39;00m\n\u001b[1;32m     36\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:350\u001b[0m, in \u001b[0;36mRecordVideo.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    348\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    349\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment using action, recording observations if :attr:`self.recording`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     obs, rew, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_trigger \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_trigger(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_id):\n",
      "File \u001b[0;32m~/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/gymnasium/core.py:550\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    548\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/3ano1sem/isia/reinforcement-learning-with-gymnasium/custom_cr.py:76\u001b[0m, in \u001b[0;36mEnhancedCarRacing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 76\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# Get car position\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     car_position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_car_position(observation)\n",
      "File \u001b[0;32m~/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/gymnasium/envs/box2d/car_racing.py:551\u001b[0m, in \u001b[0;36mCarRacing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mcontains(action):\n\u001b[0;32m--> 551\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidAction(\n\u001b[1;32m    552\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou passed the invalid action `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mtype\u001b[39m(action)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    553\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe supported action_space is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    554\u001b[0m         )\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39msteer(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.6\u001b[39m \u001b[38;5;241m*\u001b[39m (action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.6\u001b[39m \u001b[38;5;241m*\u001b[39m (action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39mgas(\u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m (action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m))\n",
      "\u001b[0;31mInvalidAction\u001b[0m: you passed the invalid action `(array([0, 0, 0]), <class 'numpy.ndarray'>)`. The supported action_space is `Discrete(5)`"
     ]
    }
   ],
   "source": [
    "# PPO custom\n",
    "\n",
    "# Define a Environment\n",
    "env = EnhancedCarRacing(render_mode=\"rgb_array\")\n",
    "env = GrayscaleObservation(env, keep_dim=True)\n",
    "print(env.action_space)\n",
    "# env = TimeLimit(env, max_episode_steps=1000)\n",
    "\n",
    "model = PPO.load(path=\"models/custom/PPO/best_model.zip\", env=env)\n",
    "\n",
    "record_video(env, model, \"./recordings/custom/PPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
