{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing OpenAI Gym Environments and Implementing Reinforcement Learning Agents with Stable Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme: Car Racing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Constança\n",
    "- Daniela Osório, 202208679\n",
    "- Inês Amorim, 202108108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import GrayscaleObservation, ResizeObservation, RecordEpisodeStatistics, RecordVideo, TimeLimit\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "import os\n",
    "import gc\n",
    "from eval import *\n",
    "from custom_cr import EnhancedCarRacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CarRacing-v3 environment from Gymnasium (previously Gym) is part of the Box2D environments, and it offers an interesting challenge for training reinforcement learning agents. It's a top-down racing simulation where the track is randomly generated at the start of each episode. The environment offers both continuous and discrete action spaces, making it adaptable to different types of reinforcement learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False, render_mode='rgb_array') \n",
    "obs, info = env.reset()\n",
    "#continuous = False to use Discrete space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Action Space:**\n",
    "\n",
    "   - **Continuous:** Three actions: steering, gas, and braking. Steering ranges from -1 (full left) to +1 (full right).\n",
    "   -  **Discrete:** Five possible actions: do nothing, steer left, steer right, gas, and brake.\n",
    "\n",
    "- **Observation Space:**\n",
    "\n",
    "    - The environment provides a 96x96 RGB image of the car and the track, which serves as the state input for the agent.\n",
    "\n",
    "- **Rewards:**\n",
    "\n",
    "    - The agent receives a -0.1 penalty for every frame, encouraging efficiency.\n",
    "    - It earns a positive reward for visiting track tiles: the formula is Reward=1000−0.1×framesReward=1000−0.1×frames, where \"frames\" is the number of frames taken to complete the lap. The reward for completing a lap depends on how many track tiles are visited.\n",
    "\n",
    "- **Episode Termination:**\n",
    "\n",
    "    - The episode ends either when all track tiles are visited or if the car goes off the track, which incurs a significant penalty (-100 reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'rgb_array', 'state_pixels']\n"
     ]
    }
   ],
   "source": [
    "#check render modes\n",
    "print(env.metadata[\"render_modes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking if everything is okay and working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment and render the first frame\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "print(\"Environment initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(5)\n",
      "Observation Space: Box(0, 255, (96, 96, 3), uint8)\n",
      "Environment Metadata: {'render_modes': ['human', 'rgb_array', 'state_pixels'], 'render_fps': 50}\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Environment Metadata:\", env.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(10):\n",
    "    \"\"\"action = env.action_space.sample()  # Random action\n",
    "    print(f\"Action before step: {action}, Type: {type(action)}\")\n",
    "    obs, reward, done, info = env.step(action)\"\"\"\n",
    "    env.step(env.action_space.sample())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-Learning (DQN) is a reinforcement learning algorithm that extends the traditional Q-Learning method using neural networks to approximate the Q-values for state-action pairs.\n",
    "DQN is inherently designed for discrete action spaces, as the neural network outputs a separate Q-value for each action. For each state, the algorithm selects actions based on the highest Q-value, making it ideal for problems where actions are discrete and finite. This is a key advantage compared to other reinforcement learning methods, which may require modifications or different approaches for discrete action selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = '../models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvSpec(id='CarRacing-v3', entry_point='gymnasium.envs.box2d.car_racing:CarRacing', reward_threshold=900, nondeterministic=False, max_episode_steps=1000, order_enforce=True, disable_env_checker=False, kwargs={'continuous': False}, namespace=None, name='CarRacing', version=3, additional_wrappers=(), vector_entry_point=None)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False)\n",
    "print(env.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(obs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'DQN_baseline_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "video_dir = os.path.join(logs_path, \"videos\")\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple DQN architecture\n",
    "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
    "\n",
    "# Set up the model with simpler hyperparameters\n",
    "model = DQN('CnnPolicy', env, policy_kwargs=policy_kwargs, \n",
    "            verbose=1, buffer_size=50000, learning_starts=1000, \n",
    "            learning_rate=0.0005, batch_size=32, exploration_fraction=0.1,\n",
    "            exploration_final_eps=0.05)\n",
    "\n",
    "# Setup evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(env, best_model_save_path=model_dir,\n",
    "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
    "                                         name_prefix='dqn_model_checkpoint')\n",
    "\n",
    "\n",
    "\n",
    "# Start training the model with callbacks for evaluation and checkpoints\n",
    "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the final model after training\n",
    "model.save(\"dqn_car_racing_model\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "del env\n",
    "foo = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'PPO_baseline_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "video_dir = os.path.join(logs_path, \"videos\")\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple DQN architecture\n",
    "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
    "\n",
    "# Set up the model with simpler hyperparameters\n",
    "model = PPO('CnnPolicy', env, policy_kwargs=policy_kwargs, \n",
    "            verbose=1, learning_rate=0.0005, batch_size=32)\n",
    "\n",
    "# Setup evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(env, best_model_save_path=model_dir,\n",
    "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
    "                                         name_prefix='ppo_model_checkpoint')\n",
    "\n",
    "\n",
    "\n",
    "# Start training the model with callbacks for evaluation and checkpoints\n",
    "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the final model after training\n",
    "model.save(\"ppo_env_model\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "del env\n",
    "foo = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Environment and Reward Modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Modification**            | **Description**                                                                                      | **Effect**                                     |\n",
    "|-----------------------------|------------------------------------------------------------------------------------------------------|-----------------------------------------------|\n",
    "| **Obstacles**               | Randomly placed obstacles on the track.                                                             | Requires avoidance and navigation skills.     |\n",
    "| **Track Width Variability** | Random track width adjustments between `[0.8, 1.2]`.                                                | Simulates narrow/wide tracks dynamically.     |\n",
    "| **Weather Conditions**      | Introduces \"rain\" and \"snow,\" which alter action effectiveness.                                     | Adds randomness and realism to driving.       |\n",
    "| **Off-Track Penalty**       | Reward reduced by `-10` if the car leaves the track.                                                | Encourages the agent to stay on track.        |\n",
    "| **Distance Reward**         | Positive reward based on the distance traveled per step.                                            | Incentivizes efficient driving.               |\n",
    "| **Obstacle Proximity Penalty** | Penalty inversely proportional to the distance to nearby obstacles (`1 / (d + 1e-6)`).             | Encourages the car to avoid obstacles safely. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observation shape: (96, 96, 3)\n",
      "Initial info: {}\n",
      "\n",
      "Step 1:\n",
      "Action taken: 0\n",
      "Reward: 5.603055423505158\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: snow\n",
      "\n",
      "Step 2:\n",
      "Action taken: 0\n",
      "Reward: -0.3278483665823053\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: snow\n",
      "\n",
      "Step 3:\n",
      "Action taken: 0\n",
      "Reward: -0.4278483665823053\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: snow\n",
      "\n",
      "Step 4:\n",
      "Action taken: 3\n",
      "Reward: -0.5278483665823053\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: snow\n",
      "\n",
      "Step 5:\n",
      "Action taken: 2\n",
      "Reward: -0.6278483665823053\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: snow\n",
      "\n",
      "Step 6:\n",
      "Action taken: 2\n",
      "Reward: -0.7278483665823052\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: snow\n",
      "\n",
      "Step 7:\n",
      "Action taken: 0\n",
      "Reward: -0.8278483665823052\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: snow\n",
      "\n",
      "Step 8:\n",
      "Action taken: 4\n",
      "Reward: -0.9278483665823052\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: snow\n",
      "\n",
      "Step 9:\n",
      "Action taken: 4\n",
      "Reward: -1.0278483665823053\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: snow\n",
      "\n",
      "Step 10:\n",
      "Action taken: 3\n",
      "Reward: -1.1278483665823051\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: snow\n"
     ]
    }
   ],
   "source": [
    "custom_env = EnhancedCarRacing(render_mode=\"rgb_array\")\n",
    "obs, info = custom_env.reset()\n",
    "\n",
    "print(\"Initial observation shape:\", obs.shape)\n",
    "print(\"Initial info:\", info)\n",
    "\n",
    "for i in range(10):\n",
    "    action = custom_env.action_space.sample()  # Your agent would make a decision here\n",
    "    observation, reward, terminated, truncated, info = custom_env.step(action)\n",
    "    print(f\"\\nStep {i+1}:\")\n",
    "    print(\"Action taken:\", action)\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"Terminated:\", terminated)\n",
    "    print(\"Truncated:\", truncated)\n",
    "    print(\"Info:\", info)\n",
    "\n",
    "    if hasattr(custom_env, 'weather_condition'):\n",
    "        print(\"Current weather:\", custom_env.weather_condition)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        print(\"Episode ended\")\n",
    "        break\n",
    "\n",
    "custom_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env = GrayscaleObservation(custom_env, keep_dim=True)\n",
    "custom_env = TimeLimit(custom_env, max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TimeLimit<GrayscaleObservation<EnhancedCarRacing instance>>>\n"
     ]
    }
   ],
   "source": [
    "print(custom_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'DQN_env_mod_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7e863829a7a0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7e863e527fa0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.962     |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 28        |\n",
      "|    time_elapsed     | 141       |\n",
      "|    total_timesteps  | 4000      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.12      |\n",
      "|    n_updates        | 749       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=-50215.65 +/- 9.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.953     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 5000      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.93      |\n",
      "|    n_updates        | 999       |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.924     |\n",
      "| time/               |           |\n",
      "|    episodes         | 8         |\n",
      "|    fps              | 17        |\n",
      "|    time_elapsed     | 463       |\n",
      "|    total_timesteps  | 8000      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.19      |\n",
      "|    n_updates        | 1749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-50167.65 +/- 36.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.905     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 10000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.34      |\n",
      "|    n_updates        | 2249      |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.886     |\n",
      "| time/               |           |\n",
      "|    episodes         | 12        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 750       |\n",
      "|    total_timesteps  | 12000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.74      |\n",
      "|    n_updates        | 2749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-50211.32 +/- 8.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.858     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 15000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.64      |\n",
      "|    n_updates        | 3499      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.848     |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 14        |\n",
      "|    time_elapsed     | 1127      |\n",
      "|    total_timesteps  | 16000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.41      |\n",
      "|    n_updates        | 3749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-50201.12 +/- 9.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.81      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 20000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 1.52      |\n",
      "|    n_updates        | 4749      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.81      |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 1526      |\n",
      "|    total_timesteps  | 20000     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.772     |\n",
      "| time/               |           |\n",
      "|    episodes         | 24        |\n",
      "|    fps              | 14        |\n",
      "|    time_elapsed     | 1682      |\n",
      "|    total_timesteps  | 24000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.47      |\n",
      "|    n_updates        | 5749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-50205.71 +/- 9.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.763     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 25000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.99      |\n",
      "|    n_updates        | 5999      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.734     |\n",
      "| time/               |           |\n",
      "|    episodes         | 28        |\n",
      "|    fps              | 14        |\n",
      "|    time_elapsed     | 1909      |\n",
      "|    total_timesteps  | 28000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.69      |\n",
      "|    n_updates        | 6749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-33812.91 +/- 20146.91\n",
      "Episode length: 768.60 +/- 286.51\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 769       |\n",
      "|    mean_reward      | -3.38e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.715     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 30000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.51      |\n",
      "|    n_updates        | 7249      |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.696     |\n",
      "| time/               |           |\n",
      "|    episodes         | 32        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 2111      |\n",
      "|    total_timesteps  | 32000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.8       |\n",
      "|    n_updates        | 7749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-50206.97 +/- 7.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.668     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 35000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.63      |\n",
      "|    n_updates        | 8499      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.658     |\n",
      "| time/               |           |\n",
      "|    episodes         | 36        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 2339      |\n",
      "|    total_timesteps  | 36000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.92      |\n",
      "|    n_updates        | 8749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-50201.85 +/- 12.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.62      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 40000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.66      |\n",
      "|    n_updates        | 9749      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.62      |\n",
      "| time/               |           |\n",
      "|    episodes         | 40        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 2567      |\n",
      "|    total_timesteps  | 40000     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.582     |\n",
      "| time/               |           |\n",
      "|    episodes         | 44        |\n",
      "|    fps              | 16        |\n",
      "|    time_elapsed     | 2696      |\n",
      "|    total_timesteps  | 44000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.39      |\n",
      "|    n_updates        | 10749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-50172.30 +/- 43.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.573     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 45000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.68      |\n",
      "|    n_updates        | 10999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.544     |\n",
      "| time/               |           |\n",
      "|    episodes         | 48        |\n",
      "|    fps              | 16        |\n",
      "|    time_elapsed     | 2971      |\n",
      "|    total_timesteps  | 48000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4         |\n",
      "|    n_updates        | 11749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-50216.08 +/- 13.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.525     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 50000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.76      |\n",
      "|    n_updates        | 12249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.506     |\n",
      "| time/               |           |\n",
      "|    episodes         | 52        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 3257      |\n",
      "|    total_timesteps  | 52000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 6         |\n",
      "|    n_updates        | 12749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-50112.29 +/- 20.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.01e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.478     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 55000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.88      |\n",
      "|    n_updates        | 13499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.468     |\n",
      "| time/               |           |\n",
      "|    episodes         | 56        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 3561      |\n",
      "|    total_timesteps  | 56000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.84      |\n",
      "|    n_updates        | 13749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-50207.37 +/- 12.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.43      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 60000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.37      |\n",
      "|    n_updates        | 14749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.43      |\n",
      "| time/               |           |\n",
      "|    episodes         | 60        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 3874      |\n",
      "|    total_timesteps  | 60000     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.392     |\n",
      "| time/               |           |\n",
      "|    episodes         | 64        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 4087      |\n",
      "|    total_timesteps  | 64000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 7.42      |\n",
      "|    n_updates        | 15749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-50208.35 +/- 11.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.383     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 65000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.9       |\n",
      "|    n_updates        | 15999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.354     |\n",
      "| time/               |           |\n",
      "|    episodes         | 68        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 4352      |\n",
      "|    total_timesteps  | 68000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.87      |\n",
      "|    n_updates        | 16749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-50192.87 +/- 21.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.335     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 70000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.99      |\n",
      "|    n_updates        | 17249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.316     |\n",
      "| time/               |           |\n",
      "|    episodes         | 72        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 4582      |\n",
      "|    total_timesteps  | 72000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.44      |\n",
      "|    n_updates        | 17749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-50206.63 +/- 16.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.288     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 75000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.2       |\n",
      "|    n_updates        | 18499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.278     |\n",
      "| time/               |           |\n",
      "|    episodes         | 76        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 4837      |\n",
      "|    total_timesteps  | 76000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.99      |\n",
      "|    n_updates        | 18749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-50205.96 +/- 11.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.24      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 80000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.08      |\n",
      "|    n_updates        | 19749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.24      |\n",
      "| time/               |           |\n",
      "|    episodes         | 80        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 5146      |\n",
      "|    total_timesteps  | 80000     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.202     |\n",
      "| time/               |           |\n",
      "|    episodes         | 84        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 5345      |\n",
      "|    total_timesteps  | 84000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.51      |\n",
      "|    n_updates        | 20749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-50171.45 +/- 25.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.193     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 85000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.73      |\n",
      "|    n_updates        | 20999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.164     |\n",
      "| time/               |           |\n",
      "|    episodes         | 88        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 5611      |\n",
      "|    total_timesteps  | 88000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.12      |\n",
      "|    n_updates        | 21749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-50212.26 +/- 4.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.145     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 90000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.17      |\n",
      "|    n_updates        | 22249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.126     |\n",
      "| time/               |           |\n",
      "|    episodes         | 92        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 5915      |\n",
      "|    total_timesteps  | 92000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.2      |\n",
      "|    n_updates        | 22749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-50202.68 +/- 18.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.0975    |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 95000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.79      |\n",
      "|    n_updates        | 23499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.088     |\n",
      "| time/               |           |\n",
      "|    episodes         | 96        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 6232      |\n",
      "|    total_timesteps  | 96000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.39      |\n",
      "|    n_updates        | 23749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-8896.11 +/- 3242.86\n",
      "Episode length: 413.00 +/- 71.52\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 413      |\n",
      "|    mean_reward      | -8.9e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 6.79     |\n",
      "|    n_updates        | 24749    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 100       |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 6458      |\n",
      "|    total_timesteps  | 100000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 104       |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 6697      |\n",
      "|    total_timesteps  | 104000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.64      |\n",
      "|    n_updates        | 25749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-50208.89 +/- 7.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 105000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.3      |\n",
      "|    n_updates        | 25999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 108       |\n",
      "|    fps              | 14        |\n",
      "|    time_elapsed     | 7687      |\n",
      "|    total_timesteps  | 108000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.63      |\n",
      "|    n_updates        | 26749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-50201.73 +/- 18.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 110000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.6      |\n",
      "|    n_updates        | 27249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 8188     |\n",
      "|    total_timesteps  | 112000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 10       |\n",
      "|    n_updates        | 27749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-50202.84 +/- 10.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 115000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.98      |\n",
      "|    n_updates        | 28499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 8754     |\n",
      "|    total_timesteps  | 116000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 10.6     |\n",
      "|    n_updates        | 28749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-50211.57 +/- 13.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 120000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 6.73      |\n",
      "|    n_updates        | 29749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 9395     |\n",
      "|    total_timesteps  | 120000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 9793     |\n",
      "|    total_timesteps  | 124000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 11.5     |\n",
      "|    n_updates        | 30749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-50191.04 +/- 17.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 125000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.8      |\n",
      "|    n_updates        | 30999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 10314    |\n",
      "|    total_timesteps  | 128000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 5.79     |\n",
      "|    n_updates        | 31749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-50197.73 +/- 11.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 130000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.03      |\n",
      "|    n_updates        | 32249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 10866    |\n",
      "|    total_timesteps  | 131959   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 9.18     |\n",
      "|    n_updates        | 32739    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=-13544.02 +/- 2978.08\n",
      "Episode length: 514.40 +/- 61.34\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 514       |\n",
      "|    mean_reward      | -1.35e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 135000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.11      |\n",
      "|    n_updates        | 33499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 11222    |\n",
      "|    total_timesteps  | 136000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 8.33     |\n",
      "|    n_updates        | 33749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-50120.15 +/- 43.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.01e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 140000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9         |\n",
      "|    n_updates        | 34749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 11781    |\n",
      "|    total_timesteps  | 140000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 12117    |\n",
      "|    total_timesteps  | 144000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 4.79     |\n",
      "|    n_updates        | 35749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=-50199.28 +/- 11.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 145000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.4      |\n",
      "|    n_updates        | 35999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 12571    |\n",
      "|    total_timesteps  | 148000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 9.67     |\n",
      "|    n_updates        | 36749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-50188.72 +/- 9.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 150000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.68      |\n",
      "|    n_updates        | 37249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 12851    |\n",
      "|    total_timesteps  | 152000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.8     |\n",
      "|    n_updates        | 37749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=-50208.45 +/- 15.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 155000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 6.76      |\n",
      "|    n_updates        | 38499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 13089    |\n",
      "|    total_timesteps  | 156000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 6.42     |\n",
      "|    n_updates        | 38749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-50194.16 +/- 5.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 160000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.2      |\n",
      "|    n_updates        | 39749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 13326    |\n",
      "|    total_timesteps  | 160000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 13461    |\n",
      "|    total_timesteps  | 164000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.3     |\n",
      "|    n_updates        | 40749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=-50206.46 +/- 11.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 165000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.6      |\n",
      "|    n_updates        | 40999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 13755    |\n",
      "|    total_timesteps  | 168000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 6.07     |\n",
      "|    n_updates        | 41749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-50197.88 +/- 13.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 170000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.97      |\n",
      "|    n_updates        | 42249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 14065    |\n",
      "|    total_timesteps  | 172000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 11.3     |\n",
      "|    n_updates        | 42749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=-35280.44 +/- 18393.08\n",
      "Episode length: 800.20 +/- 247.88\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 800       |\n",
      "|    mean_reward      | -3.53e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 175000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.32      |\n",
      "|    n_updates        | 43499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 14309    |\n",
      "|    total_timesteps  | 176000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 7.75     |\n",
      "|    n_updates        | 43749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-5775.34 +/- 1716.65\n",
      "Episode length: 332.20 +/- 52.93\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 332       |\n",
      "|    mean_reward      | -5.78e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 180000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13        |\n",
      "|    n_updates        | 44749     |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 14518    |\n",
      "|    total_timesteps  | 180000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 14713    |\n",
      "|    total_timesteps  | 184000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.1     |\n",
      "|    n_updates        | 45749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-50205.64 +/- 15.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 185000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15        |\n",
      "|    n_updates        | 45999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 14969    |\n",
      "|    total_timesteps  | 188000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 7.79     |\n",
      "|    n_updates        | 46749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-50208.19 +/- 10.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 190000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 7.75      |\n",
      "|    n_updates        | 47249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 15277    |\n",
      "|    total_timesteps  | 192000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 9.87     |\n",
      "|    n_updates        | 47749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-12453.23 +/- 4103.91\n",
      "Episode length: 488.80 +/- 86.46\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 489       |\n",
      "|    mean_reward      | -1.25e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 195000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.31      |\n",
      "|    n_updates        | 48499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 15532    |\n",
      "|    total_timesteps  | 196000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 8.6      |\n",
      "|    n_updates        | 48749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-50193.29 +/- 9.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 200000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.5      |\n",
      "|    n_updates        | 49749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 15801    |\n",
      "|    total_timesteps  | 200000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 16023    |\n",
      "|    total_timesteps  | 204000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 10.4     |\n",
      "|    n_updates        | 50749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=-50207.50 +/- 6.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 205000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11        |\n",
      "|    n_updates        | 50999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 16343    |\n",
      "|    total_timesteps  | 208000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 9.61     |\n",
      "|    n_updates        | 51749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-50207.03 +/- 12.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 210000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.1      |\n",
      "|    n_updates        | 52249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 212       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 16594     |\n",
      "|    total_timesteps  | 212000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.8      |\n",
      "|    n_updates        | 52749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=-50199.78 +/- 8.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 215000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.1      |\n",
      "|    n_updates        | 53499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 216       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 16949     |\n",
      "|    total_timesteps  | 216000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 6.23      |\n",
      "|    n_updates        | 53749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-50209.80 +/- 12.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 220000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 7.95      |\n",
      "|    n_updates        | 54749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 220       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 17235     |\n",
      "|    total_timesteps  | 220000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 224       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 17476     |\n",
      "|    total_timesteps  | 224000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.7      |\n",
      "|    n_updates        | 55749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=-50203.18 +/- 9.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 225000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.8      |\n",
      "|    n_updates        | 55999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 228       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 17747     |\n",
      "|    total_timesteps  | 228000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.3      |\n",
      "|    n_updates        | 56749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-50210.61 +/- 10.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 230000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 7.12      |\n",
      "|    n_updates        | 57249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 232       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 18058     |\n",
      "|    total_timesteps  | 232000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.8      |\n",
      "|    n_updates        | 57749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=-50197.35 +/- 13.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 235000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 7.38      |\n",
      "|    n_updates        | 58499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 236       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 18404     |\n",
      "|    total_timesteps  | 236000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.2      |\n",
      "|    n_updates        | 58749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 999       |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 240       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 18631     |\n",
      "|    total_timesteps  | 239917    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.5      |\n",
      "|    n_updates        | 59729     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-50201.37 +/- 16.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 240000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.4      |\n",
      "|    n_updates        | 59749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 244       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 18947     |\n",
      "|    total_timesteps  | 244000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.88      |\n",
      "|    n_updates        | 60749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=-50140.61 +/- 14.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.01e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 245000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.4      |\n",
      "|    n_updates        | 60999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 248       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 19185     |\n",
      "|    total_timesteps  | 248000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.4      |\n",
      "|    n_updates        | 61749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=-50199.17 +/- 10.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 250000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.7      |\n",
      "|    n_updates        | 62249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 252       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 19424     |\n",
      "|    total_timesteps  | 252000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.7      |\n",
      "|    n_updates        | 62749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=-50196.72 +/- 12.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 255000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.1      |\n",
      "|    n_updates        | 63499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 256       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 19708     |\n",
      "|    total_timesteps  | 256000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.8      |\n",
      "|    n_updates        | 63749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-50212.10 +/- 19.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 260000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.7      |\n",
      "|    n_updates        | 64749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 260       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 20027     |\n",
      "|    total_timesteps  | 260000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 264       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 20238     |\n",
      "|    total_timesteps  | 264000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.25      |\n",
      "|    n_updates        | 65749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=-50192.29 +/- 23.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 265000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.3      |\n",
      "|    n_updates        | 65999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 268       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 20503     |\n",
      "|    total_timesteps  | 268000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.2      |\n",
      "|    n_updates        | 66749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=-50206.48 +/- 7.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 270000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.2      |\n",
      "|    n_updates        | 67249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 272       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 20809     |\n",
      "|    total_timesteps  | 272000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.3      |\n",
      "|    n_updates        | 67749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=-50203.23 +/- 14.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 275000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9         |\n",
      "|    n_updates        | 68499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 276       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 21096     |\n",
      "|    total_timesteps  | 276000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.21      |\n",
      "|    n_updates        | 68749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-50217.40 +/- 7.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 280000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 26        |\n",
      "|    n_updates        | 69749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 280       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 21458     |\n",
      "|    total_timesteps  | 280000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 284       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 21695     |\n",
      "|    total_timesteps  | 284000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.9      |\n",
      "|    n_updates        | 70749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=-50188.95 +/- 21.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 285000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.2      |\n",
      "|    n_updates        | 70999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 288       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 22075     |\n",
      "|    total_timesteps  | 288000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10        |\n",
      "|    n_updates        | 71749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-50205.28 +/- 8.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 290000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.7      |\n",
      "|    n_updates        | 72249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 292       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 22438     |\n",
      "|    total_timesteps  | 292000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.6      |\n",
      "|    n_updates        | 72749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=-50205.35 +/- 12.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 295000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.2      |\n",
      "|    n_updates        | 73499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 296       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 22801     |\n",
      "|    total_timesteps  | 296000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.8      |\n",
      "|    n_updates        | 73749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-50201.09 +/- 6.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 300000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.8      |\n",
      "|    n_updates        | 74749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 300       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 23161     |\n",
      "|    total_timesteps  | 300000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 304       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 23397     |\n",
      "|    total_timesteps  | 304000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.1      |\n",
      "|    n_updates        | 75749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=305000, episode_reward=-50206.24 +/- 8.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 305000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.3      |\n",
      "|    n_updates        | 75999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 308       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 23758     |\n",
      "|    total_timesteps  | 308000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.4      |\n",
      "|    n_updates        | 76749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=-50206.38 +/- 10.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 310000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.4      |\n",
      "|    n_updates        | 77249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 312       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 24120     |\n",
      "|    total_timesteps  | 312000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.26      |\n",
      "|    n_updates        | 77749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=315000, episode_reward=-50198.97 +/- 6.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 315000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.7      |\n",
      "|    n_updates        | 78499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 316       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 24485     |\n",
      "|    total_timesteps  | 316000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.8      |\n",
      "|    n_updates        | 78749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-50204.55 +/- 9.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 320000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.34      |\n",
      "|    n_updates        | 79749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 320       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 24848     |\n",
      "|    total_timesteps  | 320000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 324       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 25085     |\n",
      "|    total_timesteps  | 324000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.6      |\n",
      "|    n_updates        | 80749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=325000, episode_reward=-50201.25 +/- 12.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 325000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16        |\n",
      "|    n_updates        | 80999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 997       |\n",
      "|    ep_rew_mean      | -4.98e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 328       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 25433     |\n",
      "|    total_timesteps  | 327697    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.2      |\n",
      "|    n_updates        | 81674     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=-50213.95 +/- 4.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 330000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.6      |\n",
      "|    n_updates        | 82249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -4.99e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 332       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 25814     |\n",
      "|    total_timesteps  | 332000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.4      |\n",
      "|    n_updates        | 82749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=335000, episode_reward=-50207.87 +/- 8.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 335000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.8      |\n",
      "|    n_updates        | 83499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -4.99e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 336       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 26180     |\n",
      "|    total_timesteps  | 336000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.9      |\n",
      "|    n_updates        | 83749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-50192.00 +/- 22.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 340000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.8      |\n",
      "|    n_updates        | 84749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 26546    |\n",
      "|    total_timesteps  | 340000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 26785    |\n",
      "|    total_timesteps  | 344000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13       |\n",
      "|    n_updates        | 85749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=345000, episode_reward=-50208.73 +/- 11.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 345000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 27        |\n",
      "|    n_updates        | 85999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 27153    |\n",
      "|    total_timesteps  | 348000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.6     |\n",
      "|    n_updates        | 86749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=-50210.24 +/- 6.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 350000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.8      |\n",
      "|    n_updates        | 87249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 27516    |\n",
      "|    total_timesteps  | 352000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 14.5     |\n",
      "|    n_updates        | 87749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=355000, episode_reward=-50197.29 +/- 17.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 355000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.7      |\n",
      "|    n_updates        | 88499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 27880    |\n",
      "|    total_timesteps  | 356000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 16       |\n",
      "|    n_updates        | 88749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-50214.35 +/- 11.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 360000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 24.8      |\n",
      "|    n_updates        | 89749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 28245    |\n",
      "|    total_timesteps  | 360000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 28483    |\n",
      "|    total_timesteps  | 364000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.6     |\n",
      "|    n_updates        | 90749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=365000, episode_reward=-41878.43 +/- 4469.47\n",
      "Episode length: 911.80 +/- 48.47\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 912       |\n",
      "|    mean_reward      | -4.19e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 365000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.4      |\n",
      "|    n_updates        | 90999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 28834    |\n",
      "|    total_timesteps  | 368000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 11.6     |\n",
      "|    n_updates        | 91749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=-50216.02 +/- 11.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 370000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.4      |\n",
      "|    n_updates        | 92249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 29199    |\n",
      "|    total_timesteps  | 372000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 21.4     |\n",
      "|    n_updates        | 92749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=375000, episode_reward=-50195.37 +/- 11.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 375000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.1      |\n",
      "|    n_updates        | 93499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 29565    |\n",
      "|    total_timesteps  | 376000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 28.1     |\n",
      "|    n_updates        | 93749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-36103.28 +/- 6437.66\n",
      "Episode length: 844.00 +/- 73.12\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 844       |\n",
      "|    mean_reward      | -3.61e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 380000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.2      |\n",
      "|    n_updates        | 94749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 29909    |\n",
      "|    total_timesteps  | 380000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 30145    |\n",
      "|    total_timesteps  | 384000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 14.6     |\n",
      "|    n_updates        | 95749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=385000, episode_reward=-50204.37 +/- 15.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 385000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.7      |\n",
      "|    n_updates        | 95999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 30509    |\n",
      "|    total_timesteps  | 388000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 12.2     |\n",
      "|    n_updates        | 96749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=-50209.47 +/- 12.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 390000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.9      |\n",
      "|    n_updates        | 97249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 30875    |\n",
      "|    total_timesteps  | 392000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 11.5     |\n",
      "|    n_updates        | 97749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=395000, episode_reward=-50208.11 +/- 7.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 395000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.4      |\n",
      "|    n_updates        | 98499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 31241    |\n",
      "|    total_timesteps  | 396000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 12.3     |\n",
      "|    n_updates        | 98749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-50201.55 +/- 8.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 400000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.7      |\n",
      "|    n_updates        | 99749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 31608    |\n",
      "|    total_timesteps  | 400000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 31849    |\n",
      "|    total_timesteps  | 404000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 11.2     |\n",
      "|    n_updates        | 100749   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=405000, episode_reward=-50211.67 +/- 3.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 405000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.2      |\n",
      "|    n_updates        | 100999    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 32223    |\n",
      "|    total_timesteps  | 408000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.2     |\n",
      "|    n_updates        | 101749   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=-50202.65 +/- 19.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 410000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.5      |\n",
      "|    n_updates        | 102249    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 32595    |\n",
      "|    total_timesteps  | 412000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 14.2     |\n",
      "|    n_updates        | 102749   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=415000, episode_reward=-50214.33 +/- 9.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 415000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.6      |\n",
      "|    n_updates        | 103499    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 32979    |\n",
      "|    total_timesteps  | 416000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 8.73     |\n",
      "|    n_updates        | 103749   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-50147.66 +/- 31.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.01e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 420000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.7      |\n",
      "|    n_updates        | 104749    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 33351    |\n",
      "|    total_timesteps  | 420000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 33595    |\n",
      "|    total_timesteps  | 424000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 16.8     |\n",
      "|    n_updates        | 105749   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=425000, episode_reward=-50205.00 +/- 13.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 425000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.23      |\n",
      "|    n_updates        | 105999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 428       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 33963     |\n",
      "|    total_timesteps  | 428000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.5      |\n",
      "|    n_updates        | 106749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=-50181.91 +/- 14.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 430000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.1      |\n",
      "|    n_updates        | 107249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 432       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 34334     |\n",
      "|    total_timesteps  | 432000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.9      |\n",
      "|    n_updates        | 107749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=435000, episode_reward=-50208.04 +/- 13.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 435000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.5      |\n",
      "|    n_updates        | 108499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 436       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 34706     |\n",
      "|    total_timesteps  | 436000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.3      |\n",
      "|    n_updates        | 108749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-10668.34 +/- 7193.35\n",
      "Episode length: 439.00 +/- 136.50\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 439       |\n",
      "|    mean_reward      | -1.07e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 440000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.8      |\n",
      "|    n_updates        | 109749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 440       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 35003     |\n",
      "|    total_timesteps  | 440000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 444       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 35246     |\n",
      "|    total_timesteps  | 444000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.51      |\n",
      "|    n_updates        | 110749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=445000, episode_reward=-50214.01 +/- 7.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 445000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.4      |\n",
      "|    n_updates        | 110999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 448       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 35616     |\n",
      "|    total_timesteps  | 448000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.3      |\n",
      "|    n_updates        | 111749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=-50187.17 +/- 20.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 450000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19        |\n",
      "|    n_updates        | 112249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 452       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 35986     |\n",
      "|    total_timesteps  | 452000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.6      |\n",
      "|    n_updates        | 112749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=455000, episode_reward=-29709.04 +/- 21149.71\n",
      "Episode length: 692.60 +/- 334.04\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 693       |\n",
      "|    mean_reward      | -2.97e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 455000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.8      |\n",
      "|    n_updates        | 113499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 456       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 36318     |\n",
      "|    total_timesteps  | 456000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.5      |\n",
      "|    n_updates        | 113749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-50212.48 +/- 7.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 460000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.8      |\n",
      "|    n_updates        | 114749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 460       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 36687     |\n",
      "|    total_timesteps  | 460000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 464       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 36929     |\n",
      "|    total_timesteps  | 464000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.8      |\n",
      "|    n_updates        | 115749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=465000, episode_reward=-50200.33 +/- 11.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 465000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.6      |\n",
      "|    n_updates        | 115999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 468       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 37301     |\n",
      "|    total_timesteps  | 468000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.1      |\n",
      "|    n_updates        | 116749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=-50210.62 +/- 12.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 470000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.7      |\n",
      "|    n_updates        | 117249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 472       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 37676     |\n",
      "|    total_timesteps  | 472000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.5      |\n",
      "|    n_updates        | 117749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=475000, episode_reward=-50171.23 +/- 25.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 475000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 29.3      |\n",
      "|    n_updates        | 118499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 476       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 38048     |\n",
      "|    total_timesteps  | 476000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.9      |\n",
      "|    n_updates        | 118749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-50188.59 +/- 30.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 480000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.4      |\n",
      "|    n_updates        | 119749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 480       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 38418     |\n",
      "|    total_timesteps  | 480000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 484       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 38661     |\n",
      "|    total_timesteps  | 484000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.6      |\n",
      "|    n_updates        | 120749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=485000, episode_reward=-50210.66 +/- 15.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 485000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.9      |\n",
      "|    n_updates        | 120999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 488       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 39035     |\n",
      "|    total_timesteps  | 488000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14        |\n",
      "|    n_updates        | 121749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=-50198.43 +/- 4.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 490000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.2      |\n",
      "|    n_updates        | 122249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 492       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 39409     |\n",
      "|    total_timesteps  | 492000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.4      |\n",
      "|    n_updates        | 122749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=495000, episode_reward=-50212.10 +/- 11.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 495000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.4      |\n",
      "|    n_updates        | 123499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 496       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 39779     |\n",
      "|    total_timesteps  | 496000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.4      |\n",
      "|    n_updates        | 123749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-50209.52 +/- 11.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 500000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.8      |\n",
      "|    n_updates        | 124749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 500       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 40168     |\n",
      "|    total_timesteps  | 500000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 504       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 40417     |\n",
      "|    total_timesteps  | 504000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.9      |\n",
      "|    n_updates        | 125749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=505000, episode_reward=-50199.54 +/- 15.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 505000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.1      |\n",
      "|    n_updates        | 125999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 508       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 40798     |\n",
      "|    total_timesteps  | 508000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.3      |\n",
      "|    n_updates        | 126749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=-50215.74 +/- 10.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 510000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19        |\n",
      "|    n_updates        | 127249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 512       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 41172     |\n",
      "|    total_timesteps  | 512000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.9      |\n",
      "|    n_updates        | 127749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=515000, episode_reward=-50194.78 +/- 13.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 515000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.5      |\n",
      "|    n_updates        | 128499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 516       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 41547     |\n",
      "|    total_timesteps  | 516000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.6      |\n",
      "|    n_updates        | 128749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-50204.87 +/- 15.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 520000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.9      |\n",
      "|    n_updates        | 129749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 520       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 41927     |\n",
      "|    total_timesteps  | 520000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 524       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 42174     |\n",
      "|    total_timesteps  | 524000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15        |\n",
      "|    n_updates        | 130749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=525000, episode_reward=-50212.83 +/- 14.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 525000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.2      |\n",
      "|    n_updates        | 130999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 528       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 42547     |\n",
      "|    total_timesteps  | 528000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.4      |\n",
      "|    n_updates        | 131749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=-50217.56 +/- 8.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 530000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.4      |\n",
      "|    n_updates        | 132249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 532       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 42926     |\n",
      "|    total_timesteps  | 532000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.4      |\n",
      "|    n_updates        | 132749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=535000, episode_reward=-50216.35 +/- 5.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 535000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.7      |\n",
      "|    n_updates        | 133499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 536       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 43302     |\n",
      "|    total_timesteps  | 536000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.6      |\n",
      "|    n_updates        | 133749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-50165.65 +/- 26.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 540000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.3      |\n",
      "|    n_updates        | 134749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 540       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 43680     |\n",
      "|    total_timesteps  | 540000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 544       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 43929     |\n",
      "|    total_timesteps  | 544000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.6      |\n",
      "|    n_updates        | 135749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=545000, episode_reward=-50213.11 +/- 15.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 545000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.5      |\n",
      "|    n_updates        | 135999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 548       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 44306     |\n",
      "|    total_timesteps  | 548000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.5      |\n",
      "|    n_updates        | 136749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=-50215.36 +/- 12.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 550000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.8      |\n",
      "|    n_updates        | 137249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 552       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 44683     |\n",
      "|    total_timesteps  | 552000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.8      |\n",
      "|    n_updates        | 137749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=555000, episode_reward=-50208.38 +/- 10.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 555000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.1      |\n",
      "|    n_updates        | 138499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 556       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 45061     |\n",
      "|    total_timesteps  | 556000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14        |\n",
      "|    n_updates        | 138749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-47902.41 +/- 3268.16\n",
      "Episode length: 976.40 +/- 34.58\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 976       |\n",
      "|    mean_reward      | -4.79e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 560000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.4      |\n",
      "|    n_updates        | 139749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 560       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 45434     |\n",
      "|    total_timesteps  | 560000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 564       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 45684     |\n",
      "|    total_timesteps  | 564000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.7      |\n",
      "|    n_updates        | 140749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=565000, episode_reward=-50194.98 +/- 5.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 565000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.7      |\n",
      "|    n_updates        | 140999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 568       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 46064     |\n",
      "|    total_timesteps  | 568000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.2      |\n",
      "|    n_updates        | 141749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=-50202.83 +/- 9.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 570000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.6      |\n",
      "|    n_updates        | 142249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 572       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 46448     |\n",
      "|    total_timesteps  | 572000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.8      |\n",
      "|    n_updates        | 142749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=575000, episode_reward=-50191.37 +/- 11.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 575000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.4      |\n",
      "|    n_updates        | 143499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 576       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 46825     |\n",
      "|    total_timesteps  | 576000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.9      |\n",
      "|    n_updates        | 143749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-50210.62 +/- 7.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 580000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.8      |\n",
      "|    n_updates        | 144749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 580       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 47203     |\n",
      "|    total_timesteps  | 580000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 584       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 47453     |\n",
      "|    total_timesteps  | 584000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 25.8      |\n",
      "|    n_updates        | 145749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=585000, episode_reward=-50217.94 +/- 10.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 585000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.1      |\n",
      "|    n_updates        | 145999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 588       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 47836     |\n",
      "|    total_timesteps  | 588000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.18      |\n",
      "|    n_updates        | 146749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=-50205.60 +/- 19.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 590000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.5      |\n",
      "|    n_updates        | 147249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 592       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 48214     |\n",
      "|    total_timesteps  | 592000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 25.9      |\n",
      "|    n_updates        | 147749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=595000, episode_reward=-50209.31 +/- 8.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 595000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.7      |\n",
      "|    n_updates        | 148499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 596       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 48595     |\n",
      "|    total_timesteps  | 596000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.2      |\n",
      "|    n_updates        | 148749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-50202.02 +/- 12.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 600000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.2      |\n",
      "|    n_updates        | 149749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 600       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 48983     |\n",
      "|    total_timesteps  | 600000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 604       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 49234     |\n",
      "|    total_timesteps  | 604000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.8      |\n",
      "|    n_updates        | 150749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=605000, episode_reward=-50208.29 +/- 13.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 605000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.9      |\n",
      "|    n_updates        | 150999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 608       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 49614     |\n",
      "|    total_timesteps  | 608000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16        |\n",
      "|    n_updates        | 151749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=-50205.95 +/- 19.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 610000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.2      |\n",
      "|    n_updates        | 152249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 999       |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 612       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 49992     |\n",
      "|    total_timesteps  | 611941    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.5      |\n",
      "|    n_updates        | 152735    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=615000, episode_reward=-50201.18 +/- 18.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 615000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.7      |\n",
      "|    n_updates        | 153499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 616       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 50377     |\n",
      "|    total_timesteps  | 616000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.9      |\n",
      "|    n_updates        | 153749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-13082.57 +/- 5533.32\n",
      "Episode length: 494.20 +/- 123.18\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 494       |\n",
      "|    mean_reward      | -1.31e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 620000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.1      |\n",
      "|    n_updates        | 154749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 620       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 50693     |\n",
      "|    total_timesteps  | 620000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 624       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 50944     |\n",
      "|    total_timesteps  | 624000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.3      |\n",
      "|    n_updates        | 155749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=625000, episode_reward=-50195.24 +/- 16.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 625000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 25.6      |\n",
      "|    n_updates        | 155999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 628       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 51328     |\n",
      "|    total_timesteps  | 628000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.8      |\n",
      "|    n_updates        | 156749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=-50199.18 +/- 13.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 630000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.9      |\n",
      "|    n_updates        | 157249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 632       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 51714     |\n",
      "|    total_timesteps  | 632000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19        |\n",
      "|    n_updates        | 157749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=635000, episode_reward=-50213.33 +/- 15.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 635000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.5      |\n",
      "|    n_updates        | 158499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 636       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 52098     |\n",
      "|    total_timesteps  | 636000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.9      |\n",
      "|    n_updates        | 158749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=-50206.66 +/- 7.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 640000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 24.8      |\n",
      "|    n_updates        | 159749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 640       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 52457     |\n",
      "|    total_timesteps  | 640000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 644       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 52693     |\n",
      "|    total_timesteps  | 644000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.1      |\n",
      "|    n_updates        | 160749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=645000, episode_reward=-50193.48 +/- 15.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 645000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 23.1      |\n",
      "|    n_updates        | 160999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 648       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 53017     |\n",
      "|    total_timesteps  | 648000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 24.5      |\n",
      "|    n_updates        | 161749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=-50216.17 +/- 10.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 650000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.2      |\n",
      "|    n_updates        | 162249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 652       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 53305     |\n",
      "|    total_timesteps  | 652000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 26.3      |\n",
      "|    n_updates        | 162749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=655000, episode_reward=-50208.79 +/- 19.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 655000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.5      |\n",
      "|    n_updates        | 163499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 656       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 53593     |\n",
      "|    total_timesteps  | 656000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22        |\n",
      "|    n_updates        | 163749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-50203.31 +/- 12.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 660000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.7      |\n",
      "|    n_updates        | 164749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 660       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 53843     |\n",
      "|    total_timesteps  | 660000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 664       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 54100     |\n",
      "|    total_timesteps  | 664000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.1      |\n",
      "|    n_updates        | 165749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=665000, episode_reward=-50216.05 +/- 12.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 665000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.4      |\n",
      "|    n_updates        | 165999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 668       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 54510     |\n",
      "|    total_timesteps  | 668000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.9      |\n",
      "|    n_updates        | 166749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=-50196.67 +/- 6.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 670000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.9      |\n",
      "|    n_updates        | 167249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 672       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 55077     |\n",
      "|    total_timesteps  | 672000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.2      |\n",
      "|    n_updates        | 167749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=675000, episode_reward=-50202.87 +/- 16.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 675000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.9      |\n",
      "|    n_updates        | 168499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 676       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 55545     |\n",
      "|    total_timesteps  | 676000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.2      |\n",
      "|    n_updates        | 168749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-50219.50 +/- 8.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 680000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.1      |\n",
      "|    n_updates        | 169749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 680       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 55896     |\n",
      "|    total_timesteps  | 680000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 684       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 56113     |\n",
      "|    total_timesteps  | 684000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 43.2      |\n",
      "|    n_updates        | 170749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=685000, episode_reward=-50208.22 +/- 15.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 685000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 34.6      |\n",
      "|    n_updates        | 170999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 688       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 56446     |\n",
      "|    total_timesteps  | 688000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.3      |\n",
      "|    n_updates        | 171749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=-50198.93 +/- 17.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 690000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.6      |\n",
      "|    n_updates        | 172249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 692       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 56782     |\n",
      "|    total_timesteps  | 692000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11        |\n",
      "|    n_updates        | 172749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=695000, episode_reward=-50208.12 +/- 9.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 695000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.5      |\n",
      "|    n_updates        | 173499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 696       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 57140     |\n",
      "|    total_timesteps  | 696000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.5      |\n",
      "|    n_updates        | 173749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=-30409.64 +/- 1976.13\n",
      "Episode length: 777.00 +/- 25.44\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 777       |\n",
      "|    mean_reward      | -3.04e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 700000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.3      |\n",
      "|    n_updates        | 174749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 700       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 57523     |\n",
      "|    total_timesteps  | 700000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 704       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 57802     |\n",
      "|    total_timesteps  | 704000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14        |\n",
      "|    n_updates        | 175749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=705000, episode_reward=-50212.34 +/- 7.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 705000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.9      |\n",
      "|    n_updates        | 175999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 708       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 58248     |\n",
      "|    total_timesteps  | 708000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.8      |\n",
      "|    n_updates        | 176749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=-50205.71 +/- 6.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 710000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 27.9      |\n",
      "|    n_updates        | 177249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 712       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 58654     |\n",
      "|    total_timesteps  | 712000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.1      |\n",
      "|    n_updates        | 177749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=715000, episode_reward=-50202.98 +/- 11.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 715000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 26.4      |\n",
      "|    n_updates        | 178499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 716       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 59012     |\n",
      "|    total_timesteps  | 716000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 34        |\n",
      "|    n_updates        | 178749    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 999      |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 59243    |\n",
      "|    total_timesteps  | 719871   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 22.9     |\n",
      "|    n_updates        | 179717   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-50212.21 +/- 12.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 720000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.6      |\n",
      "|    n_updates        | 179749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 724       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 59617     |\n",
      "|    total_timesteps  | 724000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.4      |\n",
      "|    n_updates        | 180749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=725000, episode_reward=-50196.83 +/- 8.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 725000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13        |\n",
      "|    n_updates        | 180999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 728       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 59986     |\n",
      "|    total_timesteps  | 728000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.1      |\n",
      "|    n_updates        | 181749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=-50212.45 +/- 7.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 730000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.3      |\n",
      "|    n_updates        | 182249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 732       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 60334     |\n",
      "|    total_timesteps  | 732000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 23.8      |\n",
      "|    n_updates        | 182749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=735000, episode_reward=-50214.12 +/- 7.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 735000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.5      |\n",
      "|    n_updates        | 183499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 736       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 60672     |\n",
      "|    total_timesteps  | 736000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.53      |\n",
      "|    n_updates        | 183749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=-43336.48 +/- 6079.35\n",
      "Episode length: 926.60 +/- 66.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 927       |\n",
      "|    mean_reward      | -4.33e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 740000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.4      |\n",
      "|    n_updates        | 184749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 740       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 61000     |\n",
      "|    total_timesteps  | 740000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 744       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 61220     |\n",
      "|    total_timesteps  | 744000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.7      |\n",
      "|    n_updates        | 185749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=745000, episode_reward=-50206.16 +/- 10.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 745000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.3      |\n",
      "|    n_updates        | 185999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 748       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 61557     |\n",
      "|    total_timesteps  | 748000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.9      |\n",
      "|    n_updates        | 186749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=-50199.46 +/- 17.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 750000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19        |\n",
      "|    n_updates        | 187249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 752       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 61896     |\n",
      "|    total_timesteps  | 752000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.7      |\n",
      "|    n_updates        | 187749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=755000, episode_reward=-31639.41 +/- 9715.70\n",
      "Episode length: 785.00 +/- 115.08\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 785       |\n",
      "|    mean_reward      | -3.16e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 755000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.1      |\n",
      "|    n_updates        | 188499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 756       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 62209     |\n",
      "|    total_timesteps  | 756000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.7      |\n",
      "|    n_updates        | 188749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=-50204.60 +/- 9.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 760000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.9      |\n",
      "|    n_updates        | 189749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 760       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 62547     |\n",
      "|    total_timesteps  | 760000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 764       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 62767     |\n",
      "|    total_timesteps  | 764000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 33.6      |\n",
      "|    n_updates        | 190749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=765000, episode_reward=-50185.43 +/- 32.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 765000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.6      |\n",
      "|    n_updates        | 190999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 768       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 63106     |\n",
      "|    total_timesteps  | 768000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 30.1      |\n",
      "|    n_updates        | 191749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=-50210.65 +/- 9.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 770000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.2      |\n",
      "|    n_updates        | 192249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 772       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 63448     |\n",
      "|    total_timesteps  | 772000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 33.9      |\n",
      "|    n_updates        | 192749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=775000, episode_reward=-50206.59 +/- 10.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 775000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13        |\n",
      "|    n_updates        | 193499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 776       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 63790     |\n",
      "|    total_timesteps  | 776000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 32        |\n",
      "|    n_updates        | 193749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-50199.40 +/- 29.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 780000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.1      |\n",
      "|    n_updates        | 194749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 780       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 64129     |\n",
      "|    total_timesteps  | 780000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 784       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 64347     |\n",
      "|    total_timesteps  | 784000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 33.6      |\n",
      "|    n_updates        | 195749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=785000, episode_reward=-50203.04 +/- 7.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 785000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.9      |\n",
      "|    n_updates        | 195999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 788       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 64687     |\n",
      "|    total_timesteps  | 788000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 23.5      |\n",
      "|    n_updates        | 196749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=-50209.26 +/- 11.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 790000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17        |\n",
      "|    n_updates        | 197249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 792       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 65026     |\n",
      "|    total_timesteps  | 792000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.1      |\n",
      "|    n_updates        | 197749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=795000, episode_reward=-50194.26 +/- 9.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 795000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 38.4      |\n",
      "|    n_updates        | 198499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 796       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 65367     |\n",
      "|    total_timesteps  | 796000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 31.8      |\n",
      "|    n_updates        | 198749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=-50212.25 +/- 13.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 800000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 27.1      |\n",
      "|    n_updates        | 199749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 800       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 65709     |\n",
      "|    total_timesteps  | 800000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 804       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 65930     |\n",
      "|    total_timesteps  | 804000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.7      |\n",
      "|    n_updates        | 200749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=805000, episode_reward=-50210.46 +/- 6.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 805000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.3      |\n",
      "|    n_updates        | 200999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 808       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 66274     |\n",
      "|    total_timesteps  | 808000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.56      |\n",
      "|    n_updates        | 201749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=-50207.07 +/- 12.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 810000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 31        |\n",
      "|    n_updates        | 202249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 812       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 66620     |\n",
      "|    total_timesteps  | 812000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.1      |\n",
      "|    n_updates        | 202749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=815000, episode_reward=-50210.20 +/- 8.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 815000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 26.7      |\n",
      "|    n_updates        | 203499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 816       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 66965     |\n",
      "|    total_timesteps  | 816000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22        |\n",
      "|    n_updates        | 203749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=-50183.96 +/- 25.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 820000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13        |\n",
      "|    n_updates        | 204749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 820       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 67296     |\n",
      "|    total_timesteps  | 820000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 824       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 67526     |\n",
      "|    total_timesteps  | 824000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 28.7      |\n",
      "|    n_updates        | 205749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=825000, episode_reward=-50204.71 +/- 15.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 825000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.7      |\n",
      "|    n_updates        | 205999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 828       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 67890     |\n",
      "|    total_timesteps  | 828000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.1      |\n",
      "|    n_updates        | 206749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=-50204.52 +/- 14.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 830000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.3      |\n",
      "|    n_updates        | 207249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 832       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 68236     |\n",
      "|    total_timesteps  | 832000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.5      |\n",
      "|    n_updates        | 207749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=835000, episode_reward=-50208.19 +/- 5.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 835000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 27.3      |\n",
      "|    n_updates        | 208499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 836       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 68559     |\n",
      "|    total_timesteps  | 836000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.3      |\n",
      "|    n_updates        | 208749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=-50202.96 +/- 4.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 840000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.8      |\n",
      "|    n_updates        | 209749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 840       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 68879     |\n",
      "|    total_timesteps  | 840000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 844       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 69081     |\n",
      "|    total_timesteps  | 844000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16        |\n",
      "|    n_updates        | 210749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=845000, episode_reward=-50196.14 +/- 13.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 845000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.4      |\n",
      "|    n_updates        | 210999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 848       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 69399     |\n",
      "|    total_timesteps  | 848000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.1      |\n",
      "|    n_updates        | 211749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=-41928.25 +/- 6133.67\n",
      "Episode length: 910.60 +/- 70.55\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 911       |\n",
      "|    mean_reward      | -4.19e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 850000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 23.3      |\n",
      "|    n_updates        | 212249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 852       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 69705     |\n",
      "|    total_timesteps  | 852000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.4      |\n",
      "|    n_updates        | 212749    |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simple DQN architecture\n",
    "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
    "\n",
    "# Set up the model with simpler hyperparameters\n",
    "model = DQN('CnnPolicy', custom_env, policy_kwargs=policy_kwargs, \n",
    "            verbose=1, buffer_size=50000, learning_starts=1000, \n",
    "            learning_rate=0.0005, batch_size=32, exploration_fraction=0.1,\n",
    "            exploration_final_eps=0.05)\n",
    "\n",
    "# Setup evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(custom_env, best_model_save_path=model_dir,\n",
    "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
    "                                         name_prefix='dqn_model_checkpoint')\n",
    "\n",
    "\n",
    "\n",
    "# Start training the model with callbacks for evaluation and checkpoints\n",
    "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the final model after training\n",
    "model.save(\"dqn_custom_env_model\")\n",
    "custom_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env.close()\n",
    "del custom_env\n",
    "foo = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env = EnhancedCarRacing(render_mode=\"rgb_array\")\n",
    "obs, info = custom_env.reset()\n",
    "custom_env = GrayscaleObservation(custom_env, keep_dim=True)\n",
    "custom_env = TimeLimit(custom_env, max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'PPO_env_mod_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple DQN architecture\n",
    "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
    "\n",
    "# Set up the model with simpler hyperparameters\n",
    "model = PPO('CnnPolicy', custom_env, policy_kwargs=policy_kwargs, \n",
    "            verbose=1, learning_rate=0.0005, batch_size=32)\n",
    "\n",
    "# Setup evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(custom_env, best_model_save_path=model_dir,\n",
    "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
    "                                         name_prefix='ppo_model_checkpoint')\n",
    "\n",
    "\n",
    "\n",
    "# Start training the model with callbacks for evaluation and checkpoints\n",
    "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the final model after training\n",
    "model.save(\"ppo_custom_env_model\")\n",
    "custom_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "del env\n",
    "foo = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust number of episodes based on the environment's characteristics\n",
    "if hasattr(env, \"max_episode_steps\"):\n",
    "    # If the environment has predefined max steps, use a higher number for evaluation\n",
    "    num_episodes = 50  \n",
    "else:\n",
    "    # For simpler environments, use fewer episodes\n",
    "    num_episodes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = foo #load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "episode_rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = model.predict(obs)  # Use trained policy\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        record_agent_dynamics(env)  # Record smoothness metrics\n",
    "        if done:\n",
    "            break\n",
    "    episode_rewards.append(total_reward)\n",
    "    obs = env.reset()\n",
    "\n",
    "print(f\"Average Reward: {np.mean(episode_rewards)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
