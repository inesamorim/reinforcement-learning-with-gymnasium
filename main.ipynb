{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing OpenAI Gym Environments and Implementing Reinforcement Learning Agents with Stable Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme: Car Racing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Constança\n",
    "- Daniela Osório, 202208679\n",
    "- Inês Amorim, 202108108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import GrayscaleObservation, ResizeObservation, RecordEpisodeStatistics, RecordVideo, TimeLimit\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "import os\n",
    "import gc\n",
    "from eval import *\n",
    "from custom_cr import EnhancedCarRacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CarRacing-v3 environment from Gymnasium (previously Gym) is part of the Box2D environments, and it offers an interesting challenge for training reinforcement learning agents. It's a top-down racing simulation where the track is randomly generated at the start of each episode. The environment offers both continuous and discrete action spaces, making it adaptable to different types of reinforcement learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False, render_mode='rgb_array') \n",
    "obs, info = env.reset()\n",
    "#continuous = False to use Discrete space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Action Space:**\n",
    "\n",
    "   - **Continuous:** Three actions: steering, gas, and braking. Steering ranges from -1 (full left) to +1 (full right).\n",
    "   -  **Discrete:** Five possible actions: do nothing, steer left, steer right, gas, and brake.\n",
    "\n",
    "- **Observation Space:**\n",
    "\n",
    "    - The environment provides a 96x96 RGB image of the car and the track, which serves as the state input for the agent.\n",
    "\n",
    "- **Rewards:**\n",
    "\n",
    "    - The agent receives a -0.1 penalty for every frame, encouraging efficiency.\n",
    "    - It earns a positive reward for visiting track tiles: the formula is Reward=1000−0.1×framesReward=1000−0.1×frames, where \"frames\" is the number of frames taken to complete the lap. The reward for completing a lap depends on how many track tiles are visited.\n",
    "\n",
    "- **Episode Termination:**\n",
    "\n",
    "    - The episode ends either when all track tiles are visited or if the car goes off the track, which incurs a significant penalty (-100 reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'rgb_array', 'state_pixels']\n"
     ]
    }
   ],
   "source": [
    "#check render modes\n",
    "print(env.metadata[\"render_modes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking if everything is okay and working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment and render the first frame\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "print(\"Environment initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(5)\n",
      "Observation Space: Box(0, 255, (96, 96, 3), uint8)\n",
      "Environment Metadata: {'render_modes': ['human', 'rgb_array', 'state_pixels'], 'render_fps': 50}\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Environment Metadata:\", env.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(10):\n",
    "    \"\"\"action = env.action_space.sample()  # Random action\n",
    "    print(f\"Action before step: {action}, Type: {type(action)}\")\n",
    "    obs, reward, done, info = env.step(action)\"\"\"\n",
    "    env.step(env.action_space.sample())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-Learning (DQN) is a reinforcement learning algorithm that extends the traditional Q-Learning method using neural networks to approximate the Q-values for state-action pairs.\n",
    "DQN is inherently designed for discrete action spaces, as the neural network outputs a separate Q-value for each action. For each state, the algorithm selects actions based on the highest Q-value, making it ideal for problems where actions are discrete and finite. This is a key advantage compared to other reinforcement learning methods, which may require modifications or different approaches for discrete action selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = '../2.2.2/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvSpec(id='CarRacing-v3', entry_point='gymnasium.envs.box2d.car_racing:CarRacing', reward_threshold=900, nondeterministic=False, max_episode_steps=1000, order_enforce=True, disable_env_checker=False, kwargs={'continuous': False}, namespace=None, name='CarRacing', version=3, additional_wrappers=(), vector_entry_point=None)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False)\n",
    "print(env.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(obs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'DQN_baseline_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "video_dir = os.path.join(logs_path, \"videos\")\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple DQN architecture\n",
    "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
    "\n",
    "# Set up the model with simpler hyperparameters\n",
    "model = DQN('CnnPolicy', env, policy_kwargs=policy_kwargs, \n",
    "            verbose=1, buffer_size=50000, learning_starts=1000, \n",
    "            learning_rate=0.0005, batch_size=32, exploration_fraction=0.1,\n",
    "            exploration_final_eps=0.05)\n",
    "\n",
    "# Setup evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(env, best_model_save_path=model_dir,\n",
    "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
    "                                         name_prefix='dqn_model_checkpoint')\n",
    "\n",
    "\n",
    "\n",
    "# Start training the model with callbacks for evaluation and checkpoints\n",
    "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the final model after training\n",
    "model.save(\"dqn_car_racing_model\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "del env\n",
    "foo = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'PPO_baseline_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "video_dir = os.path.join(logs_path, \"videos\")\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple DQN architecture\n",
    "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
    "\n",
    "# Set up the model with simpler hyperparameters\n",
    "model = PPO('CnnPolicy', env, policy_kwargs=policy_kwargs, \n",
    "            verbose=1, learning_rate=0.0005, batch_size=32)\n",
    "\n",
    "# Setup evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(env, best_model_save_path=model_dir,\n",
    "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
    "                                         name_prefix='ppo_model_checkpoint')\n",
    "\n",
    "\n",
    "\n",
    "# Start training the model with callbacks for evaluation and checkpoints\n",
    "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the final model after training\n",
    "model.save(\"ppo_env_model\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "del env\n",
    "foo = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Environment and Reward Modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Modification**            | **Description**                                                                                      | **Effect**                                     |\n",
    "|-----------------------------|------------------------------------------------------------------------------------------------------|-----------------------------------------------|\n",
    "| **Obstacles**               | Randomly placed obstacles on the track.                                                             | Requires avoidance and navigation skills.     |\n",
    "| **Track Width Variability** | Random track width adjustments between `[0.8, 1.2]`.                                                | Simulates narrow/wide tracks dynamically.     |\n",
    "| **Weather Conditions**      | Introduces \"rain\" and \"snow,\" which alter action effectiveness.                                     | Adds randomness and realism to driving.       |\n",
    "| **Off-Track Penalty**       | Reward reduced by `-10` if the car leaves the track.                                                | Encourages the agent to stay on track.        |\n",
    "| **Distance Reward**         | Positive reward based on the distance traveled per step.                                            | Incentivizes efficient driving.               |\n",
    "| **Obstacle Proximity Penalty** | Penalty inversely proportional to the distance to nearby obstacles (`1 / (d + 1e-6)`).             | Encourages the car to avoid obstacles safely. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observation shape: (96, 96, 3)\n",
      "Initial info: {}\n",
      "\n",
      "Step 1:\n",
      "Action taken: 2\n",
      "Reward: 5.804336925215569\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 2:\n",
      "Action taken: 3\n",
      "Reward: -0.37469043040753186\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 3:\n",
      "Action taken: 0\n",
      "Reward: -0.4746904304075319\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 4:\n",
      "Action taken: 0\n",
      "Reward: -0.5746904304075319\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 5:\n",
      "Action taken: 1\n",
      "Reward: -0.6746904304075318\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 6:\n",
      "Action taken: 3\n",
      "Reward: -0.7746904304075318\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 7:\n",
      "Action taken: 1\n",
      "Reward: -0.8746904304075318\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 8:\n",
      "Action taken: 0\n",
      "Reward: -0.9746904304075318\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 9:\n",
      "Action taken: 4\n",
      "Reward: -1.0746904304075318\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n",
      "\n",
      "Step 10:\n",
      "Action taken: 4\n",
      "Reward: -1.1746904304075318\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n",
      "Current weather: rain\n"
     ]
    }
   ],
   "source": [
    "custom_env = EnhancedCarRacing(render_mode=\"rgb_array\")\n",
    "obs, info = custom_env.reset()\n",
    "\n",
    "print(\"Initial observation shape:\", obs.shape)\n",
    "print(\"Initial info:\", info)\n",
    "\n",
    "for i in range(10):\n",
    "    action = custom_env.action_space.sample()  # Your agent would make a decision here\n",
    "    observation, reward, terminated, truncated, info = custom_env.step(action)\n",
    "    print(f\"\\nStep {i+1}:\")\n",
    "    print(\"Action taken:\", action)\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"Terminated:\", terminated)\n",
    "    print(\"Truncated:\", truncated)\n",
    "    print(\"Info:\", info)\n",
    "\n",
    "    if hasattr(custom_env, 'weather_condition'):\n",
    "        print(\"Current weather:\", custom_env.weather_condition)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        print(\"Episode ended\")\n",
    "        break\n",
    "\n",
    "custom_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env = GrayscaleObservation(custom_env, keep_dim=True)\n",
    "custom_env = TimeLimit(custom_env, max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TimeLimit<GrayscaleObservation<EnhancedCarRacing instance>>>\n"
     ]
    }
   ],
   "source": [
    "print(custom_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\", custom_env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'DQN_env_mod_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1218 10:05:02.050906178 NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "/home/derpy/Documents/Fac/ano3/semestre1/ISIA/reinforcement-learning-with-gymnasium/.venv/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7eaf196b5eb0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7eaf159685c0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.962     |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 28        |\n",
      "|    time_elapsed     | 141       |\n",
      "|    total_timesteps  | 4000      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.12      |\n",
      "|    n_updates        | 749       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=-50215.65 +/- 9.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.953     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 5000      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.93      |\n",
      "|    n_updates        | 999       |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.924     |\n",
      "| time/               |           |\n",
      "|    episodes         | 8         |\n",
      "|    fps              | 17        |\n",
      "|    time_elapsed     | 463       |\n",
      "|    total_timesteps  | 8000      |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.19      |\n",
      "|    n_updates        | 1749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-50167.65 +/- 36.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.905     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 10000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.34      |\n",
      "|    n_updates        | 2249      |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.886     |\n",
      "| time/               |           |\n",
      "|    episodes         | 12        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 750       |\n",
      "|    total_timesteps  | 12000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.74      |\n",
      "|    n_updates        | 2749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-50211.32 +/- 8.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.858     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 15000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.64      |\n",
      "|    n_updates        | 3499      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.848     |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 14        |\n",
      "|    time_elapsed     | 1127      |\n",
      "|    total_timesteps  | 16000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.41      |\n",
      "|    n_updates        | 3749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-50201.12 +/- 9.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.81      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 20000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 1.52      |\n",
      "|    n_updates        | 4749      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.81      |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 1526      |\n",
      "|    total_timesteps  | 20000     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.772     |\n",
      "| time/               |           |\n",
      "|    episodes         | 24        |\n",
      "|    fps              | 14        |\n",
      "|    time_elapsed     | 1682      |\n",
      "|    total_timesteps  | 24000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.47      |\n",
      "|    n_updates        | 5749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-50205.71 +/- 9.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.763     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 25000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.99      |\n",
      "|    n_updates        | 5999      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.734     |\n",
      "| time/               |           |\n",
      "|    episodes         | 28        |\n",
      "|    fps              | 14        |\n",
      "|    time_elapsed     | 1909      |\n",
      "|    total_timesteps  | 28000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.69      |\n",
      "|    n_updates        | 6749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-33812.91 +/- 20146.91\n",
      "Episode length: 768.60 +/- 286.51\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 769       |\n",
      "|    mean_reward      | -3.38e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.715     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 30000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.51      |\n",
      "|    n_updates        | 7249      |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.696     |\n",
      "| time/               |           |\n",
      "|    episodes         | 32        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 2111      |\n",
      "|    total_timesteps  | 32000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.8       |\n",
      "|    n_updates        | 7749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-50206.97 +/- 7.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.668     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 35000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.63      |\n",
      "|    n_updates        | 8499      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.658     |\n",
      "| time/               |           |\n",
      "|    episodes         | 36        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 2339      |\n",
      "|    total_timesteps  | 36000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.92      |\n",
      "|    n_updates        | 8749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-50201.85 +/- 12.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.62      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 40000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 2.66      |\n",
      "|    n_updates        | 9749      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.62      |\n",
      "| time/               |           |\n",
      "|    episodes         | 40        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 2567      |\n",
      "|    total_timesteps  | 40000     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.582     |\n",
      "| time/               |           |\n",
      "|    episodes         | 44        |\n",
      "|    fps              | 16        |\n",
      "|    time_elapsed     | 2696      |\n",
      "|    total_timesteps  | 44000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.39      |\n",
      "|    n_updates        | 10749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-50172.30 +/- 43.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.573     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 45000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.68      |\n",
      "|    n_updates        | 10999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.544     |\n",
      "| time/               |           |\n",
      "|    episodes         | 48        |\n",
      "|    fps              | 16        |\n",
      "|    time_elapsed     | 2971      |\n",
      "|    total_timesteps  | 48000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4         |\n",
      "|    n_updates        | 11749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-50216.08 +/- 13.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.525     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 50000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.76      |\n",
      "|    n_updates        | 12249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.506     |\n",
      "| time/               |           |\n",
      "|    episodes         | 52        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 3257      |\n",
      "|    total_timesteps  | 52000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 6         |\n",
      "|    n_updates        | 12749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-50112.29 +/- 20.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.01e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.478     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 55000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.88      |\n",
      "|    n_updates        | 13499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.468     |\n",
      "| time/               |           |\n",
      "|    episodes         | 56        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 3561      |\n",
      "|    total_timesteps  | 56000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.84      |\n",
      "|    n_updates        | 13749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-50207.37 +/- 12.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.43      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 60000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.37      |\n",
      "|    n_updates        | 14749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.43      |\n",
      "| time/               |           |\n",
      "|    episodes         | 60        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 3874      |\n",
      "|    total_timesteps  | 60000     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.392     |\n",
      "| time/               |           |\n",
      "|    episodes         | 64        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 4087      |\n",
      "|    total_timesteps  | 64000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 7.42      |\n",
      "|    n_updates        | 15749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-50208.35 +/- 11.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.383     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 65000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.9       |\n",
      "|    n_updates        | 15999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.354     |\n",
      "| time/               |           |\n",
      "|    episodes         | 68        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 4352      |\n",
      "|    total_timesteps  | 68000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.87      |\n",
      "|    n_updates        | 16749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-50192.87 +/- 21.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.335     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 70000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.99      |\n",
      "|    n_updates        | 17249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.316     |\n",
      "| time/               |           |\n",
      "|    episodes         | 72        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 4582      |\n",
      "|    total_timesteps  | 72000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.44      |\n",
      "|    n_updates        | 17749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-50206.63 +/- 16.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.288     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 75000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.2       |\n",
      "|    n_updates        | 18499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.278     |\n",
      "| time/               |           |\n",
      "|    episodes         | 76        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 4837      |\n",
      "|    total_timesteps  | 76000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.99      |\n",
      "|    n_updates        | 18749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-50205.96 +/- 11.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.24      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 80000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.08      |\n",
      "|    n_updates        | 19749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.24      |\n",
      "| time/               |           |\n",
      "|    episodes         | 80        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 5146      |\n",
      "|    total_timesteps  | 80000     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.202     |\n",
      "| time/               |           |\n",
      "|    episodes         | 84        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 5345      |\n",
      "|    total_timesteps  | 84000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.51      |\n",
      "|    n_updates        | 20749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-50171.45 +/- 25.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.193     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 85000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.73      |\n",
      "|    n_updates        | 20999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.164     |\n",
      "| time/               |           |\n",
      "|    episodes         | 88        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 5611      |\n",
      "|    total_timesteps  | 88000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 4.12      |\n",
      "|    n_updates        | 21749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-50212.26 +/- 4.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.145     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 90000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.17      |\n",
      "|    n_updates        | 22249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.126     |\n",
      "| time/               |           |\n",
      "|    episodes         | 92        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 5915      |\n",
      "|    total_timesteps  | 92000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.2      |\n",
      "|    n_updates        | 22749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-50202.68 +/- 18.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.0975    |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 95000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 3.79      |\n",
      "|    n_updates        | 23499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.088     |\n",
      "| time/               |           |\n",
      "|    episodes         | 96        |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 6232      |\n",
      "|    total_timesteps  | 96000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.39      |\n",
      "|    n_updates        | 23749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-8896.11 +/- 3242.86\n",
      "Episode length: 413.00 +/- 71.52\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 413      |\n",
      "|    mean_reward      | -8.9e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 6.79     |\n",
      "|    n_updates        | 24749    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 100       |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 6458      |\n",
      "|    total_timesteps  | 100000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 104       |\n",
      "|    fps              | 15        |\n",
      "|    time_elapsed     | 6697      |\n",
      "|    total_timesteps  | 104000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.64      |\n",
      "|    n_updates        | 25749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-50208.89 +/- 7.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 105000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.3      |\n",
      "|    n_updates        | 25999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 108       |\n",
      "|    fps              | 14        |\n",
      "|    time_elapsed     | 7687      |\n",
      "|    total_timesteps  | 108000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 5.63      |\n",
      "|    n_updates        | 26749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-50201.73 +/- 18.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 110000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.6      |\n",
      "|    n_updates        | 27249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 8188     |\n",
      "|    total_timesteps  | 112000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 10       |\n",
      "|    n_updates        | 27749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-50202.84 +/- 10.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 115000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.98      |\n",
      "|    n_updates        | 28499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 8754     |\n",
      "|    total_timesteps  | 116000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 10.6     |\n",
      "|    n_updates        | 28749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-50211.57 +/- 13.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 120000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 6.73      |\n",
      "|    n_updates        | 29749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 9395     |\n",
      "|    total_timesteps  | 120000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 9793     |\n",
      "|    total_timesteps  | 124000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 11.5     |\n",
      "|    n_updates        | 30749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-50191.04 +/- 17.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 125000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.8      |\n",
      "|    n_updates        | 30999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 10314    |\n",
      "|    total_timesteps  | 128000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 5.79     |\n",
      "|    n_updates        | 31749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-50197.73 +/- 11.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 130000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.03      |\n",
      "|    n_updates        | 32249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 10866    |\n",
      "|    total_timesteps  | 131959   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 9.18     |\n",
      "|    n_updates        | 32739    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=-13544.02 +/- 2978.08\n",
      "Episode length: 514.40 +/- 61.34\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 514       |\n",
      "|    mean_reward      | -1.35e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 135000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.11      |\n",
      "|    n_updates        | 33499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 11222    |\n",
      "|    total_timesteps  | 136000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 8.33     |\n",
      "|    n_updates        | 33749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-50120.15 +/- 43.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.01e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 140000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9         |\n",
      "|    n_updates        | 34749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 11781    |\n",
      "|    total_timesteps  | 140000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 12117    |\n",
      "|    total_timesteps  | 144000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 4.79     |\n",
      "|    n_updates        | 35749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=-50199.28 +/- 11.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 145000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.4      |\n",
      "|    n_updates        | 35999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 12571    |\n",
      "|    total_timesteps  | 148000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 9.67     |\n",
      "|    n_updates        | 36749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-50188.72 +/- 9.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 150000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.68      |\n",
      "|    n_updates        | 37249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 12851    |\n",
      "|    total_timesteps  | 152000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.8     |\n",
      "|    n_updates        | 37749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=-50208.45 +/- 15.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 155000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 6.76      |\n",
      "|    n_updates        | 38499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 13089    |\n",
      "|    total_timesteps  | 156000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 6.42     |\n",
      "|    n_updates        | 38749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-50194.16 +/- 5.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 160000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.2      |\n",
      "|    n_updates        | 39749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 13326    |\n",
      "|    total_timesteps  | 160000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 13461    |\n",
      "|    total_timesteps  | 164000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.3     |\n",
      "|    n_updates        | 40749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=-50206.46 +/- 11.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 165000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.6      |\n",
      "|    n_updates        | 40999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 13755    |\n",
      "|    total_timesteps  | 168000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 6.07     |\n",
      "|    n_updates        | 41749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-50197.88 +/- 13.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 170000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.97      |\n",
      "|    n_updates        | 42249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 14065    |\n",
      "|    total_timesteps  | 172000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 11.3     |\n",
      "|    n_updates        | 42749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=-35280.44 +/- 18393.08\n",
      "Episode length: 800.20 +/- 247.88\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 800       |\n",
      "|    mean_reward      | -3.53e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 175000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.32      |\n",
      "|    n_updates        | 43499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 14309    |\n",
      "|    total_timesteps  | 176000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 7.75     |\n",
      "|    n_updates        | 43749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-5775.34 +/- 1716.65\n",
      "Episode length: 332.20 +/- 52.93\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 332       |\n",
      "|    mean_reward      | -5.78e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 180000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13        |\n",
      "|    n_updates        | 44749     |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 14518    |\n",
      "|    total_timesteps  | 180000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 14713    |\n",
      "|    total_timesteps  | 184000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.1     |\n",
      "|    n_updates        | 45749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-50205.64 +/- 15.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 185000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15        |\n",
      "|    n_updates        | 45999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 14969    |\n",
      "|    total_timesteps  | 188000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 7.79     |\n",
      "|    n_updates        | 46749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-50208.19 +/- 10.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 190000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 7.75      |\n",
      "|    n_updates        | 47249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 15277    |\n",
      "|    total_timesteps  | 192000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 9.87     |\n",
      "|    n_updates        | 47749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-12453.23 +/- 4103.91\n",
      "Episode length: 488.80 +/- 86.46\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 489       |\n",
      "|    mean_reward      | -1.25e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 195000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.31      |\n",
      "|    n_updates        | 48499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 15532    |\n",
      "|    total_timesteps  | 196000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 8.6      |\n",
      "|    n_updates        | 48749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-50193.29 +/- 9.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 200000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.5      |\n",
      "|    n_updates        | 49749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 15801    |\n",
      "|    total_timesteps  | 200000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 16023    |\n",
      "|    total_timesteps  | 204000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 10.4     |\n",
      "|    n_updates        | 50749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=-50207.50 +/- 6.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 205000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11        |\n",
      "|    n_updates        | 50999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 16343    |\n",
      "|    total_timesteps  | 208000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 9.61     |\n",
      "|    n_updates        | 51749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-50207.03 +/- 12.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 210000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.1      |\n",
      "|    n_updates        | 52249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 212       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 16594     |\n",
      "|    total_timesteps  | 212000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.8      |\n",
      "|    n_updates        | 52749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=-50199.78 +/- 8.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 215000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.1      |\n",
      "|    n_updates        | 53499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 216       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 16949     |\n",
      "|    total_timesteps  | 216000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 6.23      |\n",
      "|    n_updates        | 53749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-50209.80 +/- 12.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 220000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 7.95      |\n",
      "|    n_updates        | 54749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 220       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 17235     |\n",
      "|    total_timesteps  | 220000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 224       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 17476     |\n",
      "|    total_timesteps  | 224000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.7      |\n",
      "|    n_updates        | 55749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=-50203.18 +/- 9.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 225000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.8      |\n",
      "|    n_updates        | 55999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 228       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 17747     |\n",
      "|    total_timesteps  | 228000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.3      |\n",
      "|    n_updates        | 56749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-50210.61 +/- 10.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 230000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 7.12      |\n",
      "|    n_updates        | 57249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 232       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 18058     |\n",
      "|    total_timesteps  | 232000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.8      |\n",
      "|    n_updates        | 57749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=-50197.35 +/- 13.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 235000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 7.38      |\n",
      "|    n_updates        | 58499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 236       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 18404     |\n",
      "|    total_timesteps  | 236000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.2      |\n",
      "|    n_updates        | 58749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 999       |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 240       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 18631     |\n",
      "|    total_timesteps  | 239917    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.5      |\n",
      "|    n_updates        | 59729     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-50201.37 +/- 16.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 240000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.4      |\n",
      "|    n_updates        | 59749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 244       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 18947     |\n",
      "|    total_timesteps  | 244000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.88      |\n",
      "|    n_updates        | 60749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=-50140.61 +/- 14.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.01e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 245000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.4      |\n",
      "|    n_updates        | 60999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 248       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 19185     |\n",
      "|    total_timesteps  | 248000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.4      |\n",
      "|    n_updates        | 61749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=-50199.17 +/- 10.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 250000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.7      |\n",
      "|    n_updates        | 62249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 252       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 19424     |\n",
      "|    total_timesteps  | 252000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.7      |\n",
      "|    n_updates        | 62749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=-50196.72 +/- 12.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 255000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.1      |\n",
      "|    n_updates        | 63499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 256       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 19708     |\n",
      "|    total_timesteps  | 256000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.8      |\n",
      "|    n_updates        | 63749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-50212.10 +/- 19.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 260000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.7      |\n",
      "|    n_updates        | 64749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 260       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 20027     |\n",
      "|    total_timesteps  | 260000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 264       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 20238     |\n",
      "|    total_timesteps  | 264000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.25      |\n",
      "|    n_updates        | 65749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=-50192.29 +/- 23.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 265000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.3      |\n",
      "|    n_updates        | 65999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 268       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 20503     |\n",
      "|    total_timesteps  | 268000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.2      |\n",
      "|    n_updates        | 66749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=-50206.48 +/- 7.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 270000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.2      |\n",
      "|    n_updates        | 67249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 272       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 20809     |\n",
      "|    total_timesteps  | 272000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.3      |\n",
      "|    n_updates        | 67749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=-50203.23 +/- 14.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 275000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9         |\n",
      "|    n_updates        | 68499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 276       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 21096     |\n",
      "|    total_timesteps  | 276000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.21      |\n",
      "|    n_updates        | 68749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-50217.40 +/- 7.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 280000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 26        |\n",
      "|    n_updates        | 69749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 280       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 21458     |\n",
      "|    total_timesteps  | 280000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 284       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 21695     |\n",
      "|    total_timesteps  | 284000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.9      |\n",
      "|    n_updates        | 70749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=-50188.95 +/- 21.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 285000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.2      |\n",
      "|    n_updates        | 70999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 288       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 22075     |\n",
      "|    total_timesteps  | 288000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10        |\n",
      "|    n_updates        | 71749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-50205.28 +/- 8.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 290000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.7      |\n",
      "|    n_updates        | 72249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 292       |\n",
      "|    fps              | 13        |\n",
      "|    time_elapsed     | 22438     |\n",
      "|    total_timesteps  | 292000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.6      |\n",
      "|    n_updates        | 72749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=-50205.35 +/- 12.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 295000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.2      |\n",
      "|    n_updates        | 73499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 296       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 22801     |\n",
      "|    total_timesteps  | 296000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.8      |\n",
      "|    n_updates        | 73749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-50201.09 +/- 6.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 300000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.8      |\n",
      "|    n_updates        | 74749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 300       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 23161     |\n",
      "|    total_timesteps  | 300000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 304       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 23397     |\n",
      "|    total_timesteps  | 304000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.1      |\n",
      "|    n_updates        | 75749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=305000, episode_reward=-50206.24 +/- 8.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 305000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.3      |\n",
      "|    n_updates        | 75999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 308       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 23758     |\n",
      "|    total_timesteps  | 308000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.4      |\n",
      "|    n_updates        | 76749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=-50206.38 +/- 10.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 310000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.4      |\n",
      "|    n_updates        | 77249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 312       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 24120     |\n",
      "|    total_timesteps  | 312000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.26      |\n",
      "|    n_updates        | 77749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=315000, episode_reward=-50198.97 +/- 6.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 315000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.7      |\n",
      "|    n_updates        | 78499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 316       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 24485     |\n",
      "|    total_timesteps  | 316000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.8      |\n",
      "|    n_updates        | 78749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-50204.55 +/- 9.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 320000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.34      |\n",
      "|    n_updates        | 79749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 320       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 24848     |\n",
      "|    total_timesteps  | 320000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 324       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 25085     |\n",
      "|    total_timesteps  | 324000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.6      |\n",
      "|    n_updates        | 80749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=325000, episode_reward=-50201.25 +/- 12.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 325000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16        |\n",
      "|    n_updates        | 80999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 997       |\n",
      "|    ep_rew_mean      | -4.98e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 328       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 25433     |\n",
      "|    total_timesteps  | 327697    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.2      |\n",
      "|    n_updates        | 81674     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=-50213.95 +/- 4.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 330000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.6      |\n",
      "|    n_updates        | 82249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -4.99e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 332       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 25814     |\n",
      "|    total_timesteps  | 332000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.4      |\n",
      "|    n_updates        | 82749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=335000, episode_reward=-50207.87 +/- 8.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 335000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.8      |\n",
      "|    n_updates        | 83499     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -4.99e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 336       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 26180     |\n",
      "|    total_timesteps  | 336000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.9      |\n",
      "|    n_updates        | 83749     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-50192.00 +/- 22.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 340000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.8      |\n",
      "|    n_updates        | 84749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 26546    |\n",
      "|    total_timesteps  | 340000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 26785    |\n",
      "|    total_timesteps  | 344000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13       |\n",
      "|    n_updates        | 85749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=345000, episode_reward=-50208.73 +/- 11.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 345000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 27        |\n",
      "|    n_updates        | 85999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 27153    |\n",
      "|    total_timesteps  | 348000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.6     |\n",
      "|    n_updates        | 86749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=-50210.24 +/- 6.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 350000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.8      |\n",
      "|    n_updates        | 87249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 27516    |\n",
      "|    total_timesteps  | 352000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 14.5     |\n",
      "|    n_updates        | 87749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=355000, episode_reward=-50197.29 +/- 17.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 355000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.7      |\n",
      "|    n_updates        | 88499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 27880    |\n",
      "|    total_timesteps  | 356000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 16       |\n",
      "|    n_updates        | 88749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-50214.35 +/- 11.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 360000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 24.8      |\n",
      "|    n_updates        | 89749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 28245    |\n",
      "|    total_timesteps  | 360000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 28483    |\n",
      "|    total_timesteps  | 364000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.6     |\n",
      "|    n_updates        | 90749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=365000, episode_reward=-41878.43 +/- 4469.47\n",
      "Episode length: 911.80 +/- 48.47\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 912       |\n",
      "|    mean_reward      | -4.19e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 365000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.4      |\n",
      "|    n_updates        | 90999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 28834    |\n",
      "|    total_timesteps  | 368000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 11.6     |\n",
      "|    n_updates        | 91749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=-50216.02 +/- 11.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 370000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.4      |\n",
      "|    n_updates        | 92249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 29199    |\n",
      "|    total_timesteps  | 372000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 21.4     |\n",
      "|    n_updates        | 92749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=375000, episode_reward=-50195.37 +/- 11.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 375000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.1      |\n",
      "|    n_updates        | 93499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 29565    |\n",
      "|    total_timesteps  | 376000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 28.1     |\n",
      "|    n_updates        | 93749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-36103.28 +/- 6437.66\n",
      "Episode length: 844.00 +/- 73.12\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 844       |\n",
      "|    mean_reward      | -3.61e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 380000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.2      |\n",
      "|    n_updates        | 94749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 29909    |\n",
      "|    total_timesteps  | 380000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 30145    |\n",
      "|    total_timesteps  | 384000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 14.6     |\n",
      "|    n_updates        | 95749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=385000, episode_reward=-50204.37 +/- 15.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 385000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.7      |\n",
      "|    n_updates        | 95999     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 30509    |\n",
      "|    total_timesteps  | 388000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 12.2     |\n",
      "|    n_updates        | 96749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=-50209.47 +/- 12.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 390000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.9      |\n",
      "|    n_updates        | 97249     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 30875    |\n",
      "|    total_timesteps  | 392000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 11.5     |\n",
      "|    n_updates        | 97749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=395000, episode_reward=-50208.11 +/- 7.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 395000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.4      |\n",
      "|    n_updates        | 98499     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 31241    |\n",
      "|    total_timesteps  | 396000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 12.3     |\n",
      "|    n_updates        | 98749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-50201.55 +/- 8.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 400000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.7      |\n",
      "|    n_updates        | 99749     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 31608    |\n",
      "|    total_timesteps  | 400000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 31849    |\n",
      "|    total_timesteps  | 404000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 11.2     |\n",
      "|    n_updates        | 100749   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=405000, episode_reward=-50211.67 +/- 3.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 405000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.2      |\n",
      "|    n_updates        | 100999    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 32223    |\n",
      "|    total_timesteps  | 408000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 13.2     |\n",
      "|    n_updates        | 101749   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=-50202.65 +/- 19.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 410000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.5      |\n",
      "|    n_updates        | 102249    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 32595    |\n",
      "|    total_timesteps  | 412000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 14.2     |\n",
      "|    n_updates        | 102749   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=415000, episode_reward=-50214.33 +/- 9.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 415000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.6      |\n",
      "|    n_updates        | 103499    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 32979    |\n",
      "|    total_timesteps  | 416000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 8.73     |\n",
      "|    n_updates        | 103749   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-50147.66 +/- 31.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.01e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 420000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.7      |\n",
      "|    n_updates        | 104749    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 33351    |\n",
      "|    total_timesteps  | 420000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 33595    |\n",
      "|    total_timesteps  | 424000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 16.8     |\n",
      "|    n_updates        | 105749   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=425000, episode_reward=-50205.00 +/- 13.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 425000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 8.23      |\n",
      "|    n_updates        | 105999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 428       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 33963     |\n",
      "|    total_timesteps  | 428000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.5      |\n",
      "|    n_updates        | 106749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=-50181.91 +/- 14.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 430000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.1      |\n",
      "|    n_updates        | 107249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 432       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 34334     |\n",
      "|    total_timesteps  | 432000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.9      |\n",
      "|    n_updates        | 107749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=435000, episode_reward=-50208.04 +/- 13.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 435000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.5      |\n",
      "|    n_updates        | 108499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 436       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 34706     |\n",
      "|    total_timesteps  | 436000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.3      |\n",
      "|    n_updates        | 108749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-10668.34 +/- 7193.35\n",
      "Episode length: 439.00 +/- 136.50\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 439       |\n",
      "|    mean_reward      | -1.07e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 440000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.8      |\n",
      "|    n_updates        | 109749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 440       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 35003     |\n",
      "|    total_timesteps  | 440000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 444       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 35246     |\n",
      "|    total_timesteps  | 444000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.51      |\n",
      "|    n_updates        | 110749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=445000, episode_reward=-50214.01 +/- 7.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 445000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.4      |\n",
      "|    n_updates        | 110999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 448       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 35616     |\n",
      "|    total_timesteps  | 448000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.3      |\n",
      "|    n_updates        | 111749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=-50187.17 +/- 20.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 450000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19        |\n",
      "|    n_updates        | 112249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 452       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 35986     |\n",
      "|    total_timesteps  | 452000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.6      |\n",
      "|    n_updates        | 112749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=455000, episode_reward=-29709.04 +/- 21149.71\n",
      "Episode length: 692.60 +/- 334.04\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 693       |\n",
      "|    mean_reward      | -2.97e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 455000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.8      |\n",
      "|    n_updates        | 113499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 456       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 36318     |\n",
      "|    total_timesteps  | 456000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.5      |\n",
      "|    n_updates        | 113749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-50212.48 +/- 7.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 460000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.8      |\n",
      "|    n_updates        | 114749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 460       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 36687     |\n",
      "|    total_timesteps  | 460000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 464       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 36929     |\n",
      "|    total_timesteps  | 464000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.8      |\n",
      "|    n_updates        | 115749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=465000, episode_reward=-50200.33 +/- 11.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 465000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.6      |\n",
      "|    n_updates        | 115999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 468       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 37301     |\n",
      "|    total_timesteps  | 468000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.1      |\n",
      "|    n_updates        | 116749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=-50210.62 +/- 12.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 470000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.7      |\n",
      "|    n_updates        | 117249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 472       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 37676     |\n",
      "|    total_timesteps  | 472000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.5      |\n",
      "|    n_updates        | 117749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=475000, episode_reward=-50171.23 +/- 25.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 475000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 29.3      |\n",
      "|    n_updates        | 118499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 476       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 38048     |\n",
      "|    total_timesteps  | 476000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.9      |\n",
      "|    n_updates        | 118749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-50188.59 +/- 30.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 480000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.4      |\n",
      "|    n_updates        | 119749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 480       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 38418     |\n",
      "|    total_timesteps  | 480000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 484       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 38661     |\n",
      "|    total_timesteps  | 484000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.6      |\n",
      "|    n_updates        | 120749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=485000, episode_reward=-50210.66 +/- 15.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 485000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.9      |\n",
      "|    n_updates        | 120999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 488       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 39035     |\n",
      "|    total_timesteps  | 488000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14        |\n",
      "|    n_updates        | 121749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=-50198.43 +/- 4.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 490000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.2      |\n",
      "|    n_updates        | 122249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 492       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 39409     |\n",
      "|    total_timesteps  | 492000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.4      |\n",
      "|    n_updates        | 122749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=495000, episode_reward=-50212.10 +/- 11.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 495000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.4      |\n",
      "|    n_updates        | 123499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 496       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 39779     |\n",
      "|    total_timesteps  | 496000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.4      |\n",
      "|    n_updates        | 123749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-50209.52 +/- 11.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 500000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.8      |\n",
      "|    n_updates        | 124749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 500       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 40168     |\n",
      "|    total_timesteps  | 500000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 504       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 40417     |\n",
      "|    total_timesteps  | 504000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.9      |\n",
      "|    n_updates        | 125749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=505000, episode_reward=-50199.54 +/- 15.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 505000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.1      |\n",
      "|    n_updates        | 125999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 508       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 40798     |\n",
      "|    total_timesteps  | 508000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.3      |\n",
      "|    n_updates        | 126749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=-50215.74 +/- 10.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 510000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19        |\n",
      "|    n_updates        | 127249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 512       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 41172     |\n",
      "|    total_timesteps  | 512000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.9      |\n",
      "|    n_updates        | 127749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=515000, episode_reward=-50194.78 +/- 13.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 515000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.5      |\n",
      "|    n_updates        | 128499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 516       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 41547     |\n",
      "|    total_timesteps  | 516000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.6      |\n",
      "|    n_updates        | 128749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-50204.87 +/- 15.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 520000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.9      |\n",
      "|    n_updates        | 129749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 520       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 41927     |\n",
      "|    total_timesteps  | 520000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 524       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 42174     |\n",
      "|    total_timesteps  | 524000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15        |\n",
      "|    n_updates        | 130749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=525000, episode_reward=-50212.83 +/- 14.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 525000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.2      |\n",
      "|    n_updates        | 130999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 528       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 42547     |\n",
      "|    total_timesteps  | 528000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.4      |\n",
      "|    n_updates        | 131749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=-50217.56 +/- 8.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 530000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.4      |\n",
      "|    n_updates        | 132249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 532       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 42926     |\n",
      "|    total_timesteps  | 532000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.4      |\n",
      "|    n_updates        | 132749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=535000, episode_reward=-50216.35 +/- 5.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 535000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.7      |\n",
      "|    n_updates        | 133499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 536       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 43302     |\n",
      "|    total_timesteps  | 536000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.6      |\n",
      "|    n_updates        | 133749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-50165.65 +/- 26.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 540000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.3      |\n",
      "|    n_updates        | 134749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 540       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 43680     |\n",
      "|    total_timesteps  | 540000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 544       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 43929     |\n",
      "|    total_timesteps  | 544000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.6      |\n",
      "|    n_updates        | 135749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=545000, episode_reward=-50213.11 +/- 15.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 545000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.5      |\n",
      "|    n_updates        | 135999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 548       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 44306     |\n",
      "|    total_timesteps  | 548000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.5      |\n",
      "|    n_updates        | 136749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=-50215.36 +/- 12.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 550000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.8      |\n",
      "|    n_updates        | 137249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 552       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 44683     |\n",
      "|    total_timesteps  | 552000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.8      |\n",
      "|    n_updates        | 137749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=555000, episode_reward=-50208.38 +/- 10.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 555000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.1      |\n",
      "|    n_updates        | 138499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 556       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 45061     |\n",
      "|    total_timesteps  | 556000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14        |\n",
      "|    n_updates        | 138749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-47902.41 +/- 3268.16\n",
      "Episode length: 976.40 +/- 34.58\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 976       |\n",
      "|    mean_reward      | -4.79e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 560000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.4      |\n",
      "|    n_updates        | 139749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 560       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 45434     |\n",
      "|    total_timesteps  | 560000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 564       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 45684     |\n",
      "|    total_timesteps  | 564000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.7      |\n",
      "|    n_updates        | 140749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=565000, episode_reward=-50194.98 +/- 5.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 565000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.7      |\n",
      "|    n_updates        | 140999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 568       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 46064     |\n",
      "|    total_timesteps  | 568000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.2      |\n",
      "|    n_updates        | 141749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=-50202.83 +/- 9.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 570000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.6      |\n",
      "|    n_updates        | 142249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 572       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 46448     |\n",
      "|    total_timesteps  | 572000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.8      |\n",
      "|    n_updates        | 142749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=575000, episode_reward=-50191.37 +/- 11.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 575000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.4      |\n",
      "|    n_updates        | 143499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 576       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 46825     |\n",
      "|    total_timesteps  | 576000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.9      |\n",
      "|    n_updates        | 143749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-50210.62 +/- 7.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 580000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.8      |\n",
      "|    n_updates        | 144749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 580       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 47203     |\n",
      "|    total_timesteps  | 580000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 584       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 47453     |\n",
      "|    total_timesteps  | 584000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 25.8      |\n",
      "|    n_updates        | 145749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=585000, episode_reward=-50217.94 +/- 10.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 585000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.1      |\n",
      "|    n_updates        | 145999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 588       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 47836     |\n",
      "|    total_timesteps  | 588000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.18      |\n",
      "|    n_updates        | 146749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=-50205.60 +/- 19.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 590000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.5      |\n",
      "|    n_updates        | 147249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 592       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 48214     |\n",
      "|    total_timesteps  | 592000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 25.9      |\n",
      "|    n_updates        | 147749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=595000, episode_reward=-50209.31 +/- 8.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 595000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.7      |\n",
      "|    n_updates        | 148499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 596       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 48595     |\n",
      "|    total_timesteps  | 596000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.2      |\n",
      "|    n_updates        | 148749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-50202.02 +/- 12.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 600000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.2      |\n",
      "|    n_updates        | 149749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 600       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 48983     |\n",
      "|    total_timesteps  | 600000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 604       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 49234     |\n",
      "|    total_timesteps  | 604000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.8      |\n",
      "|    n_updates        | 150749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=605000, episode_reward=-50208.29 +/- 13.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 605000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.9      |\n",
      "|    n_updates        | 150999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 608       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 49614     |\n",
      "|    total_timesteps  | 608000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16        |\n",
      "|    n_updates        | 151749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=-50205.95 +/- 19.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 610000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.2      |\n",
      "|    n_updates        | 152249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 999       |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 612       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 49992     |\n",
      "|    total_timesteps  | 611941    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.5      |\n",
      "|    n_updates        | 152735    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=615000, episode_reward=-50201.18 +/- 18.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 615000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.7      |\n",
      "|    n_updates        | 153499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 616       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 50377     |\n",
      "|    total_timesteps  | 616000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.9      |\n",
      "|    n_updates        | 153749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-13082.57 +/- 5533.32\n",
      "Episode length: 494.20 +/- 123.18\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 494       |\n",
      "|    mean_reward      | -1.31e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 620000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.1      |\n",
      "|    n_updates        | 154749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 620       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 50693     |\n",
      "|    total_timesteps  | 620000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 624       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 50944     |\n",
      "|    total_timesteps  | 624000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.3      |\n",
      "|    n_updates        | 155749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=625000, episode_reward=-50195.24 +/- 16.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 625000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 25.6      |\n",
      "|    n_updates        | 155999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 628       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 51328     |\n",
      "|    total_timesteps  | 628000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.8      |\n",
      "|    n_updates        | 156749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=-50199.18 +/- 13.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 630000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.9      |\n",
      "|    n_updates        | 157249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 632       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 51714     |\n",
      "|    total_timesteps  | 632000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19        |\n",
      "|    n_updates        | 157749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=635000, episode_reward=-50213.33 +/- 15.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 635000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11.5      |\n",
      "|    n_updates        | 158499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 636       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 52098     |\n",
      "|    total_timesteps  | 636000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.9      |\n",
      "|    n_updates        | 158749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=-50206.66 +/- 7.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 640000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 24.8      |\n",
      "|    n_updates        | 159749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 640       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 52457     |\n",
      "|    total_timesteps  | 640000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 644       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 52693     |\n",
      "|    total_timesteps  | 644000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.1      |\n",
      "|    n_updates        | 160749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=645000, episode_reward=-50193.48 +/- 15.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 645000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 23.1      |\n",
      "|    n_updates        | 160999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 648       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 53017     |\n",
      "|    total_timesteps  | 648000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 24.5      |\n",
      "|    n_updates        | 161749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=-50216.17 +/- 10.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 650000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.2      |\n",
      "|    n_updates        | 162249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 652       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 53305     |\n",
      "|    total_timesteps  | 652000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 26.3      |\n",
      "|    n_updates        | 162749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=655000, episode_reward=-50208.79 +/- 19.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 655000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.5      |\n",
      "|    n_updates        | 163499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 656       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 53593     |\n",
      "|    total_timesteps  | 656000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22        |\n",
      "|    n_updates        | 163749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-50203.31 +/- 12.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 660000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.7      |\n",
      "|    n_updates        | 164749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 660       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 53843     |\n",
      "|    total_timesteps  | 660000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 664       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 54100     |\n",
      "|    total_timesteps  | 664000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.1      |\n",
      "|    n_updates        | 165749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=665000, episode_reward=-50216.05 +/- 12.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 665000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.4      |\n",
      "|    n_updates        | 165999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 668       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 54510     |\n",
      "|    total_timesteps  | 668000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.9      |\n",
      "|    n_updates        | 166749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=-50196.67 +/- 6.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 670000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.9      |\n",
      "|    n_updates        | 167249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 672       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 55077     |\n",
      "|    total_timesteps  | 672000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.2      |\n",
      "|    n_updates        | 167749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=675000, episode_reward=-50202.87 +/- 16.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 675000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.9      |\n",
      "|    n_updates        | 168499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 676       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 55545     |\n",
      "|    total_timesteps  | 676000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.2      |\n",
      "|    n_updates        | 168749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-50219.50 +/- 8.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 680000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.1      |\n",
      "|    n_updates        | 169749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 680       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 55896     |\n",
      "|    total_timesteps  | 680000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 684       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 56113     |\n",
      "|    total_timesteps  | 684000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 43.2      |\n",
      "|    n_updates        | 170749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=685000, episode_reward=-50208.22 +/- 15.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 685000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 34.6      |\n",
      "|    n_updates        | 170999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 688       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 56446     |\n",
      "|    total_timesteps  | 688000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.3      |\n",
      "|    n_updates        | 171749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=-50198.93 +/- 17.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 690000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.6      |\n",
      "|    n_updates        | 172249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 692       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 56782     |\n",
      "|    total_timesteps  | 692000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 11        |\n",
      "|    n_updates        | 172749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=695000, episode_reward=-50208.12 +/- 9.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 695000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.5      |\n",
      "|    n_updates        | 173499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 696       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 57140     |\n",
      "|    total_timesteps  | 696000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.5      |\n",
      "|    n_updates        | 173749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=-30409.64 +/- 1976.13\n",
      "Episode length: 777.00 +/- 25.44\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 777       |\n",
      "|    mean_reward      | -3.04e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 700000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.3      |\n",
      "|    n_updates        | 174749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 700       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 57523     |\n",
      "|    total_timesteps  | 700000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 704       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 57802     |\n",
      "|    total_timesteps  | 704000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14        |\n",
      "|    n_updates        | 175749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=705000, episode_reward=-50212.34 +/- 7.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 705000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.9      |\n",
      "|    n_updates        | 175999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 708       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 58248     |\n",
      "|    total_timesteps  | 708000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.8      |\n",
      "|    n_updates        | 176749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=-50205.71 +/- 6.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 710000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 27.9      |\n",
      "|    n_updates        | 177249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 712       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 58654     |\n",
      "|    total_timesteps  | 712000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.1      |\n",
      "|    n_updates        | 177749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=715000, episode_reward=-50202.98 +/- 11.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 715000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 26.4      |\n",
      "|    n_updates        | 178499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 716       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 59012     |\n",
      "|    total_timesteps  | 716000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 34        |\n",
      "|    n_updates        | 178749    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 999      |\n",
      "|    ep_rew_mean      | -5e+04   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 59243    |\n",
      "|    total_timesteps  | 719871   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 22.9     |\n",
      "|    n_updates        | 179717   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-50212.21 +/- 12.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 720000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.6      |\n",
      "|    n_updates        | 179749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 724       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 59617     |\n",
      "|    total_timesteps  | 724000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.4      |\n",
      "|    n_updates        | 180749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=725000, episode_reward=-50196.83 +/- 8.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 725000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13        |\n",
      "|    n_updates        | 180999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 728       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 59986     |\n",
      "|    total_timesteps  | 728000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.1      |\n",
      "|    n_updates        | 181749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=-50212.45 +/- 7.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 730000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.3      |\n",
      "|    n_updates        | 182249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 732       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 60334     |\n",
      "|    total_timesteps  | 732000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 23.8      |\n",
      "|    n_updates        | 182749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=735000, episode_reward=-50214.12 +/- 7.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 735000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.5      |\n",
      "|    n_updates        | 183499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 736       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 60672     |\n",
      "|    total_timesteps  | 736000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.53      |\n",
      "|    n_updates        | 183749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=-43336.48 +/- 6079.35\n",
      "Episode length: 926.60 +/- 66.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 927       |\n",
      "|    mean_reward      | -4.33e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 740000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.4      |\n",
      "|    n_updates        | 184749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 740       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 61000     |\n",
      "|    total_timesteps  | 740000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 744       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 61220     |\n",
      "|    total_timesteps  | 744000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.7      |\n",
      "|    n_updates        | 185749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=745000, episode_reward=-50206.16 +/- 10.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 745000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.3      |\n",
      "|    n_updates        | 185999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 748       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 61557     |\n",
      "|    total_timesteps  | 748000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22.9      |\n",
      "|    n_updates        | 186749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=-50199.46 +/- 17.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 750000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19        |\n",
      "|    n_updates        | 187249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 752       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 61896     |\n",
      "|    total_timesteps  | 752000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.7      |\n",
      "|    n_updates        | 187749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=755000, episode_reward=-31639.41 +/- 9715.70\n",
      "Episode length: 785.00 +/- 115.08\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 785       |\n",
      "|    mean_reward      | -3.16e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 755000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.1      |\n",
      "|    n_updates        | 188499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 756       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 62209     |\n",
      "|    total_timesteps  | 756000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.7      |\n",
      "|    n_updates        | 188749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=-50204.60 +/- 9.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 760000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 14.9      |\n",
      "|    n_updates        | 189749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 760       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 62547     |\n",
      "|    total_timesteps  | 760000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 764       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 62767     |\n",
      "|    total_timesteps  | 764000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 33.6      |\n",
      "|    n_updates        | 190749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=765000, episode_reward=-50185.43 +/- 32.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 765000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.6      |\n",
      "|    n_updates        | 190999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 768       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 63106     |\n",
      "|    total_timesteps  | 768000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 30.1      |\n",
      "|    n_updates        | 191749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=-50210.65 +/- 9.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 770000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.2      |\n",
      "|    n_updates        | 192249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 772       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 63448     |\n",
      "|    total_timesteps  | 772000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 33.9      |\n",
      "|    n_updates        | 192749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=775000, episode_reward=-50206.59 +/- 10.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 775000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13        |\n",
      "|    n_updates        | 193499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 776       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 63790     |\n",
      "|    total_timesteps  | 776000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 32        |\n",
      "|    n_updates        | 193749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-50199.40 +/- 29.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 780000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 15.1      |\n",
      "|    n_updates        | 194749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 780       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 64129     |\n",
      "|    total_timesteps  | 780000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 784       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 64347     |\n",
      "|    total_timesteps  | 784000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 33.6      |\n",
      "|    n_updates        | 195749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=785000, episode_reward=-50203.04 +/- 7.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 785000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.9      |\n",
      "|    n_updates        | 195999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 788       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 64687     |\n",
      "|    total_timesteps  | 788000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 23.5      |\n",
      "|    n_updates        | 196749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=-50209.26 +/- 11.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 790000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17        |\n",
      "|    n_updates        | 197249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 792       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 65026     |\n",
      "|    total_timesteps  | 792000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.1      |\n",
      "|    n_updates        | 197749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=795000, episode_reward=-50194.26 +/- 9.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 795000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 38.4      |\n",
      "|    n_updates        | 198499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 796       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 65367     |\n",
      "|    total_timesteps  | 796000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 31.8      |\n",
      "|    n_updates        | 198749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=-50212.25 +/- 13.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 800000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 27.1      |\n",
      "|    n_updates        | 199749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 800       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 65709     |\n",
      "|    total_timesteps  | 800000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 804       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 65930     |\n",
      "|    total_timesteps  | 804000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 10.7      |\n",
      "|    n_updates        | 200749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=805000, episode_reward=-50210.46 +/- 6.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 805000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 20.3      |\n",
      "|    n_updates        | 200999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 808       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 66274     |\n",
      "|    total_timesteps  | 808000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 9.56      |\n",
      "|    n_updates        | 201749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=-50207.07 +/- 12.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 810000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 31        |\n",
      "|    n_updates        | 202249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 812       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 66620     |\n",
      "|    total_timesteps  | 812000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.1      |\n",
      "|    n_updates        | 202749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=815000, episode_reward=-50210.20 +/- 8.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 815000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 26.7      |\n",
      "|    n_updates        | 203499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 816       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 66965     |\n",
      "|    total_timesteps  | 816000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 22        |\n",
      "|    n_updates        | 203749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=-50183.96 +/- 25.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 820000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13        |\n",
      "|    n_updates        | 204749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 820       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 67296     |\n",
      "|    total_timesteps  | 820000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 824       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 67526     |\n",
      "|    total_timesteps  | 824000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 28.7      |\n",
      "|    n_updates        | 205749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=825000, episode_reward=-50204.71 +/- 15.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 825000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.7      |\n",
      "|    n_updates        | 205999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 828       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 67890     |\n",
      "|    total_timesteps  | 828000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 12.1      |\n",
      "|    n_updates        | 206749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=-50204.52 +/- 14.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 830000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 17.3      |\n",
      "|    n_updates        | 207249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 832       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 68236     |\n",
      "|    total_timesteps  | 832000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 21.5      |\n",
      "|    n_updates        | 207749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=835000, episode_reward=-50208.19 +/- 5.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 835000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 27.3      |\n",
      "|    n_updates        | 208499    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 836       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 68559     |\n",
      "|    total_timesteps  | 836000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 18.3      |\n",
      "|    n_updates        | 208749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=-50202.96 +/- 4.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 840000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16.8      |\n",
      "|    n_updates        | 209749    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 840       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 68879     |\n",
      "|    total_timesteps  | 840000    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 844       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 69081     |\n",
      "|    total_timesteps  | 844000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 16        |\n",
      "|    n_updates        | 210749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=845000, episode_reward=-50196.14 +/- 13.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 1e+03     |\n",
      "|    mean_reward      | -5.02e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 845000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 13.4      |\n",
      "|    n_updates        | 210999    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 848       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 69399     |\n",
      "|    total_timesteps  | 848000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.1      |\n",
      "|    n_updates        | 211749    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=-41928.25 +/- 6133.67\n",
      "Episode length: 910.60 +/- 70.55\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 911       |\n",
      "|    mean_reward      | -4.19e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 850000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 23.3      |\n",
      "|    n_updates        | 212249    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -5.02e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 852       |\n",
      "|    fps              | 12        |\n",
      "|    time_elapsed     | 69705     |\n",
      "|    total_timesteps  | 852000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0005    |\n",
      "|    loss             | 19.4      |\n",
      "|    n_updates        | 212749    |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simple DQN architecture\n",
    "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
    "\n",
    "# Set up the model with simpler hyperparameters\n",
    "model = DQN('CnnPolicy', custom_env, policy_kwargs=policy_kwargs, \n",
    "            verbose=1, buffer_size=50000, learning_starts=1000, \n",
    "            learning_rate=0.0005, batch_size=32, exploration_fraction=0.1,\n",
    "            exploration_final_eps=0.05)\n",
    "\n",
    "# Setup evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(custom_env, best_model_save_path=model_dir,\n",
    "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
    "                                         name_prefix='dqn_model_checkpoint')\n",
    "\n",
    "\n",
    "\n",
    "# Start training the model with callbacks for evaluation and checkpoints\n",
    "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the final model after training\n",
    "model.save(\"dqn_custom_env_model\")\n",
    "custom_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env.close()\n",
    "del custom_env\n",
    "foo = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env = EnhancedCarRacing(render_mode=\"rgb_array\")\n",
    "obs, info = custom_env.reset()\n",
    "custom_env = GrayscaleObservation(custom_env, keep_dim=True)\n",
    "custom_env = TimeLimit(custom_env, max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories\n",
    "logs_dir = 'PPO_env_mod_logs'\n",
    "logs_path = os.path.join(MODELS_DIR, logs_dir)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "tensorboard_dir = os.path.join(logs_path, \"tensorboard\")\n",
    "model_dir = os.path.join(logs_path, \"models\")\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\", custom_env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derpy/Documents/Fac/ano3/semestre1/ISIA/reinforcement-learning-with-gymnasium/.venv/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x73efe1397080> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x73efe1394e90>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 142       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 14        |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013724199 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 4.04e-05    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.35e+05    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 8.74e+05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derpy/Documents/Fac/ano3/semestre1/ISIA/reinforcement-learning-with-gymnasium/.venv/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=-50210.37 +/- 12.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010220876 |\n",
      "|    clip_fraction        | 0.0176      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.24e+05    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00163    |\n",
      "|    value_loss           | 8.78e+05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 51        |\n",
      "|    iterations      | 3         |\n",
      "|    time_elapsed    | 120       |\n",
      "|    total_timesteps | 6144      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012792959 |\n",
      "|    clip_fraction        | 0.0195      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.1e+05     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0023     |\n",
      "|    value_loss           | 8.73e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-50204.47 +/- 9.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008375072 |\n",
      "|    clip_fraction        | 0.0217      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.84e+05    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00245    |\n",
      "|    value_loss           | 8.67e+05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 47        |\n",
      "|    iterations      | 5         |\n",
      "|    time_elapsed    | 216       |\n",
      "|    total_timesteps | 10240     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 247         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014505267 |\n",
      "|    clip_fraction        | 0.0347      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.24e+05    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0029     |\n",
      "|    value_loss           | 8.62e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 278         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010195928 |\n",
      "|    clip_fraction        | 0.0298      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.14e+05    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00218    |\n",
      "|    value_loss           | 8.58e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-50210.27 +/- 13.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010377973 |\n",
      "|    clip_fraction        | 0.0516      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.14e+05    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00441    |\n",
      "|    value_loss           | 8.53e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 47        |\n",
      "|    iterations      | 8         |\n",
      "|    time_elapsed    | 341       |\n",
      "|    total_timesteps | 16384     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 373         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013702306 |\n",
      "|    clip_fraction        | 0.0192      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.56e+05    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    value_loss           | 8.49e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-50208.49 +/- 15.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014494494 |\n",
      "|    clip_fraction        | 0.0478      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.68e+05    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00429    |\n",
      "|    value_loss           | 8.45e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 10        |\n",
      "|    time_elapsed    | 437       |\n",
      "|    total_timesteps | 20480     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 468         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011236157 |\n",
      "|    clip_fraction        | 0.0563      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.62e+05    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00541    |\n",
      "|    value_loss           | 8.41e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 499         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006577569 |\n",
      "|    clip_fraction        | 0.0188      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.2e+05     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00125    |\n",
      "|    value_loss           | 8.37e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-50204.77 +/- 9.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 25000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073979306 |\n",
      "|    clip_fraction        | 0.0435       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.57        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.97e+05     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0045      |\n",
      "|    value_loss           | 8.33e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 47        |\n",
      "|    iterations      | 13        |\n",
      "|    time_elapsed    | 562       |\n",
      "|    total_timesteps | 26624     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 593         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010400828 |\n",
      "|    clip_fraction        | 0.0455      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.89e+05    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00351    |\n",
      "|    value_loss           | 8.29e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-50201.14 +/- 18.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0117651755 |\n",
      "|    clip_fraction        | 0.0276       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.58        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 4.78e+05     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00266     |\n",
      "|    value_loss           | 8.25e+05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 15        |\n",
      "|    time_elapsed    | 656       |\n",
      "|    total_timesteps | 30720     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 686         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016954439 |\n",
      "|    clip_fraction        | 0.0706      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.61e+05    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00658    |\n",
      "|    value_loss           | 8.22e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 717         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013899915 |\n",
      "|    clip_fraction        | 0.0949      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.9e+05     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0093     |\n",
      "|    value_loss           | 8.19e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-50206.61 +/- 10.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011216195 |\n",
      "|    clip_fraction        | 0.0681      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.04e+05    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00545    |\n",
      "|    value_loss           | 8.16e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 47        |\n",
      "|    iterations      | 18        |\n",
      "|    time_elapsed    | 779       |\n",
      "|    total_timesteps | 36864     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 810         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010603748 |\n",
      "|    clip_fraction        | 0.0238      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.39e+05    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00195    |\n",
      "|    value_loss           | 8.13e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-50210.35 +/- 7.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010867569 |\n",
      "|    clip_fraction        | 0.0119      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.82e+05    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00148    |\n",
      "|    value_loss           | 8.11e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 20        |\n",
      "|    time_elapsed    | 873       |\n",
      "|    total_timesteps | 40960     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 47           |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 904          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0145029705 |\n",
      "|    clip_fraction        | 0.0473       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.47e+05     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00517     |\n",
      "|    value_loss           | 8.08e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-50212.10 +/- 13.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010793347 |\n",
      "|    clip_fraction        | 0.0324      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.59e+05    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00399    |\n",
      "|    value_loss           | 7.99e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 22        |\n",
      "|    time_elapsed    | 967       |\n",
      "|    total_timesteps | 45056     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 998         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010229749 |\n",
      "|    clip_fraction        | 0.0392      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.57e+05    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00375    |\n",
      "|    value_loss           | 7.71e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 1029        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015911173 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.75e+05    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 7.66e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-50215.22 +/- 8.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011767937 |\n",
      "|    clip_fraction        | 0.0311      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.1e+05     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00339    |\n",
      "|    value_loss           | 7.62e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 25        |\n",
      "|    time_elapsed    | 1091      |\n",
      "|    total_timesteps | 51200     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 1122        |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006926872 |\n",
      "|    clip_fraction        | 0.0253      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.31e+05    |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00201    |\n",
      "|    value_loss           | 7.57e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-50204.69 +/- 13.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011748089 |\n",
      "|    clip_fraction        | 0.0421      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.73e+05    |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00393    |\n",
      "|    value_loss           | 7.52e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 27        |\n",
      "|    time_elapsed    | 1184      |\n",
      "|    total_timesteps | 55296     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 1215        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007009603 |\n",
      "|    clip_fraction        | 0.0214      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.81e+05    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    value_loss           | 7.48e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 47         |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 1246       |\n",
      "|    total_timesteps      | 59392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00558946 |\n",
      "|    clip_fraction        | 0.0478     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.29      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 4.67e+05   |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.00337   |\n",
      "|    value_loss           | 7.44e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-50209.82 +/- 20.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056785014 |\n",
      "|    clip_fraction        | 0.0277       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.35e+05     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00234     |\n",
      "|    value_loss           | 7.4e+05      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 30        |\n",
      "|    time_elapsed    | 1308      |\n",
      "|    total_timesteps | 61440     |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 47         |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 1339       |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00419306 |\n",
      "|    clip_fraction        | 0.0236     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.29      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 3.48e+05   |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.00154   |\n",
      "|    value_loss           | 7.36e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-50202.99 +/- 9.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 65000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003839924 |\n",
      "|    clip_fraction        | 0.0133      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.47e+05    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.000896   |\n",
      "|    value_loss           | 7.32e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 32        |\n",
      "|    time_elapsed    | 1403      |\n",
      "|    total_timesteps | 65536     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 47           |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 1434         |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061841086 |\n",
      "|    clip_fraction        | 0.0127       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.54e+05     |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000808    |\n",
      "|    value_loss           | 7.28e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 1465        |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006395237 |\n",
      "|    clip_fraction        | 0.0203      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.52e+05    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00206    |\n",
      "|    value_loss           | 7.25e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-50215.82 +/- 14.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0139788035 |\n",
      "|    clip_fraction        | 0.0294       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.65e+05     |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00296     |\n",
      "|    value_loss           | 7.22e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 35        |\n",
      "|    time_elapsed    | 1528      |\n",
      "|    total_timesteps | 71680     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 1559        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005821702 |\n",
      "|    clip_fraction        | 0.0491      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.4e+05     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0048     |\n",
      "|    value_loss           | 7.19e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-50202.86 +/- 15.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 75000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005288278 |\n",
      "|    clip_fraction        | 0.0262      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.85e+05    |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00216    |\n",
      "|    value_loss           | 7.16e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 37        |\n",
      "|    time_elapsed    | 1622      |\n",
      "|    total_timesteps | 75776     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 47           |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 1653         |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035238136 |\n",
      "|    clip_fraction        | 0.00854      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.95e+05     |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.000206    |\n",
      "|    value_loss           | 7.13e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 47           |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 1684         |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026052445 |\n",
      "|    clip_fraction        | 0.00654      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.31e+05     |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.000224    |\n",
      "|    value_loss           | 7.11e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-50204.94 +/- 7.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019121744 |\n",
      "|    clip_fraction        | 0.0118       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.86e+05     |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.000366    |\n",
      "|    value_loss           | 7.08e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 40        |\n",
      "|    time_elapsed    | 1747      |\n",
      "|    total_timesteps | 81920     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 1778        |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009246077 |\n",
      "|    clip_fraction        | 0.0461      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.961      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.87e+05    |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00414    |\n",
      "|    value_loss           | 7.05e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-50204.97 +/- 9.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 85000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024573752 |\n",
      "|    clip_fraction        | 0.0202       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.915       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 4.12e+05     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    value_loss           | 7.03e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 42        |\n",
      "|    time_elapsed    | 1841      |\n",
      "|    total_timesteps | 86016     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 1872        |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004922283 |\n",
      "|    clip_fraction        | 0.0104      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.862      |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.03e+05    |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.000953   |\n",
      "|    value_loss           | 6.88e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-50196.61 +/- 4.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 90000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026731254 |\n",
      "|    clip_fraction        | 0.0187       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.813       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.99e+05     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00185     |\n",
      "|    value_loss           | 6.7e+05      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 44        |\n",
      "|    time_elapsed    | 1935      |\n",
      "|    total_timesteps | 90112     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 1966         |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023362995 |\n",
      "|    clip_fraction        | 0.00918      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.818       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.69e+05     |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    value_loss           | 6.65e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 1997        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004932112 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.739      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.32e+05    |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00305    |\n",
      "|    value_loss           | 6.61e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-50208.45 +/- 19.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 95000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013555193 |\n",
      "|    clip_fraction        | 0.00576      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.748       |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.95e+05     |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.000126    |\n",
      "|    value_loss           | 6.57e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 47        |\n",
      "|    time_elapsed    | 2061      |\n",
      "|    total_timesteps | 96256     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 2092         |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028100228 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.691       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.11e+05     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    value_loss           | 6.52e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-50202.32 +/- 14.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023495844 |\n",
      "|    clip_fraction        | 0.0043       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.679       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.28e+05     |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.000569    |\n",
      "|    value_loss           | 6.48e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 49        |\n",
      "|    time_elapsed    | 2155      |\n",
      "|    total_timesteps | 100352    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 2186         |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021202932 |\n",
      "|    clip_fraction        | 0.0214       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.704       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.28e+05     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    value_loss           | 6.44e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 47           |\n",
      "|    iterations           | 51           |\n",
      "|    time_elapsed         | 2217         |\n",
      "|    total_timesteps      | 104448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021213829 |\n",
      "|    clip_fraction        | 0.00454      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.673       |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.82e+05     |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.000296    |\n",
      "|    value_loss           | 6.4e+05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-50215.93 +/- 7.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 105000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025631865 |\n",
      "|    clip_fraction        | 0.0155       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.599       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.01e+05     |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.00138     |\n",
      "|    value_loss           | 6.37e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 52        |\n",
      "|    time_elapsed    | 2280      |\n",
      "|    total_timesteps | 106496    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 2312         |\n",
      "|    total_timesteps      | 108544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026315681 |\n",
      "|    clip_fraction        | 0.0385       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.655       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.3e+05      |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.0036      |\n",
      "|    value_loss           | 6.34e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-50208.51 +/- 5.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 110000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026465044 |\n",
      "|    clip_fraction        | 0.0233       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.676       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.3e+05      |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.00233     |\n",
      "|    value_loss           | 6.31e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 54        |\n",
      "|    time_elapsed    | 2375      |\n",
      "|    total_timesteps | 110592    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 2406        |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001996749 |\n",
      "|    clip_fraction        | 0.0111      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.659      |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.86e+05    |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.000792   |\n",
      "|    value_loss           | 6.27e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 47           |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 2437         |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025721372 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.713       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.77e+05     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.000893    |\n",
      "|    value_loss           | 6.25e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-50206.76 +/- 13.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 115000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041629523 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.622       |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.37e+05     |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    value_loss           | 6.22e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 57        |\n",
      "|    time_elapsed    | 2500      |\n",
      "|    total_timesteps | 116736    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 2531        |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002974799 |\n",
      "|    clip_fraction        | 0.0148      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.543      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.61e+05    |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    value_loss           | 6.19e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-50214.14 +/- 16.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026206668 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.485       |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.6e+05      |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    value_loss           | 6.17e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 59        |\n",
      "|    time_elapsed    | 2594      |\n",
      "|    total_timesteps | 120832    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 60            |\n",
      "|    time_elapsed         | 2625          |\n",
      "|    total_timesteps      | 122880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043072584 |\n",
      "|    clip_fraction        | 0.0063        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.502        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 2.82e+05      |\n",
      "|    n_updates            | 590           |\n",
      "|    policy_gradient_loss | -0.000336     |\n",
      "|    value_loss           | 6.14e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 47           |\n",
      "|    iterations           | 61           |\n",
      "|    time_elapsed         | 2657         |\n",
      "|    total_timesteps      | 124928       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013399357 |\n",
      "|    clip_fraction        | 0.00972      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.48        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.69e+05     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    value_loss           | 6.13e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-50210.38 +/- 7.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 125000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002116882 |\n",
      "|    clip_fraction        | 0.00835     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.443      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.19e+05    |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.000742   |\n",
      "|    value_loss           | 6.11e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 62        |\n",
      "|    time_elapsed    | 2719      |\n",
      "|    total_timesteps | 126976    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 2751         |\n",
      "|    total_timesteps      | 129024       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013227968 |\n",
      "|    clip_fraction        | 0.0106       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.404       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.98e+05     |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00108     |\n",
      "|    value_loss           | 6.09e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-50223.47 +/- 9.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 130000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017761893 |\n",
      "|    clip_fraction        | 0.00635      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.37        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.8e+05      |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.000684    |\n",
      "|    value_loss           | 5.9e+05      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 64        |\n",
      "|    time_elapsed    | 2813      |\n",
      "|    total_timesteps | 131072    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 65           |\n",
      "|    time_elapsed         | 2844         |\n",
      "|    total_timesteps      | 133120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011345871 |\n",
      "|    clip_fraction        | 0.00479      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.4         |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.43e+05     |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.000196    |\n",
      "|    value_loss           | 5.78e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=-50197.02 +/- 6.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 135000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012577865 |\n",
      "|    clip_fraction        | 0.00601      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.378       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.87e+05     |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.000318    |\n",
      "|    value_loss           | 5.74e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 66        |\n",
      "|    time_elapsed    | 2907      |\n",
      "|    total_timesteps | 135168    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 2938         |\n",
      "|    total_timesteps      | 137216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010511985 |\n",
      "|    clip_fraction        | 0.0109       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.385       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.88e+05     |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    value_loss           | 5.7e+05      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 2969        |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001235283 |\n",
      "|    clip_fraction        | 0.00718     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.419      |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.05e+05    |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.000951   |\n",
      "|    value_loss           | 5.66e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-50214.29 +/- 4.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018894721 |\n",
      "|    clip_fraction        | 0.0162       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.387       |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.26e+05     |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.00182     |\n",
      "|    value_loss           | 5.63e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 69        |\n",
      "|    time_elapsed    | 3033      |\n",
      "|    total_timesteps | 141312    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 3064         |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014510299 |\n",
      "|    clip_fraction        | 0.0134       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.323       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.2e+05      |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    value_loss           | 5.58e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=-50210.37 +/- 6.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 145000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009517177 |\n",
      "|    clip_fraction        | 0.0149       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.355       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.05e+05     |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    value_loss           | 5.55e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 71        |\n",
      "|    time_elapsed    | 3127      |\n",
      "|    total_timesteps | 145408    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 72            |\n",
      "|    time_elapsed         | 3158          |\n",
      "|    total_timesteps      | 147456        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00087343925 |\n",
      "|    clip_fraction        | 0.00767       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.387        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.93e+05      |\n",
      "|    n_updates            | 710           |\n",
      "|    policy_gradient_loss | -0.000697     |\n",
      "|    value_loss           | 5.52e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 73           |\n",
      "|    time_elapsed         | 3190         |\n",
      "|    total_timesteps      | 149504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016129299 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.398       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.22e+05     |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    value_loss           | 5.49e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-50204.48 +/- 13.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 150000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019319251 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.34        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.67e+05     |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.0033      |\n",
      "|    value_loss           | 5.46e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 74        |\n",
      "|    time_elapsed    | 3253      |\n",
      "|    total_timesteps | 151552    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 75           |\n",
      "|    time_elapsed         | 3284         |\n",
      "|    total_timesteps      | 153600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011655677 |\n",
      "|    clip_fraction        | 0.0129       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.366       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.24e+05     |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    value_loss           | 5.43e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=-50206.92 +/- 6.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 155000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00083768356 |\n",
      "|    clip_fraction        | 0.0107        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.378        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 2.96e+05      |\n",
      "|    n_updates            | 750           |\n",
      "|    policy_gradient_loss | -0.00147      |\n",
      "|    value_loss           | 5.4e+05       |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 76        |\n",
      "|    time_elapsed    | 3347      |\n",
      "|    total_timesteps | 155648    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 77           |\n",
      "|    time_elapsed         | 3378         |\n",
      "|    total_timesteps      | 157696       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015728234 |\n",
      "|    clip_fraction        | 0.0136       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.325       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.01e+05     |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    value_loss           | 5.37e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 3409         |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011648319 |\n",
      "|    clip_fraction        | 0.0177       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.362       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.58e+05     |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    value_loss           | 5.35e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-50203.14 +/- 13.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010934859 |\n",
      "|    clip_fraction        | 0.00996      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.33        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.93e+05     |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.00107     |\n",
      "|    value_loss           | 5.33e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 79        |\n",
      "|    time_elapsed    | 3473      |\n",
      "|    total_timesteps | 161792    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 3504         |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010552928 |\n",
      "|    clip_fraction        | 0.00942      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.354       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.85e+05     |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    value_loss           | 5.31e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=-50211.94 +/- 4.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 165000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011780087 |\n",
      "|    clip_fraction        | 0.00586      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.329       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.7e+05      |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    value_loss           | 5.29e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 81        |\n",
      "|    time_elapsed    | 3567      |\n",
      "|    total_timesteps | 165888    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 82            |\n",
      "|    time_elapsed         | 3598          |\n",
      "|    total_timesteps      | 167936        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00096013857 |\n",
      "|    clip_fraction        | 0.00923       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.311        |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 2.79e+05      |\n",
      "|    n_updates            | 810           |\n",
      "|    policy_gradient_loss | -0.00103      |\n",
      "|    value_loss           | 5.27e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 83            |\n",
      "|    time_elapsed         | 3630          |\n",
      "|    total_timesteps      | 169984        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00051083794 |\n",
      "|    clip_fraction        | 0.00405       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.296        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 2.44e+05      |\n",
      "|    n_updates            | 820           |\n",
      "|    policy_gradient_loss | -0.000158     |\n",
      "|    value_loss           | 5.26e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-50204.95 +/- 4.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 170000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036392617 |\n",
      "|    clip_fraction        | 0.00225       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.281        |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 3.07e+05      |\n",
      "|    n_updates            | 830           |\n",
      "|    policy_gradient_loss | -0.000295     |\n",
      "|    value_loss           | 5.25e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 84        |\n",
      "|    time_elapsed    | 3694      |\n",
      "|    total_timesteps | 172032    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 3725         |\n",
      "|    total_timesteps      | 174080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011726413 |\n",
      "|    clip_fraction        | 0.013        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.242       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.4e+05      |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    value_loss           | 5.03e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=-50204.45 +/- 23.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 175000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002640667 |\n",
      "|    clip_fraction        | 0.00171      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.244       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.18e+05     |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | 2.63e-05     |\n",
      "|    value_loss           | 4.97e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 86        |\n",
      "|    time_elapsed    | 3789      |\n",
      "|    total_timesteps | 176128    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 87            |\n",
      "|    time_elapsed         | 3821          |\n",
      "|    total_timesteps      | 178176        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00060253614 |\n",
      "|    clip_fraction        | 0.00693       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.24         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.8e+05       |\n",
      "|    n_updates            | 860           |\n",
      "|    policy_gradient_loss | -0.000987     |\n",
      "|    value_loss           | 4.94e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-50208.27 +/- 8.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 180000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047816942 |\n",
      "|    clip_fraction        | 0.00532       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.23         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 2.12e+05      |\n",
      "|    n_updates            | 870           |\n",
      "|    policy_gradient_loss | 1.86e-05      |\n",
      "|    value_loss           | 4.9e+05       |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 88        |\n",
      "|    time_elapsed    | 3884      |\n",
      "|    total_timesteps | 180224    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 3915         |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006229577 |\n",
      "|    clip_fraction        | 0.004        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.218       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.48e+05     |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.000633    |\n",
      "|    value_loss           | 4.86e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 3947         |\n",
      "|    total_timesteps      | 184320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008469126 |\n",
      "|    clip_fraction        | 0.0135       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.182       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.93e+05     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.00143     |\n",
      "|    value_loss           | 4.82e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-50201.29 +/- 21.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 185000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010540328 |\n",
      "|    clip_fraction        | 0.00132       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.187        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 2.85e+05      |\n",
      "|    n_updates            | 900           |\n",
      "|    policy_gradient_loss | -0.000128     |\n",
      "|    value_loss           | 4.79e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 91        |\n",
      "|    time_elapsed    | 4010      |\n",
      "|    total_timesteps | 186368    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 92            |\n",
      "|    time_elapsed         | 4042          |\n",
      "|    total_timesteps      | 188416        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00049624126 |\n",
      "|    clip_fraction        | 0.00718       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.188        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.73e+05      |\n",
      "|    n_updates            | 910           |\n",
      "|    policy_gradient_loss | -0.00125      |\n",
      "|    value_loss           | 4.76e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-50206.51 +/- 9.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 190000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00041591498 |\n",
      "|    clip_fraction        | 0.00352       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.192        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.78e+05      |\n",
      "|    n_updates            | 920           |\n",
      "|    policy_gradient_loss | -0.000533     |\n",
      "|    value_loss           | 4.73e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 93        |\n",
      "|    time_elapsed    | 4107      |\n",
      "|    total_timesteps | 190464    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 94           |\n",
      "|    time_elapsed         | 4138         |\n",
      "|    total_timesteps      | 192512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003429732 |\n",
      "|    clip_fraction        | 0.00195      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.18        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.88e+05     |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.00024     |\n",
      "|    value_loss           | 4.7e+05      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 4169         |\n",
      "|    total_timesteps      | 194560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007602639 |\n",
      "|    clip_fraction        | 0.00908      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.159       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.36e+05     |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.000818    |\n",
      "|    value_loss           | 4.68e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-50198.95 +/- 12.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 195000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.327878e-05 |\n",
      "|    clip_fraction        | 0.000537     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.156       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.75e+05     |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | -0.000144    |\n",
      "|    value_loss           | 4.66e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 96        |\n",
      "|    time_elapsed    | 4233      |\n",
      "|    total_timesteps | 196608    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 97            |\n",
      "|    time_elapsed         | 4264          |\n",
      "|    total_timesteps      | 198656        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029052593 |\n",
      "|    clip_fraction        | 0.000977      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.151        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.95e+05      |\n",
      "|    n_updates            | 960           |\n",
      "|    policy_gradient_loss | -0.000394     |\n",
      "|    value_loss           | 4.63e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-50198.79 +/- 15.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 200000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043948184 |\n",
      "|    clip_fraction        | 0.002         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.139        |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 2.77e+05      |\n",
      "|    n_updates            | 970           |\n",
      "|    policy_gradient_loss | -0.000263     |\n",
      "|    value_loss           | 4.61e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 98        |\n",
      "|    time_elapsed    | 4328      |\n",
      "|    total_timesteps | 200704    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 99            |\n",
      "|    time_elapsed         | 4359          |\n",
      "|    total_timesteps      | 202752        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035159691 |\n",
      "|    clip_fraction        | 0.00166       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.135        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.63e+05      |\n",
      "|    n_updates            | 980           |\n",
      "|    policy_gradient_loss | -0.000532     |\n",
      "|    value_loss           | 4.59e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 100           |\n",
      "|    time_elapsed         | 4391          |\n",
      "|    total_timesteps      | 204800        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011170303 |\n",
      "|    clip_fraction        | 0.000977      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.121        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 2.98e+05      |\n",
      "|    n_updates            | 990           |\n",
      "|    policy_gradient_loss | -0.000215     |\n",
      "|    value_loss           | 4.57e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=-50203.05 +/- 13.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 205000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012342798 |\n",
      "|    clip_fraction        | 0.00103       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.108        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 2.28e+05      |\n",
      "|    n_updates            | 1000          |\n",
      "|    policy_gradient_loss | -0.000229     |\n",
      "|    value_loss           | 4.55e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 101       |\n",
      "|    time_elapsed    | 4455      |\n",
      "|    total_timesteps | 206848    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 102          |\n",
      "|    time_elapsed         | 4487         |\n",
      "|    total_timesteps      | 208896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002498806 |\n",
      "|    clip_fraction        | 0.00747      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.12        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.62e+05     |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | -0.000984    |\n",
      "|    value_loss           | 4.54e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-50206.29 +/- 11.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 210000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004020117 |\n",
      "|    clip_fraction        | 0.00498      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.134       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 3.05e+05     |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.000847    |\n",
      "|    value_loss           | 4.53e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 103       |\n",
      "|    time_elapsed    | 4550      |\n",
      "|    total_timesteps | 210944    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 104          |\n",
      "|    time_elapsed         | 4583         |\n",
      "|    total_timesteps      | 212992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005570191 |\n",
      "|    clip_fraction        | 0.0128       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.155       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.54e+05     |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    value_loss           | 4.52e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=-50215.99 +/- 10.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 215000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019289524 |\n",
      "|    clip_fraction        | 0.00303       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.179        |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.21e+05      |\n",
      "|    n_updates            | 1040          |\n",
      "|    policy_gradient_loss | -0.000104     |\n",
      "|    value_loss           | 4.52e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 105       |\n",
      "|    time_elapsed    | 4647      |\n",
      "|    total_timesteps | 215040    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 106           |\n",
      "|    time_elapsed         | 4679          |\n",
      "|    total_timesteps      | 217088        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00061252934 |\n",
      "|    clip_fraction        | 0.0145        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.151        |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 2.04e+05      |\n",
      "|    n_updates            | 1050          |\n",
      "|    policy_gradient_loss | -0.00131      |\n",
      "|    value_loss           | 4.3e+05       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 107          |\n",
      "|    time_elapsed         | 4711         |\n",
      "|    total_timesteps      | 219136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001921138 |\n",
      "|    clip_fraction        | 0.000928     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.149       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.95e+05     |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -5.37e-05    |\n",
      "|    value_loss           | 4.27e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-50206.44 +/- 11.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 220000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.142062e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.126       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.32e+05     |\n",
      "|    n_updates            | 1070         |\n",
      "|    policy_gradient_loss | 6.7e-05      |\n",
      "|    value_loss           | 4.23e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 108       |\n",
      "|    time_elapsed    | 4777      |\n",
      "|    total_timesteps | 221184    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 109           |\n",
      "|    time_elapsed         | 4809          |\n",
      "|    total_timesteps      | 223232        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043438104 |\n",
      "|    clip_fraction        | 0.00508       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.144        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 2.23e+05      |\n",
      "|    n_updates            | 1080          |\n",
      "|    policy_gradient_loss | -0.000679     |\n",
      "|    value_loss           | 4.2e+05       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=-50215.71 +/- 6.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 225000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.084216e-05 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.153       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.24e+05     |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | -7.36e-05    |\n",
      "|    value_loss           | 4.16e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 110       |\n",
      "|    time_elapsed    | 4873      |\n",
      "|    total_timesteps | 225280    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 111          |\n",
      "|    time_elapsed         | 4906         |\n",
      "|    total_timesteps      | 227328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004013408 |\n",
      "|    clip_fraction        | 0.00269      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.136       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.19e+05     |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.000528    |\n",
      "|    value_loss           | 4.13e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 112          |\n",
      "|    time_elapsed         | 4939         |\n",
      "|    total_timesteps      | 229376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001623997 |\n",
      "|    clip_fraction        | 0.00313      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.149       |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.41e+05     |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | -0.000495    |\n",
      "|    value_loss           | 4.11e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-50206.55 +/- 10.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 230000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003361241 |\n",
      "|    clip_fraction        | 0.00391      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.16        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.13e+05     |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.000842    |\n",
      "|    value_loss           | 4.08e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 113       |\n",
      "|    time_elapsed    | 5003      |\n",
      "|    total_timesteps | 231424    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 114          |\n",
      "|    time_elapsed         | 5035         |\n",
      "|    total_timesteps      | 233472       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002859807 |\n",
      "|    clip_fraction        | 0.00654      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.173       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2e+05        |\n",
      "|    n_updates            | 1130         |\n",
      "|    policy_gradient_loss | -0.000762    |\n",
      "|    value_loss           | 4.05e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=-50198.76 +/- 12.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 235000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007355246 |\n",
      "|    clip_fraction        | 0.00884      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.167       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.43e+05     |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    value_loss           | 4.03e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 115       |\n",
      "|    time_elapsed    | 5099      |\n",
      "|    total_timesteps | 235520    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 116          |\n",
      "|    time_elapsed         | 5131         |\n",
      "|    total_timesteps      | 237568       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005718258 |\n",
      "|    clip_fraction        | 0.0082       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.183       |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.9e+05      |\n",
      "|    n_updates            | 1150         |\n",
      "|    policy_gradient_loss | -0.00108     |\n",
      "|    value_loss           | 4e+05        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 117           |\n",
      "|    time_elapsed         | 5164          |\n",
      "|    total_timesteps      | 239616        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024902023 |\n",
      "|    clip_fraction        | 0.00464       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.203        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 2.13e+05      |\n",
      "|    n_updates            | 1160          |\n",
      "|    policy_gradient_loss | -0.000603     |\n",
      "|    value_loss           | 3.99e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-50201.02 +/- 11.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 240000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00051662535 |\n",
      "|    clip_fraction        | 0.00488       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.22         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 2.22e+05      |\n",
      "|    n_updates            | 1170          |\n",
      "|    policy_gradient_loss | -0.000824     |\n",
      "|    value_loss           | 3.97e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 118       |\n",
      "|    time_elapsed    | 5228      |\n",
      "|    total_timesteps | 241664    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 119          |\n",
      "|    time_elapsed         | 5260         |\n",
      "|    total_timesteps      | 243712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010840085 |\n",
      "|    clip_fraction        | 0.0194       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.257       |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.45e+05     |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    value_loss           | 3.95e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=-50199.52 +/- 15.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 245000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011044835 |\n",
      "|    clip_fraction        | 0.0186       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.292       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.83e+05     |\n",
      "|    n_updates            | 1190         |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    value_loss           | 3.93e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 120       |\n",
      "|    time_elapsed    | 5325      |\n",
      "|    total_timesteps | 245760    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 121          |\n",
      "|    time_elapsed         | 5357         |\n",
      "|    total_timesteps      | 247808       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012303111 |\n",
      "|    clip_fraction        | 0.0107       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.315       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.02e+05     |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    value_loss           | 3.92e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 122          |\n",
      "|    time_elapsed         | 5390         |\n",
      "|    total_timesteps      | 249856       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009865414 |\n",
      "|    clip_fraction        | 0.0102       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.289       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.57e+05     |\n",
      "|    n_updates            | 1210         |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    value_loss           | 3.91e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=-50206.52 +/- 9.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 250000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009125171 |\n",
      "|    clip_fraction        | 0.00552      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.277       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.51e+05     |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.000536    |\n",
      "|    value_loss           | 3.9e+05      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 123       |\n",
      "|    time_elapsed    | 5454      |\n",
      "|    total_timesteps | 251904    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 124         |\n",
      "|    time_elapsed         | 5486        |\n",
      "|    total_timesteps      | 253952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000840556 |\n",
      "|    clip_fraction        | 0.00776     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.246      |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 2.38e+05    |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.00141    |\n",
      "|    value_loss           | 3.88e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=-50216.55 +/- 9.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 255000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007558862 |\n",
      "|    clip_fraction        | 0.0107       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.264       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.71e+05     |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.000968    |\n",
      "|    value_loss           | 3.88e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 125       |\n",
      "|    time_elapsed    | 5551      |\n",
      "|    total_timesteps | 256000    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 126           |\n",
      "|    time_elapsed         | 5584          |\n",
      "|    total_timesteps      | 258048        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00085143186 |\n",
      "|    clip_fraction        | 0.00264       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.263        |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.46e+05      |\n",
      "|    n_updates            | 1250          |\n",
      "|    policy_gradient_loss | -0.000287     |\n",
      "|    value_loss           | 3.89e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-50206.28 +/- 10.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 260000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012688817 |\n",
      "|    clip_fraction        | 0.0113       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.217       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.4e+05      |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -0.000938    |\n",
      "|    value_loss           | 3.7e+05      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 127       |\n",
      "|    time_elapsed    | 5647      |\n",
      "|    total_timesteps | 260096    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 128          |\n",
      "|    time_elapsed         | 5680         |\n",
      "|    total_timesteps      | 262144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009998363 |\n",
      "|    clip_fraction        | 0.0083       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.195       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.03e+05     |\n",
      "|    n_updates            | 1270         |\n",
      "|    policy_gradient_loss | -0.000754    |\n",
      "|    value_loss           | 3.66e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 129          |\n",
      "|    time_elapsed         | 5712         |\n",
      "|    total_timesteps      | 264192       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005802927 |\n",
      "|    clip_fraction        | 0.0115       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.197       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.06e+05     |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    value_loss           | 3.63e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=-50191.62 +/- 23.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 265000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017653374 |\n",
      "|    clip_fraction        | 0.00298       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.181        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.31e+05      |\n",
      "|    n_updates            | 1290          |\n",
      "|    policy_gradient_loss | 0.000175      |\n",
      "|    value_loss           | 3.6e+05       |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 130       |\n",
      "|    time_elapsed    | 5776      |\n",
      "|    total_timesteps | 266240    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 131          |\n",
      "|    time_elapsed         | 5809         |\n",
      "|    total_timesteps      | 268288       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006221166 |\n",
      "|    clip_fraction        | 0.00688      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.213       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.8e+05      |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.00042     |\n",
      "|    value_loss           | 3.57e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=-50209.26 +/- 14.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 270000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002321759 |\n",
      "|    clip_fraction        | 0.00083      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.196       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.96e+05     |\n",
      "|    n_updates            | 1310         |\n",
      "|    policy_gradient_loss | -0.000122    |\n",
      "|    value_loss           | 3.54e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 132       |\n",
      "|    time_elapsed    | 5874      |\n",
      "|    total_timesteps | 270336    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 133           |\n",
      "|    time_elapsed         | 5906          |\n",
      "|    total_timesteps      | 272384        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00068435154 |\n",
      "|    clip_fraction        | 0.00342       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.213        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 2.35e+05      |\n",
      "|    n_updates            | 1320          |\n",
      "|    policy_gradient_loss | -0.000372     |\n",
      "|    value_loss           | 3.52e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 134          |\n",
      "|    time_elapsed         | 5939         |\n",
      "|    total_timesteps      | 274432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008101136 |\n",
      "|    clip_fraction        | 0.00474      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.08e+05     |\n",
      "|    n_updates            | 1330         |\n",
      "|    policy_gradient_loss | -0.000668    |\n",
      "|    value_loss           | 3.5e+05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=-50211.81 +/- 11.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 275000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006639828 |\n",
      "|    clip_fraction        | 0.0063       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.16        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2.17e+05     |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.000789    |\n",
      "|    value_loss           | 3.47e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 135       |\n",
      "|    time_elapsed    | 6003      |\n",
      "|    total_timesteps | 276480    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 136           |\n",
      "|    time_elapsed         | 6036          |\n",
      "|    total_timesteps      | 278528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030157843 |\n",
      "|    clip_fraction        | 0.000293      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.15         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.59e+05      |\n",
      "|    n_updates            | 1350          |\n",
      "|    policy_gradient_loss | 0.000104      |\n",
      "|    value_loss           | 3.45e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-50196.70 +/- 15.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 280000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00050835893 |\n",
      "|    clip_fraction        | 0.00371       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.127        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.43e+05      |\n",
      "|    n_updates            | 1360          |\n",
      "|    policy_gradient_loss | -0.00025      |\n",
      "|    value_loss           | 3.43e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 137       |\n",
      "|    time_elapsed    | 6100      |\n",
      "|    total_timesteps | 280576    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 138           |\n",
      "|    time_elapsed         | 6132          |\n",
      "|    total_timesteps      | 282624        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00049891835 |\n",
      "|    clip_fraction        | 0.00366       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.119        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.88e+05      |\n",
      "|    n_updates            | 1370          |\n",
      "|    policy_gradient_loss | -0.00048      |\n",
      "|    value_loss           | 3.42e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 139           |\n",
      "|    time_elapsed         | 6164          |\n",
      "|    total_timesteps      | 284672        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043134115 |\n",
      "|    clip_fraction        | 0.00913       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.125        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.02e+05      |\n",
      "|    n_updates            | 1380          |\n",
      "|    policy_gradient_loss | -0.000608     |\n",
      "|    value_loss           | 3.41e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=-50210.62 +/- 6.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 285000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00059509475 |\n",
      "|    clip_fraction        | 0.00688       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.108        |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.71e+05      |\n",
      "|    n_updates            | 1390          |\n",
      "|    policy_gradient_loss | -0.000764     |\n",
      "|    value_loss           | 3.39e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 140       |\n",
      "|    time_elapsed    | 6229      |\n",
      "|    total_timesteps | 286720    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 141          |\n",
      "|    time_elapsed         | 6261         |\n",
      "|    total_timesteps      | 288768       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003009586 |\n",
      "|    clip_fraction        | 0.00259      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.103       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.47e+05     |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.000722    |\n",
      "|    value_loss           | 3.37e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-50204.76 +/- 12.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 290000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021180895 |\n",
      "|    clip_fraction        | 0.00298       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.115        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.34e+05      |\n",
      "|    n_updates            | 1410          |\n",
      "|    policy_gradient_loss | -0.000397     |\n",
      "|    value_loss           | 3.37e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 142       |\n",
      "|    time_elapsed    | 6325      |\n",
      "|    total_timesteps | 290816    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 143          |\n",
      "|    time_elapsed         | 6358         |\n",
      "|    total_timesteps      | 292864       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002178573 |\n",
      "|    clip_fraction        | 0.00278      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.12        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 2e+05        |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.000143    |\n",
      "|    value_loss           | 3.36e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 144          |\n",
      "|    time_elapsed         | 6390         |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003646493 |\n",
      "|    clip_fraction        | 0.000879     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.113       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.38e+05     |\n",
      "|    n_updates            | 1430         |\n",
      "|    policy_gradient_loss | 0.000151     |\n",
      "|    value_loss           | 3.35e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=-50208.63 +/- 13.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 295000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005024094 |\n",
      "|    clip_fraction        | 0.00757      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.101       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.78e+05     |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.000863    |\n",
      "|    value_loss           | 3.34e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 46        |\n",
      "|    iterations      | 145       |\n",
      "|    time_elapsed    | 6454      |\n",
      "|    total_timesteps | 296960    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 146           |\n",
      "|    time_elapsed         | 6486          |\n",
      "|    total_timesteps      | 299008        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036700937 |\n",
      "|    clip_fraction        | 0.00264       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.102        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.52e+05      |\n",
      "|    n_updates            | 1450          |\n",
      "|    policy_gradient_loss | -0.000607     |\n",
      "|    value_loss           | 3.34e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-50204.67 +/- 14.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 300000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016608139 |\n",
      "|    clip_fraction        | 0.00493       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.126        |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.64e+05      |\n",
      "|    n_updates            | 1460          |\n",
      "|    policy_gradient_loss | -0.000287     |\n",
      "|    value_loss           | 3.3e+05       |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 147       |\n",
      "|    time_elapsed    | 6550      |\n",
      "|    total_timesteps | 301056    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 148          |\n",
      "|    time_elapsed         | 6582         |\n",
      "|    total_timesteps      | 303104       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017060615 |\n",
      "|    clip_fraction        | 0.00625      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.137       |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.23e+05     |\n",
      "|    n_updates            | 1470         |\n",
      "|    policy_gradient_loss | -0.000405    |\n",
      "|    value_loss           | 3.19e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=305000, episode_reward=-50213.73 +/- 9.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 305000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037748413 |\n",
      "|    clip_fraction        | 0.00508       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.104        |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.87e+05      |\n",
      "|    n_updates            | 1480          |\n",
      "|    policy_gradient_loss | -0.000148     |\n",
      "|    value_loss           | 3.16e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 149       |\n",
      "|    time_elapsed    | 6646      |\n",
      "|    total_timesteps | 305152    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 150           |\n",
      "|    time_elapsed         | 6678          |\n",
      "|    total_timesteps      | 307200        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035283796 |\n",
      "|    clip_fraction        | 0.0064        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.126        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.79e+05      |\n",
      "|    n_updates            | 1490          |\n",
      "|    policy_gradient_loss | -0.000373     |\n",
      "|    value_loss           | 3.13e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 151           |\n",
      "|    time_elapsed         | 6710          |\n",
      "|    total_timesteps      | 309248        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036857877 |\n",
      "|    clip_fraction        | 0.0063        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.137        |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.42e+05      |\n",
      "|    n_updates            | 1500          |\n",
      "|    policy_gradient_loss | -0.000504     |\n",
      "|    value_loss           | 3.11e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=-50206.64 +/- 6.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 310000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00060886855 |\n",
      "|    clip_fraction        | 0.00903       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.117        |\n",
      "|    explained_variance   | 3.58e-07      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.28e+05      |\n",
      "|    n_updates            | 1510          |\n",
      "|    policy_gradient_loss | -0.000818     |\n",
      "|    value_loss           | 3.08e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 152       |\n",
      "|    time_elapsed    | 6775      |\n",
      "|    total_timesteps | 311296    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 153           |\n",
      "|    time_elapsed         | 6807          |\n",
      "|    total_timesteps      | 313344        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026365413 |\n",
      "|    clip_fraction        | 0.00264       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.105        |\n",
      "|    explained_variance   | 2.98e-07      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.12e+05      |\n",
      "|    n_updates            | 1520          |\n",
      "|    policy_gradient_loss | -0.000485     |\n",
      "|    value_loss           | 3.06e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=315000, episode_reward=-50206.70 +/- 10.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 315000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.591465e-05 |\n",
      "|    clip_fraction        | 0.00254      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.109       |\n",
      "|    explained_variance   | 4.17e-07     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.43e+05     |\n",
      "|    n_updates            | 1530         |\n",
      "|    policy_gradient_loss | 0.00015      |\n",
      "|    value_loss           | 3.04e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 154       |\n",
      "|    time_elapsed    | 6872      |\n",
      "|    total_timesteps | 315392    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 155           |\n",
      "|    time_elapsed         | 6904          |\n",
      "|    total_timesteps      | 317440        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013519611 |\n",
      "|    clip_fraction        | 0.000732      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0905       |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.91e+05      |\n",
      "|    n_updates            | 1540          |\n",
      "|    policy_gradient_loss | 1.6e-05       |\n",
      "|    value_loss           | 3.02e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 156           |\n",
      "|    time_elapsed         | 6936          |\n",
      "|    total_timesteps      | 319488        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019748209 |\n",
      "|    clip_fraction        | 0.00303       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.107        |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.33e+05      |\n",
      "|    n_updates            | 1550          |\n",
      "|    policy_gradient_loss | -0.00027      |\n",
      "|    value_loss           | 3e+05         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-50200.89 +/- 20.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 320000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00028593 |\n",
      "|    clip_fraction        | 0.00308    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.108     |\n",
      "|    explained_variance   | 2.98e-07   |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.53e+05   |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.000533  |\n",
      "|    value_loss           | 2.98e+05   |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 157       |\n",
      "|    time_elapsed    | 7000      |\n",
      "|    total_timesteps | 321536    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 158           |\n",
      "|    time_elapsed         | 7033          |\n",
      "|    total_timesteps      | 323584        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015318085 |\n",
      "|    clip_fraction        | 0.0041        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.11         |\n",
      "|    explained_variance   | 2.98e-07      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.73e+05      |\n",
      "|    n_updates            | 1570          |\n",
      "|    policy_gradient_loss | 0.000346      |\n",
      "|    value_loss           | 2.97e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=325000, episode_reward=-50204.71 +/- 16.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 325000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047657086 |\n",
      "|    clip_fraction        | 0.00503       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.11         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.76e+05      |\n",
      "|    n_updates            | 1580          |\n",
      "|    policy_gradient_loss | -0.000561     |\n",
      "|    value_loss           | 2.96e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 159       |\n",
      "|    time_elapsed    | 7097      |\n",
      "|    total_timesteps | 325632    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 160          |\n",
      "|    time_elapsed         | 7129         |\n",
      "|    total_timesteps      | 327680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.528856e-05 |\n",
      "|    clip_fraction        | 0.000879     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.115       |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.24e+05     |\n",
      "|    n_updates            | 1590         |\n",
      "|    policy_gradient_loss | -0.000181    |\n",
      "|    value_loss           | 2.94e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 46            |\n",
      "|    iterations           | 161           |\n",
      "|    time_elapsed         | 7161          |\n",
      "|    total_timesteps      | 329728        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023253073 |\n",
      "|    clip_fraction        | 0.00513       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.1          |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 7.64e+04      |\n",
      "|    n_updates            | 1600          |\n",
      "|    policy_gradient_loss | -0.000749     |\n",
      "|    value_loss           | 2.94e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=-50202.95 +/- 11.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 330000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024169081 |\n",
      "|    clip_fraction        | 0.00322       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.107        |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.57e+05      |\n",
      "|    n_updates            | 1610          |\n",
      "|    policy_gradient_loss | -0.000255     |\n",
      "|    value_loss           | 2.93e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 162       |\n",
      "|    time_elapsed    | 7226      |\n",
      "|    total_timesteps | 331776    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 163          |\n",
      "|    time_elapsed         | 7258         |\n",
      "|    total_timesteps      | 333824       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002872242 |\n",
      "|    clip_fraction        | 0.00278      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.103       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.61e+05     |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.000434    |\n",
      "|    value_loss           | 2.92e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=335000, episode_reward=-50200.97 +/- 15.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 335000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030283857 |\n",
      "|    clip_fraction        | 0.00371       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0956       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.51e+05      |\n",
      "|    n_updates            | 1630          |\n",
      "|    policy_gradient_loss | -0.000342     |\n",
      "|    value_loss           | 2.91e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 164       |\n",
      "|    time_elapsed    | 7322      |\n",
      "|    total_timesteps | 335872    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 165           |\n",
      "|    time_elapsed         | 7355          |\n",
      "|    total_timesteps      | 337920        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037963488 |\n",
      "|    clip_fraction        | 0.0107        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.109        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.3e+05       |\n",
      "|    n_updates            | 1640          |\n",
      "|    policy_gradient_loss | -0.00117      |\n",
      "|    value_loss           | 2.91e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 166          |\n",
      "|    time_elapsed         | 7388         |\n",
      "|    total_timesteps      | 339968       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003823656 |\n",
      "|    clip_fraction        | 0.00801      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0935      |\n",
      "|    explained_variance   | 2.98e-07     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.58e+05     |\n",
      "|    n_updates            | 1650         |\n",
      "|    policy_gradient_loss | -0.000636    |\n",
      "|    value_loss           | 2.91e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-50207.13 +/- 14.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 340000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00059946417 |\n",
      "|    clip_fraction        | 0.00664       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.112        |\n",
      "|    explained_variance   | 2.38e-07      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.53e+05      |\n",
      "|    n_updates            | 1660          |\n",
      "|    policy_gradient_loss | 0.00129       |\n",
      "|    value_loss           | 2.91e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 167       |\n",
      "|    time_elapsed    | 7453      |\n",
      "|    total_timesteps | 342016    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 45         |\n",
      "|    iterations           | 168        |\n",
      "|    time_elapsed         | 7485       |\n",
      "|    total_timesteps      | 344064     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00032003 |\n",
      "|    clip_fraction        | 0.0063     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0718    |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.37e+05   |\n",
      "|    n_updates            | 1670       |\n",
      "|    policy_gradient_loss | -0.000714  |\n",
      "|    value_loss           | 2.85e+05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=345000, episode_reward=-50210.51 +/- 12.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 345000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.5589494e-05 |\n",
      "|    clip_fraction        | 0.0019        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.065        |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.2e+05       |\n",
      "|    n_updates            | 1680          |\n",
      "|    policy_gradient_loss | -0.000268     |\n",
      "|    value_loss           | 2.79e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 169       |\n",
      "|    time_elapsed    | 7549      |\n",
      "|    total_timesteps | 346112    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 170           |\n",
      "|    time_elapsed         | 7581          |\n",
      "|    total_timesteps      | 348160        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014564066 |\n",
      "|    clip_fraction        | 0.00273       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0604       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.38e+05      |\n",
      "|    n_updates            | 1690          |\n",
      "|    policy_gradient_loss | -7.55e-05     |\n",
      "|    value_loss           | 2.76e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=-50214.56 +/- 7.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 350000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018938133 |\n",
      "|    clip_fraction        | 0.00283       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0696       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.47e+05      |\n",
      "|    n_updates            | 1700          |\n",
      "|    policy_gradient_loss | -0.000491     |\n",
      "|    value_loss           | 2.74e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 171       |\n",
      "|    time_elapsed    | 7646      |\n",
      "|    total_timesteps | 350208    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 172           |\n",
      "|    time_elapsed         | 7680          |\n",
      "|    total_timesteps      | 352256        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021176055 |\n",
      "|    clip_fraction        | 0.00273       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0733       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.29e+05      |\n",
      "|    n_updates            | 1710          |\n",
      "|    policy_gradient_loss | -0.000434     |\n",
      "|    value_loss           | 2.72e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 173           |\n",
      "|    time_elapsed         | 7713          |\n",
      "|    total_timesteps      | 354304        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.3896656e-05 |\n",
      "|    clip_fraction        | 0.00122       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0757       |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.3e+05       |\n",
      "|    n_updates            | 1720          |\n",
      "|    policy_gradient_loss | 0.000151      |\n",
      "|    value_loss           | 2.7e+05       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=355000, episode_reward=-50204.65 +/- 9.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 355000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002361591 |\n",
      "|    clip_fraction        | 0.00522      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0669      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.19e+05     |\n",
      "|    n_updates            | 1730         |\n",
      "|    policy_gradient_loss | -0.000965    |\n",
      "|    value_loss           | 2.67e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 174       |\n",
      "|    time_elapsed    | 7777      |\n",
      "|    total_timesteps | 356352    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 175          |\n",
      "|    time_elapsed         | 7809         |\n",
      "|    total_timesteps      | 358400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011643767 |\n",
      "|    clip_fraction        | 0.00308      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0613      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.81e+05     |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | -8.52e-05    |\n",
      "|    value_loss           | 2.66e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-50203.09 +/- 9.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 360000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019411664 |\n",
      "|    clip_fraction        | 0.00381       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0445       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.35e+05      |\n",
      "|    n_updates            | 1750          |\n",
      "|    policy_gradient_loss | -0.000353     |\n",
      "|    value_loss           | 2.64e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 176       |\n",
      "|    time_elapsed    | 7873      |\n",
      "|    total_timesteps | 360448    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 177           |\n",
      "|    time_elapsed         | 7906          |\n",
      "|    total_timesteps      | 362496        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00072153646 |\n",
      "|    clip_fraction        | 0.00542       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0429       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.11e+05      |\n",
      "|    n_updates            | 1760          |\n",
      "|    policy_gradient_loss | -0.000597     |\n",
      "|    value_loss           | 2.63e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 178          |\n",
      "|    time_elapsed         | 7938         |\n",
      "|    total_timesteps      | 364544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004056506 |\n",
      "|    clip_fraction        | 0.00234      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0458      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.57e+05     |\n",
      "|    n_updates            | 1770         |\n",
      "|    policy_gradient_loss | -0.000324    |\n",
      "|    value_loss           | 2.62e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=365000, episode_reward=-50202.99 +/- 10.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 365000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029244172 |\n",
      "|    clip_fraction        | 0.00781       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0467       |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.12e+05      |\n",
      "|    n_updates            | 1780          |\n",
      "|    policy_gradient_loss | -0.00133      |\n",
      "|    value_loss           | 2.61e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 179       |\n",
      "|    time_elapsed    | 8001      |\n",
      "|    total_timesteps | 366592    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 180           |\n",
      "|    time_elapsed         | 8034          |\n",
      "|    total_timesteps      | 368640        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021177126 |\n",
      "|    clip_fraction        | 0.00313       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0419       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.34e+05      |\n",
      "|    n_updates            | 1790          |\n",
      "|    policy_gradient_loss | 0.000224      |\n",
      "|    value_loss           | 2.6e+05       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=-50217.75 +/- 19.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 1e+03          |\n",
      "|    mean_reward          | -5.02e+04      |\n",
      "| time/                   |                |\n",
      "|    total_timesteps      | 370000         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000115189556 |\n",
      "|    clip_fraction        | 0.00215        |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.0435        |\n",
      "|    explained_variance   | -1.19e-07      |\n",
      "|    learning_rate        | 0.0005         |\n",
      "|    loss                 | 1.58e+05       |\n",
      "|    n_updates            | 1800           |\n",
      "|    policy_gradient_loss | -0.000236      |\n",
      "|    value_loss           | 2.59e+05       |\n",
      "--------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 181       |\n",
      "|    time_elapsed    | 8098      |\n",
      "|    total_timesteps | 370688    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 182           |\n",
      "|    time_elapsed         | 8130          |\n",
      "|    total_timesteps      | 372736        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00085356436 |\n",
      "|    clip_fraction        | 0.00469       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0546       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.42e+05      |\n",
      "|    n_updates            | 1810          |\n",
      "|    policy_gradient_loss | 0.0011        |\n",
      "|    value_loss           | 2.58e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 183          |\n",
      "|    time_elapsed         | 8163         |\n",
      "|    total_timesteps      | 374784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003649731 |\n",
      "|    clip_fraction        | 0.00151      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0578      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.23e+05     |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.000301    |\n",
      "|    value_loss           | 2.58e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=375000, episode_reward=-50212.28 +/- 15.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 375000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019074422 |\n",
      "|    clip_fraction        | 0.00298      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0458      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.3e+05      |\n",
      "|    n_updates            | 1830         |\n",
      "|    policy_gradient_loss | -0.000472    |\n",
      "|    value_loss           | 2.57e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 184       |\n",
      "|    time_elapsed    | 8227      |\n",
      "|    total_timesteps | 376832    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 185           |\n",
      "|    time_elapsed         | 8259          |\n",
      "|    total_timesteps      | 378880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.2571744e-05 |\n",
      "|    clip_fraction        | 0.000879      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0261       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.53e+05      |\n",
      "|    n_updates            | 1840          |\n",
      "|    policy_gradient_loss | -9.02e-05     |\n",
      "|    value_loss           | 2.57e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-50197.57 +/- 10.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 380000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.466434e-05 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0287      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.2e+05      |\n",
      "|    n_updates            | 1850         |\n",
      "|    policy_gradient_loss | -0.000121    |\n",
      "|    value_loss           | 2.58e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 186       |\n",
      "|    time_elapsed    | 8323      |\n",
      "|    total_timesteps | 380928    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 187          |\n",
      "|    time_elapsed         | 8355         |\n",
      "|    total_timesteps      | 382976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.942581e-05 |\n",
      "|    clip_fraction        | 0.00186      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0224      |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.31e+05     |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | 4.43e-05     |\n",
      "|    value_loss           | 2.58e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=385000, episode_reward=-50216.31 +/- 12.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 385000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.2623607e-05 |\n",
      "|    clip_fraction        | 0.00122       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0181       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.38e+05      |\n",
      "|    n_updates            | 1870          |\n",
      "|    policy_gradient_loss | 7.56e-05      |\n",
      "|    value_loss           | 2.58e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 188       |\n",
      "|    time_elapsed    | 8419      |\n",
      "|    total_timesteps | 385024    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 189           |\n",
      "|    time_elapsed         | 8451          |\n",
      "|    total_timesteps      | 387072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3471959e-05 |\n",
      "|    clip_fraction        | 0.000195      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0176       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.07e+05      |\n",
      "|    n_updates            | 1880          |\n",
      "|    policy_gradient_loss | -8.15e-05     |\n",
      "|    value_loss           | 2.51e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 190           |\n",
      "|    time_elapsed         | 8483          |\n",
      "|    total_timesteps      | 389120        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.0377916e-05 |\n",
      "|    clip_fraction        | 0.000977      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0198       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.19e+05      |\n",
      "|    n_updates            | 1890          |\n",
      "|    policy_gradient_loss | -0.000299     |\n",
      "|    value_loss           | 2.5e+05       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=-50200.91 +/- 15.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 390000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.929112e-05 |\n",
      "|    clip_fraction        | 0.00117      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0192      |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.39e+05     |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -0.000334    |\n",
      "|    value_loss           | 2.47e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 191       |\n",
      "|    time_elapsed    | 8547      |\n",
      "|    total_timesteps | 391168    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 192           |\n",
      "|    time_elapsed         | 8579          |\n",
      "|    total_timesteps      | 393216        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9358675e-05 |\n",
      "|    clip_fraction        | 0.000293      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0171       |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.64e+05      |\n",
      "|    n_updates            | 1910          |\n",
      "|    policy_gradient_loss | -0.000128     |\n",
      "|    value_loss           | 2.45e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=395000, episode_reward=-50198.93 +/- 13.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 395000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1624677e-05 |\n",
      "|    clip_fraction        | 0.00122       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0144       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.44e+05      |\n",
      "|    n_updates            | 1920          |\n",
      "|    policy_gradient_loss | -0.000267     |\n",
      "|    value_loss           | 2.43e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 193       |\n",
      "|    time_elapsed    | 8642      |\n",
      "|    total_timesteps | 395264    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 194          |\n",
      "|    time_elapsed         | 8674         |\n",
      "|    total_timesteps      | 397312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.799805e-06 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0157      |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.29e+05     |\n",
      "|    n_updates            | 1930         |\n",
      "|    policy_gradient_loss | -3.76e-05    |\n",
      "|    value_loss           | 2.42e+05     |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 1e+03          |\n",
      "|    ep_rew_mean          | -5.02e+04      |\n",
      "| time/                   |                |\n",
      "|    fps                  | 45             |\n",
      "|    iterations           | 195            |\n",
      "|    time_elapsed         | 8706           |\n",
      "|    total_timesteps      | 399360         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000116552925 |\n",
      "|    clip_fraction        | 0.000195       |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.0169        |\n",
      "|    explained_variance   | 5.96e-08       |\n",
      "|    learning_rate        | 0.0005         |\n",
      "|    loss                 | 1.03e+05       |\n",
      "|    n_updates            | 1940           |\n",
      "|    policy_gradient_loss | -0.000114      |\n",
      "|    value_loss           | 2.4e+05        |\n",
      "--------------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-50206.72 +/- 18.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 400000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003180563 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0128      |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.23e+05     |\n",
      "|    n_updates            | 1950         |\n",
      "|    policy_gradient_loss | -0.000136    |\n",
      "|    value_loss           | 2.39e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 196       |\n",
      "|    time_elapsed    | 8770      |\n",
      "|    total_timesteps | 401408    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 197           |\n",
      "|    time_elapsed         | 8803          |\n",
      "|    total_timesteps      | 403456        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2236211e-05 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0143       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.46e+05      |\n",
      "|    n_updates            | 1960          |\n",
      "|    policy_gradient_loss | -3.04e-05     |\n",
      "|    value_loss           | 2.38e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=405000, episode_reward=-50219.69 +/- 10.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 1e+03          |\n",
      "|    mean_reward          | -5.02e+04      |\n",
      "| time/                   |                |\n",
      "|    total_timesteps      | 405000         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000102267106 |\n",
      "|    clip_fraction        | 0.0022         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.0159        |\n",
      "|    explained_variance   | 0              |\n",
      "|    learning_rate        | 0.0005         |\n",
      "|    loss                 | 1.23e+05       |\n",
      "|    n_updates            | 1970           |\n",
      "|    policy_gradient_loss | -7.43e-05      |\n",
      "|    value_loss           | 2.37e+05       |\n",
      "--------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 198       |\n",
      "|    time_elapsed    | 8867      |\n",
      "|    total_timesteps | 405504    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 199           |\n",
      "|    time_elapsed         | 8899          |\n",
      "|    total_timesteps      | 407552        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013770451 |\n",
      "|    clip_fraction        | 0.00151       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0119       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1e+05         |\n",
      "|    n_updates            | 1980          |\n",
      "|    policy_gradient_loss | -0.000203     |\n",
      "|    value_loss           | 2.36e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 200           |\n",
      "|    time_elapsed         | 8931          |\n",
      "|    total_timesteps      | 409600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030062682 |\n",
      "|    clip_fraction        | 0.000732      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0197       |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.13e+05      |\n",
      "|    n_updates            | 1990          |\n",
      "|    policy_gradient_loss | 8.09e-05      |\n",
      "|    value_loss           | 2.35e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=-50212.37 +/- 7.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 410000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3672445e-05 |\n",
      "|    clip_fraction        | 0.00225       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0236       |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.08e+05      |\n",
      "|    n_updates            | 2000          |\n",
      "|    policy_gradient_loss | -0.000284     |\n",
      "|    value_loss           | 2.35e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 201       |\n",
      "|    time_elapsed    | 8995      |\n",
      "|    total_timesteps | 411648    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 202          |\n",
      "|    time_elapsed         | 9028         |\n",
      "|    total_timesteps      | 413696       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.455938e-05 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0219      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.23e+05     |\n",
      "|    n_updates            | 2010         |\n",
      "|    policy_gradient_loss | -1.8e-05     |\n",
      "|    value_loss           | 2.34e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=415000, episode_reward=-50208.52 +/- 17.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 415000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012356933 |\n",
      "|    clip_fraction        | 0.00161      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0337      |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.29e+05     |\n",
      "|    n_updates            | 2020         |\n",
      "|    policy_gradient_loss | 0.000679     |\n",
      "|    value_loss           | 2.34e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 203       |\n",
      "|    time_elapsed    | 9092      |\n",
      "|    total_timesteps | 415744    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 204          |\n",
      "|    time_elapsed         | 9124         |\n",
      "|    total_timesteps      | 417792       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002312586 |\n",
      "|    clip_fraction        | 0.00166      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0388      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 9.02e+04     |\n",
      "|    n_updates            | 2030         |\n",
      "|    policy_gradient_loss | -0.000401    |\n",
      "|    value_loss           | 2.34e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 205           |\n",
      "|    time_elapsed         | 9157          |\n",
      "|    total_timesteps      | 419840        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022118137 |\n",
      "|    clip_fraction        | 0.00195       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0447       |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.05e+05      |\n",
      "|    n_updates            | 2040          |\n",
      "|    policy_gradient_loss | 0.00049       |\n",
      "|    value_loss           | 2.34e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-50210.55 +/- 14.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 420000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008738335 |\n",
      "|    clip_fraction        | 0.00303      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.034       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.03e+05     |\n",
      "|    n_updates            | 2050         |\n",
      "|    policy_gradient_loss | 0.000298     |\n",
      "|    value_loss           | 2.35e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 206       |\n",
      "|    time_elapsed    | 9221      |\n",
      "|    total_timesteps | 421888    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 207          |\n",
      "|    time_elapsed         | 9253         |\n",
      "|    total_timesteps      | 423936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.698155e-05 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0281      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.12e+05     |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -8.81e-05    |\n",
      "|    value_loss           | 2.35e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=425000, episode_reward=-50199.32 +/- 8.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 425000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016613345 |\n",
      "|    clip_fraction        | 0.00322       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0204       |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.56e+05      |\n",
      "|    n_updates            | 2070          |\n",
      "|    policy_gradient_loss | -0.000112     |\n",
      "|    value_loss           | 2.36e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 208       |\n",
      "|    time_elapsed    | 9316      |\n",
      "|    total_timesteps | 425984    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 209          |\n",
      "|    time_elapsed         | 9349         |\n",
      "|    total_timesteps      | 428032       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022243573 |\n",
      "|    clip_fraction        | 0.00137      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.102       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 9.71e+04     |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | 0.000713     |\n",
      "|    value_loss           | 2.37e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=-50185.85 +/- 9.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 430000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00059081754 |\n",
      "|    clip_fraction        | 0.00703       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0754       |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.41e+05      |\n",
      "|    n_updates            | 2090          |\n",
      "|    policy_gradient_loss | 6.04e-05      |\n",
      "|    value_loss           | 2.31e+05      |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 210       |\n",
      "|    time_elapsed    | 9414      |\n",
      "|    total_timesteps | 430080    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 211          |\n",
      "|    time_elapsed         | 9446         |\n",
      "|    total_timesteps      | 432128       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.659958e-05 |\n",
      "|    clip_fraction        | 0.002        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0707      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.09e+05     |\n",
      "|    n_updates            | 2100         |\n",
      "|    policy_gradient_loss | 0.000246     |\n",
      "|    value_loss           | 2.31e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 212         |\n",
      "|    time_elapsed         | 9478        |\n",
      "|    total_timesteps      | 434176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005281133 |\n",
      "|    clip_fraction        | 0.00605     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0703     |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.17e+05    |\n",
      "|    n_updates            | 2110        |\n",
      "|    policy_gradient_loss | 0.00136     |\n",
      "|    value_loss           | 2.29e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=435000, episode_reward=-50204.98 +/- 4.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 435000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 5.27871e-05 |\n",
      "|    clip_fraction        | 0.00107     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0238     |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 9.87e+04    |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.000172   |\n",
      "|    value_loss           | 2.28e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 213       |\n",
      "|    time_elapsed    | 9543      |\n",
      "|    total_timesteps | 436224    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 214          |\n",
      "|    time_elapsed         | 9575         |\n",
      "|    total_timesteps      | 438272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019832582 |\n",
      "|    clip_fraction        | 0.00142      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0325      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 9.96e+04     |\n",
      "|    n_updates            | 2130         |\n",
      "|    policy_gradient_loss | -5.87e-05    |\n",
      "|    value_loss           | 2.27e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-50204.56 +/- 17.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 440000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006906297 |\n",
      "|    clip_fraction        | 0.00244      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0604      |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.06e+05     |\n",
      "|    n_updates            | 2140         |\n",
      "|    policy_gradient_loss | 0.000376     |\n",
      "|    value_loss           | 2.25e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 215       |\n",
      "|    time_elapsed    | 9639      |\n",
      "|    total_timesteps | 440320    |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -5.02e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 45         |\n",
      "|    iterations           | 216        |\n",
      "|    time_elapsed         | 9672       |\n",
      "|    total_timesteps      | 442368     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00978399 |\n",
      "|    clip_fraction        | 0.0061     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0609    |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.29e+05   |\n",
      "|    n_updates            | 2150       |\n",
      "|    policy_gradient_loss | -0.00022   |\n",
      "|    value_loss           | 2.24e+05   |\n",
      "----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 217           |\n",
      "|    time_elapsed         | 9704          |\n",
      "|    total_timesteps      | 444416        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1535594e-05 |\n",
      "|    clip_fraction        | 0.000293      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0145       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.17e+05      |\n",
      "|    n_updates            | 2160          |\n",
      "|    policy_gradient_loss | -6.17e-05     |\n",
      "|    value_loss           | 2.23e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=445000, episode_reward=-50206.59 +/- 10.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 445000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028802906 |\n",
      "|    clip_fraction        | 0.00151       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0102       |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.24e+05      |\n",
      "|    n_updates            | 2170          |\n",
      "|    policy_gradient_loss | -9.49e-05     |\n",
      "|    value_loss           | 2.22e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 218       |\n",
      "|    time_elapsed    | 9772      |\n",
      "|    total_timesteps | 446464    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 219          |\n",
      "|    time_elapsed         | 9805         |\n",
      "|    total_timesteps      | 448512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015733746 |\n",
      "|    clip_fraction        | 0.00171      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0029      |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.17e+05     |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | 0.000769     |\n",
      "|    value_loss           | 2.22e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=-50210.62 +/- 7.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 450000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00206  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.76e+04  |\n",
      "|    n_updates            | 2190      |\n",
      "|    policy_gradient_loss | -5.59e-09 |\n",
      "|    value_loss           | 2.22e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 220       |\n",
      "|    time_elapsed    | 9870      |\n",
      "|    total_timesteps | 450560    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 221           |\n",
      "|    time_elapsed         | 9902          |\n",
      "|    total_timesteps      | 452608        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2161327e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00212      |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.2e+05       |\n",
      "|    n_updates            | 2200          |\n",
      "|    policy_gradient_loss | -1.28e-05     |\n",
      "|    value_loss           | 2.21e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 222           |\n",
      "|    time_elapsed         | 9934          |\n",
      "|    total_timesteps      | 454656        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3755634e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00212      |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.08e+05      |\n",
      "|    n_updates            | 2210          |\n",
      "|    policy_gradient_loss | -5.82e-06     |\n",
      "|    value_loss           | 2.21e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=455000, episode_reward=-50206.76 +/- 6.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 455000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00194  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.11e+05  |\n",
      "|    n_updates            | 2220      |\n",
      "|    policy_gradient_loss | 2.47e-09  |\n",
      "|    value_loss           | 2.21e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 223       |\n",
      "|    time_elapsed    | 9999      |\n",
      "|    total_timesteps | 456704    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 224       |\n",
      "|    time_elapsed         | 10031     |\n",
      "|    total_timesteps      | 458752    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00194  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.49e+04  |\n",
      "|    n_updates            | 2230      |\n",
      "|    policy_gradient_loss | -3.83e-10 |\n",
      "|    value_loss           | 2.22e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-50208.40 +/- 18.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 460000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00194  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.25e+05  |\n",
      "|    n_updates            | 2240      |\n",
      "|    policy_gradient_loss | -2.26e-11 |\n",
      "|    value_loss           | 2.22e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 225       |\n",
      "|    time_elapsed    | 10096     |\n",
      "|    total_timesteps | 460800    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 226       |\n",
      "|    time_elapsed         | 10128     |\n",
      "|    total_timesteps      | 462848    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00194  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.99e+04  |\n",
      "|    n_updates            | 2250      |\n",
      "|    policy_gradient_loss | 5.9e-10   |\n",
      "|    value_loss           | 2.22e+05  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 227           |\n",
      "|    time_elapsed         | 10161         |\n",
      "|    total_timesteps      | 464896        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4106994e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00186      |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.32e+05      |\n",
      "|    n_updates            | 2260          |\n",
      "|    policy_gradient_loss | -3.7e-06      |\n",
      "|    value_loss           | 2.23e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=465000, episode_reward=-50202.48 +/- 9.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 465000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2649485e-05 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00162      |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.28e+05      |\n",
      "|    n_updates            | 2270          |\n",
      "|    policy_gradient_loss | -0.000116     |\n",
      "|    value_loss           | 2.24e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 228       |\n",
      "|    time_elapsed    | 10225     |\n",
      "|    total_timesteps | 466944    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 229       |\n",
      "|    time_elapsed         | 10258     |\n",
      "|    total_timesteps      | 468992    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0016   |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.22e+04  |\n",
      "|    n_updates            | 2280      |\n",
      "|    policy_gradient_loss | -7.22e-10 |\n",
      "|    value_loss           | 2.25e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=-50210.01 +/- 10.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 470000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0016   |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.91e+04  |\n",
      "|    n_updates            | 2290      |\n",
      "|    policy_gradient_loss | 7.04e-10  |\n",
      "|    value_loss           | 2.27e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 230       |\n",
      "|    time_elapsed    | 10322     |\n",
      "|    total_timesteps | 471040    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 231       |\n",
      "|    time_elapsed         | 10355     |\n",
      "|    total_timesteps      | 473088    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0016   |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.92e+04  |\n",
      "|    n_updates            | 2300      |\n",
      "|    policy_gradient_loss | 8.91e-10  |\n",
      "|    value_loss           | 2.23e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=475000, episode_reward=-50210.20 +/- 3.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 475000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0016   |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.74e+04  |\n",
      "|    n_updates            | 2310      |\n",
      "|    policy_gradient_loss | 1.4e-09   |\n",
      "|    value_loss           | 2.23e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 232       |\n",
      "|    time_elapsed    | 10420     |\n",
      "|    total_timesteps | 475136    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 233       |\n",
      "|    time_elapsed         | 10453     |\n",
      "|    total_timesteps      | 477184    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0016   |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.61e+04  |\n",
      "|    n_updates            | 2320      |\n",
      "|    policy_gradient_loss | 3.34e-10  |\n",
      "|    value_loss           | 2.21e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 234       |\n",
      "|    time_elapsed         | 10485     |\n",
      "|    total_timesteps      | 479232    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0016   |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.24e+05  |\n",
      "|    n_updates            | 2330      |\n",
      "|    policy_gradient_loss | 1.18e-09  |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-50210.21 +/- 11.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 480000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0016   |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.04e+05  |\n",
      "|    n_updates            | 2340      |\n",
      "|    policy_gradient_loss | -7.61e-10 |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 235       |\n",
      "|    time_elapsed    | 10550     |\n",
      "|    total_timesteps | 481280    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 236       |\n",
      "|    time_elapsed         | 10582     |\n",
      "|    total_timesteps      | 483328    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0016   |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.38e+05  |\n",
      "|    n_updates            | 2350      |\n",
      "|    policy_gradient_loss | -4.13e-10 |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=485000, episode_reward=-50215.64 +/- 13.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 485000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0016   |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.87e+04  |\n",
      "|    n_updates            | 2360      |\n",
      "|    policy_gradient_loss | -1.06e-09 |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 237       |\n",
      "|    time_elapsed    | 10647     |\n",
      "|    total_timesteps | 485376    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 238       |\n",
      "|    time_elapsed         | 10680     |\n",
      "|    total_timesteps      | 487424    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0016   |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.05e+05  |\n",
      "|    n_updates            | 2370      |\n",
      "|    policy_gradient_loss | -1.72e-09 |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 239       |\n",
      "|    time_elapsed         | 10713     |\n",
      "|    total_timesteps      | 489472    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0016   |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.03e+04  |\n",
      "|    n_updates            | 2380      |\n",
      "|    policy_gradient_loss | 7.49e-10  |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=-50212.13 +/- 12.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 490000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0254578e-05 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00194      |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.04e+05      |\n",
      "|    n_updates            | 2390          |\n",
      "|    policy_gradient_loss | -0.000115     |\n",
      "|    value_loss           | 2.16e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 240       |\n",
      "|    time_elapsed    | 10778     |\n",
      "|    total_timesteps | 491520    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 241       |\n",
      "|    time_elapsed         | 10811     |\n",
      "|    total_timesteps      | 493568    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00198  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.44e+05  |\n",
      "|    n_updates            | 2400      |\n",
      "|    policy_gradient_loss | 2.55e-10  |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=495000, episode_reward=-50211.78 +/- 6.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 495000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00198  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.14e+05  |\n",
      "|    n_updates            | 2410      |\n",
      "|    policy_gradient_loss | -1.69e-10 |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 242       |\n",
      "|    time_elapsed    | 10875     |\n",
      "|    total_timesteps | 495616    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 243       |\n",
      "|    time_elapsed         | 10908     |\n",
      "|    total_timesteps      | 497664    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00198  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.08e+05  |\n",
      "|    n_updates            | 2420      |\n",
      "|    policy_gradient_loss | 8.95e-10  |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 244           |\n",
      "|    time_elapsed         | 10941         |\n",
      "|    total_timesteps      | 499712        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8164807e-05 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00166      |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 7.51e+04      |\n",
      "|    n_updates            | 2430          |\n",
      "|    policy_gradient_loss | -3.69e-05     |\n",
      "|    value_loss           | 2.16e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-50205.19 +/- 16.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 500000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00158  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 7.65e+04  |\n",
      "|    n_updates            | 2440      |\n",
      "|    policy_gradient_loss | -4.73e-10 |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 245       |\n",
      "|    time_elapsed    | 11006     |\n",
      "|    total_timesteps | 501760    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 246       |\n",
      "|    time_elapsed         | 11039     |\n",
      "|    total_timesteps      | 503808    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00158  |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.15e+05  |\n",
      "|    n_updates            | 2450      |\n",
      "|    policy_gradient_loss | 6.21e-10  |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=505000, episode_reward=-50208.17 +/- 13.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 505000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00158  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.04e+05  |\n",
      "|    n_updates            | 2460      |\n",
      "|    policy_gradient_loss | -7.5e-10  |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 247       |\n",
      "|    time_elapsed    | 11103     |\n",
      "|    total_timesteps | 505856    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 248       |\n",
      "|    time_elapsed         | 11136     |\n",
      "|    total_timesteps      | 507904    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00158  |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.04e+05  |\n",
      "|    n_updates            | 2470      |\n",
      "|    policy_gradient_loss | 4.45e-10  |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 249       |\n",
      "|    time_elapsed         | 11169     |\n",
      "|    total_timesteps      | 509952    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00158  |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.24e+05  |\n",
      "|    n_updates            | 2480      |\n",
      "|    policy_gradient_loss | 6.18e-12  |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=-50204.94 +/- 10.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 510000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00158  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.07e+05  |\n",
      "|    n_updates            | 2490      |\n",
      "|    policy_gradient_loss | 1.22e-09  |\n",
      "|    value_loss           | 2.21e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 250       |\n",
      "|    time_elapsed    | 11235     |\n",
      "|    total_timesteps | 512000    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 251       |\n",
      "|    time_elapsed         | 11269     |\n",
      "|    total_timesteps      | 514048    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00158  |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.21e+05  |\n",
      "|    n_updates            | 2500      |\n",
      "|    policy_gradient_loss | 1.72e-09  |\n",
      "|    value_loss           | 2.23e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=515000, episode_reward=-50201.12 +/- 10.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 515000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00158  |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.1e+05   |\n",
      "|    n_updates            | 2510      |\n",
      "|    policy_gradient_loss | -5.33e-10 |\n",
      "|    value_loss           | 2.22e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 252       |\n",
      "|    time_elapsed    | 11335     |\n",
      "|    total_timesteps | 516096    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 253       |\n",
      "|    time_elapsed         | 11368     |\n",
      "|    total_timesteps      | 518144    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00158  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.15e+05  |\n",
      "|    n_updates            | 2520      |\n",
      "|    policy_gradient_loss | 5.01e-10  |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-50223.44 +/- 9.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 520000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00158  |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.36e+05  |\n",
      "|    n_updates            | 2530      |\n",
      "|    policy_gradient_loss | -3.62e-10 |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 254       |\n",
      "|    time_elapsed    | 11433     |\n",
      "|    total_timesteps | 520192    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 255       |\n",
      "|    time_elapsed         | 11466     |\n",
      "|    total_timesteps      | 522240    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00158  |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.99e+04  |\n",
      "|    n_updates            | 2540      |\n",
      "|    policy_gradient_loss | -1.4e-09  |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 256       |\n",
      "|    time_elapsed         | 11500     |\n",
      "|    total_timesteps      | 524288    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00158  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.33e+05  |\n",
      "|    n_updates            | 2550      |\n",
      "|    policy_gradient_loss | -1.49e-09 |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=525000, episode_reward=-50212.22 +/- 12.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 525000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00158  |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.06e+05  |\n",
      "|    n_updates            | 2560      |\n",
      "|    policy_gradient_loss | 2.97e-10  |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 257       |\n",
      "|    time_elapsed    | 11564     |\n",
      "|    total_timesteps | 526336    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 258          |\n",
      "|    time_elapsed         | 11597        |\n",
      "|    total_timesteps      | 528384       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002756454 |\n",
      "|    clip_fraction        | 0.000391     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.00304     |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 9.34e+04     |\n",
      "|    n_updates            | 2570         |\n",
      "|    policy_gradient_loss | -6.25e-05    |\n",
      "|    value_loss           | 2.15e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=-50221.51 +/- 15.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 530000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00324  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.04e+05  |\n",
      "|    n_updates            | 2580      |\n",
      "|    policy_gradient_loss | 2.26e-09  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 259       |\n",
      "|    time_elapsed    | 11662     |\n",
      "|    total_timesteps | 530432    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 260       |\n",
      "|    time_elapsed         | 11696     |\n",
      "|    total_timesteps      | 532480    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00324  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.16e+05  |\n",
      "|    n_updates            | 2590      |\n",
      "|    policy_gradient_loss | -7.83e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 261           |\n",
      "|    time_elapsed         | 11729         |\n",
      "|    total_timesteps      | 534528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0350395e-05 |\n",
      "|    clip_fraction        | 0.000537      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00267      |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.19e+05      |\n",
      "|    n_updates            | 2600          |\n",
      "|    policy_gradient_loss | -5.03e-05     |\n",
      "|    value_loss           | 2.14e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=535000, episode_reward=-50195.31 +/- 12.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 535000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.837901e-05 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.00215     |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.39e+05     |\n",
      "|    n_updates            | 2610         |\n",
      "|    policy_gradient_loss | -5.8e-05     |\n",
      "|    value_loss           | 2.14e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 262       |\n",
      "|    time_elapsed    | 11794     |\n",
      "|    total_timesteps | 536576    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 263          |\n",
      "|    time_elapsed         | 11827        |\n",
      "|    total_timesteps      | 538624       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.538754e-05 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.00169     |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 7.08e+04     |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.000129    |\n",
      "|    value_loss           | 2.14e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-50210.19 +/- 13.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 540000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00167  |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.4e+04   |\n",
      "|    n_updates            | 2630      |\n",
      "|    policy_gradient_loss | 1.09e-10  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 264       |\n",
      "|    time_elapsed    | 11892     |\n",
      "|    total_timesteps | 540672    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 265       |\n",
      "|    time_elapsed         | 11925     |\n",
      "|    total_timesteps      | 542720    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00167  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.98e+04  |\n",
      "|    n_updates            | 2640      |\n",
      "|    policy_gradient_loss | -1.45e-09 |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 266           |\n",
      "|    time_elapsed         | 11958         |\n",
      "|    total_timesteps      | 544768        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3812591e-05 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0014       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.11e+05      |\n",
      "|    n_updates            | 2650          |\n",
      "|    policy_gradient_loss | -4.61e-05     |\n",
      "|    value_loss           | 2.15e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=545000, episode_reward=-50202.84 +/- 21.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 545000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00137  |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.41e+04  |\n",
      "|    n_updates            | 2660      |\n",
      "|    policy_gradient_loss | -7.75e-10 |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 267       |\n",
      "|    time_elapsed    | 12023     |\n",
      "|    total_timesteps | 546816    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 268       |\n",
      "|    time_elapsed         | 12055     |\n",
      "|    total_timesteps      | 548864    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00137  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.1e+05   |\n",
      "|    n_updates            | 2670      |\n",
      "|    policy_gradient_loss | -8.51e-10 |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=-50201.26 +/- 14.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 550000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00137  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.13e+05  |\n",
      "|    n_updates            | 2680      |\n",
      "|    policy_gradient_loss | -3.45e-10 |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 269       |\n",
      "|    time_elapsed    | 12121     |\n",
      "|    total_timesteps | 550912    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 270           |\n",
      "|    time_elapsed         | 12154         |\n",
      "|    total_timesteps      | 552960        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.6023996e-05 |\n",
      "|    clip_fraction        | 0.000879      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000987     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 8.88e+04      |\n",
      "|    n_updates            | 2690          |\n",
      "|    policy_gradient_loss | -0.000212     |\n",
      "|    value_loss           | 2.19e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=555000, episode_reward=-50202.61 +/- 9.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 555000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000959 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.08e+05  |\n",
      "|    n_updates            | 2700      |\n",
      "|    policy_gradient_loss | -9.83e-10 |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 271       |\n",
      "|    time_elapsed    | 12220     |\n",
      "|    total_timesteps | 555008    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 272          |\n",
      "|    time_elapsed         | 12252        |\n",
      "|    total_timesteps      | 557056       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003000428 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.00152     |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 9.13e+04     |\n",
      "|    n_updates            | 2710         |\n",
      "|    policy_gradient_loss | -0.0001      |\n",
      "|    value_loss           | 2.2e+05      |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 273       |\n",
      "|    time_elapsed         | 12286     |\n",
      "|    total_timesteps      | 559104    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00157  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.95e+04  |\n",
      "|    n_updates            | 2720      |\n",
      "|    policy_gradient_loss | -1.92e-10 |\n",
      "|    value_loss           | 2.21e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-50215.70 +/- 15.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 560000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00157  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.15e+05  |\n",
      "|    n_updates            | 2730      |\n",
      "|    policy_gradient_loss | 9.28e-11  |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 274       |\n",
      "|    time_elapsed    | 12351     |\n",
      "|    total_timesteps | 561152    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 275       |\n",
      "|    time_elapsed         | 12384     |\n",
      "|    total_timesteps      | 563200    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00157  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.14e+05  |\n",
      "|    n_updates            | 2740      |\n",
      "|    policy_gradient_loss | 5.24e-10  |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=565000, episode_reward=-50208.42 +/- 13.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 565000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00157  |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.2e+05   |\n",
      "|    n_updates            | 2750      |\n",
      "|    policy_gradient_loss | -9.51e-10 |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 276       |\n",
      "|    time_elapsed    | 12449     |\n",
      "|    total_timesteps | 565248    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 277       |\n",
      "|    time_elapsed         | 12482     |\n",
      "|    total_timesteps      | 567296    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00157  |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.42e+04  |\n",
      "|    n_updates            | 2760      |\n",
      "|    policy_gradient_loss | 4.28e-10  |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 278       |\n",
      "|    time_elapsed         | 12516     |\n",
      "|    total_timesteps      | 569344    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00157  |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.03e+04  |\n",
      "|    n_updates            | 2770      |\n",
      "|    policy_gradient_loss | 1.01e-09  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=-50199.18 +/- 5.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 570000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00157  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.01e+05  |\n",
      "|    n_updates            | 2780      |\n",
      "|    policy_gradient_loss | -8.58e-10 |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 279       |\n",
      "|    time_elapsed    | 12581     |\n",
      "|    total_timesteps | 571392    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 280       |\n",
      "|    time_elapsed         | 12614     |\n",
      "|    total_timesteps      | 573440    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00157  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.2e+05   |\n",
      "|    n_updates            | 2790      |\n",
      "|    policy_gradient_loss | 6.94e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=575000, episode_reward=-50204.52 +/- 15.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 575000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00157  |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.11e+05  |\n",
      "|    n_updates            | 2800      |\n",
      "|    policy_gradient_loss | -1.09e-11 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 281       |\n",
      "|    time_elapsed    | 12679     |\n",
      "|    total_timesteps | 575488    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 282       |\n",
      "|    time_elapsed         | 12711     |\n",
      "|    total_timesteps      | 577536    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00157  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.01e+05  |\n",
      "|    n_updates            | 2810      |\n",
      "|    policy_gradient_loss | 3.13e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 283       |\n",
      "|    time_elapsed         | 12744     |\n",
      "|    total_timesteps      | 579584    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00157  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.07e+05  |\n",
      "|    n_updates            | 2820      |\n",
      "|    policy_gradient_loss | 6.79e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-50212.22 +/- 17.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 580000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00157  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.13e+05  |\n",
      "|    n_updates            | 2830      |\n",
      "|    policy_gradient_loss | -1.08e-09 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 284       |\n",
      "|    time_elapsed    | 12810     |\n",
      "|    total_timesteps | 581632    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 285           |\n",
      "|    time_elapsed         | 12843         |\n",
      "|    total_timesteps      | 583680        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5053258e-05 |\n",
      "|    clip_fraction        | 0.000293      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00135      |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 9.71e+04      |\n",
      "|    n_updates            | 2840          |\n",
      "|    policy_gradient_loss | -2.84e-05     |\n",
      "|    value_loss           | 2.14e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=585000, episode_reward=-50204.75 +/- 15.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 585000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00129  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.66e+05  |\n",
      "|    n_updates            | 2850      |\n",
      "|    policy_gradient_loss | -7.57e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 286       |\n",
      "|    time_elapsed    | 12908     |\n",
      "|    total_timesteps | 585728    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 287       |\n",
      "|    time_elapsed         | 12942     |\n",
      "|    total_timesteps      | 587776    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00129  |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.04e+05  |\n",
      "|    n_updates            | 2860      |\n",
      "|    policy_gradient_loss | 5.46e-10  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 288       |\n",
      "|    time_elapsed         | 12975     |\n",
      "|    total_timesteps      | 589824    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00129  |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 7.72e+04  |\n",
      "|    n_updates            | 2870      |\n",
      "|    policy_gradient_loss | -1.64e-10 |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=-50208.29 +/- 6.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 590000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00129  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.01e+05  |\n",
      "|    n_updates            | 2880      |\n",
      "|    policy_gradient_loss | 7.43e-10  |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 289       |\n",
      "|    time_elapsed    | 13040     |\n",
      "|    total_timesteps | 591872    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 290           |\n",
      "|    time_elapsed         | 13074         |\n",
      "|    total_timesteps      | 593920        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8404524e-05 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00108      |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.4e+05       |\n",
      "|    n_updates            | 2890          |\n",
      "|    policy_gradient_loss | -8.79e-05     |\n",
      "|    value_loss           | 2.18e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=595000, episode_reward=-50204.57 +/- 7.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 595000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00106  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.28e+05  |\n",
      "|    n_updates            | 2900      |\n",
      "|    policy_gradient_loss | -5.26e-10 |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 291       |\n",
      "|    time_elapsed    | 13139     |\n",
      "|    total_timesteps | 595968    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 292       |\n",
      "|    time_elapsed         | 13171     |\n",
      "|    total_timesteps      | 598016    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00106  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.23e+05  |\n",
      "|    n_updates            | 2910      |\n",
      "|    policy_gradient_loss | -1.08e-10 |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-50189.37 +/- 5.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 600000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00106  |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.01e+05  |\n",
      "|    n_updates            | 2920      |\n",
      "|    policy_gradient_loss | 1.01e-10  |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 293       |\n",
      "|    time_elapsed    | 13236     |\n",
      "|    total_timesteps | 600064    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 294       |\n",
      "|    time_elapsed         | 13268     |\n",
      "|    total_timesteps      | 602112    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00106  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.28e+05  |\n",
      "|    n_updates            | 2930      |\n",
      "|    policy_gradient_loss | 2.58e-10  |\n",
      "|    value_loss           | 2.21e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 295       |\n",
      "|    time_elapsed         | 13302     |\n",
      "|    total_timesteps      | 604160    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00106  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.05e+05  |\n",
      "|    n_updates            | 2940      |\n",
      "|    policy_gradient_loss | -1.04e-09 |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=605000, episode_reward=-50206.62 +/- 15.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 605000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00106  |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.02e+05  |\n",
      "|    n_updates            | 2950      |\n",
      "|    policy_gradient_loss | -2.96e-10 |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 296       |\n",
      "|    time_elapsed    | 13366     |\n",
      "|    total_timesteps | 606208    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 297       |\n",
      "|    time_elapsed         | 13400     |\n",
      "|    total_timesteps      | 608256    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00106  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.95e+04  |\n",
      "|    n_updates            | 2960      |\n",
      "|    policy_gradient_loss | 2.51e-10  |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=-50208.77 +/- 15.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 610000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00106  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.01e+04  |\n",
      "|    n_updates            | 2970      |\n",
      "|    policy_gradient_loss | -9.17e-10 |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 298       |\n",
      "|    time_elapsed    | 13467     |\n",
      "|    total_timesteps | 610304    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 299       |\n",
      "|    time_elapsed         | 13500     |\n",
      "|    total_timesteps      | 612352    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00106  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.09e+05  |\n",
      "|    n_updates            | 2980      |\n",
      "|    policy_gradient_loss | -9.9e-10  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 300       |\n",
      "|    time_elapsed         | 13534     |\n",
      "|    total_timesteps      | 614400    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00106  |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.03e+05  |\n",
      "|    n_updates            | 2990      |\n",
      "|    policy_gradient_loss | 1.52e-09  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=615000, episode_reward=-50189.92 +/- 12.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 615000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6599923e-05 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000932     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 9.57e+04      |\n",
      "|    n_updates            | 3000          |\n",
      "|    policy_gradient_loss | -0.000105     |\n",
      "|    value_loss           | 2.14e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 301       |\n",
      "|    time_elapsed    | 13599     |\n",
      "|    total_timesteps | 616448    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 302          |\n",
      "|    time_elapsed         | 13632        |\n",
      "|    total_timesteps      | 618496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.176205e-06 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.00106     |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 9.97e+04     |\n",
      "|    n_updates            | 3010         |\n",
      "|    policy_gradient_loss | -0.000153    |\n",
      "|    value_loss           | 2.14e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-50212.29 +/- 12.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 620000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00106  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.12e+05  |\n",
      "|    n_updates            | 3020      |\n",
      "|    policy_gradient_loss | 1.06e-09  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 303       |\n",
      "|    time_elapsed    | 13698     |\n",
      "|    total_timesteps | 620544    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 304       |\n",
      "|    time_elapsed         | 13732     |\n",
      "|    total_timesteps      | 622592    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00106  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.11e+05  |\n",
      "|    n_updates            | 3030      |\n",
      "|    policy_gradient_loss | -3.4e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 305       |\n",
      "|    time_elapsed         | 13765     |\n",
      "|    total_timesteps      | 624640    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00106  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.04e+04  |\n",
      "|    n_updates            | 3040      |\n",
      "|    policy_gradient_loss | 1.2e-09   |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=625000, episode_reward=-50204.65 +/- 12.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 625000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0379357e-05 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00116      |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.4e+05       |\n",
      "|    n_updates            | 3050          |\n",
      "|    policy_gradient_loss | -3.18e-05     |\n",
      "|    value_loss           | 2.14e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 306       |\n",
      "|    time_elapsed    | 13830     |\n",
      "|    total_timesteps | 626688    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 307       |\n",
      "|    time_elapsed         | 13863     |\n",
      "|    total_timesteps      | 628736    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00118  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.53e+04  |\n",
      "|    n_updates            | 3060      |\n",
      "|    policy_gradient_loss | -2.08e-10 |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=-50208.24 +/- 11.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 630000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00118  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.24e+05  |\n",
      "|    n_updates            | 3070      |\n",
      "|    policy_gradient_loss | 9.11e-11  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 308       |\n",
      "|    time_elapsed    | 13928     |\n",
      "|    total_timesteps | 630784    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 309           |\n",
      "|    time_elapsed         | 13960         |\n",
      "|    total_timesteps      | 632832        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2870914e-05 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000897     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.11e+05      |\n",
      "|    n_updates            | 3080          |\n",
      "|    policy_gradient_loss | -3.89e-05     |\n",
      "|    value_loss           | 2.16e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 310       |\n",
      "|    time_elapsed         | 13993     |\n",
      "|    total_timesteps      | 634880    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000873 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.27e+05  |\n",
      "|    n_updates            | 3090      |\n",
      "|    policy_gradient_loss | -4.44e-10 |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=635000, episode_reward=-50204.53 +/- 13.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 635000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000873 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.46e+04  |\n",
      "|    n_updates            | 3100      |\n",
      "|    policy_gradient_loss | 2.58e-10  |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 311       |\n",
      "|    time_elapsed    | 14058     |\n",
      "|    total_timesteps | 636928    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 312       |\n",
      "|    time_elapsed         | 14091     |\n",
      "|    total_timesteps      | 638976    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000873 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.3e+05   |\n",
      "|    n_updates            | 3110      |\n",
      "|    policy_gradient_loss | 1.61e-09  |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=-50208.46 +/- 8.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 640000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000873 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.04e+05  |\n",
      "|    n_updates            | 3120      |\n",
      "|    policy_gradient_loss | -1.64e-12 |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 313       |\n",
      "|    time_elapsed    | 14156     |\n",
      "|    total_timesteps | 641024    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 314       |\n",
      "|    time_elapsed         | 14188     |\n",
      "|    total_timesteps      | 643072    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000873 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.11e+05  |\n",
      "|    n_updates            | 3130      |\n",
      "|    policy_gradient_loss | -1.53e-09 |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=645000, episode_reward=-50200.17 +/- 12.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 645000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000873 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.11e+05  |\n",
      "|    n_updates            | 3140      |\n",
      "|    policy_gradient_loss | 6.58e-10  |\n",
      "|    value_loss           | 2.21e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 315       |\n",
      "|    time_elapsed    | 14253     |\n",
      "|    total_timesteps | 645120    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 316       |\n",
      "|    time_elapsed         | 14286     |\n",
      "|    total_timesteps      | 647168    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000873 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.22e+05  |\n",
      "|    n_updates            | 3150      |\n",
      "|    policy_gradient_loss | 3.49e-10  |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 317       |\n",
      "|    time_elapsed         | 14319     |\n",
      "|    total_timesteps      | 649216    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000873 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.91e+04  |\n",
      "|    n_updates            | 3160      |\n",
      "|    policy_gradient_loss | -1.93e-09 |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=-50193.76 +/- 14.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 650000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000873 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.31e+04  |\n",
      "|    n_updates            | 3170      |\n",
      "|    policy_gradient_loss | 1.03e-09  |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 318       |\n",
      "|    time_elapsed    | 14383     |\n",
      "|    total_timesteps | 651264    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 319       |\n",
      "|    time_elapsed         | 14416     |\n",
      "|    total_timesteps      | 653312    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000873 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.23e+05  |\n",
      "|    n_updates            | 3180      |\n",
      "|    policy_gradient_loss | 9.67e-10  |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=655000, episode_reward=-50210.74 +/- 11.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 655000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000873 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.06e+05  |\n",
      "|    n_updates            | 3190      |\n",
      "|    policy_gradient_loss | -1.49e-09 |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 320       |\n",
      "|    time_elapsed    | 14481     |\n",
      "|    total_timesteps | 655360    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 321           |\n",
      "|    time_elapsed         | 14514         |\n",
      "|    total_timesteps      | 657408        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5374651e-05 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00077      |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.23e+05      |\n",
      "|    n_updates            | 3200          |\n",
      "|    policy_gradient_loss | -0.00013      |\n",
      "|    value_loss           | 2.15e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 322       |\n",
      "|    time_elapsed         | 14546     |\n",
      "|    total_timesteps      | 659456    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000762 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.89e+04  |\n",
      "|    n_updates            | 3210      |\n",
      "|    policy_gradient_loss | 7.94e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-50208.57 +/- 11.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 660000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.614634e-05 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000913    |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.13e+05     |\n",
      "|    n_updates            | 3220         |\n",
      "|    policy_gradient_loss | -0.000141    |\n",
      "|    value_loss           | 2.14e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 323       |\n",
      "|    time_elapsed    | 14611     |\n",
      "|    total_timesteps | 661504    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 324       |\n",
      "|    time_elapsed         | 14645     |\n",
      "|    total_timesteps      | 663552    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000921 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.41e+05  |\n",
      "|    n_updates            | 3230      |\n",
      "|    policy_gradient_loss | -5.16e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=665000, episode_reward=-50212.60 +/- 7.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 665000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000922 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.83e+04  |\n",
      "|    n_updates            | 3240      |\n",
      "|    policy_gradient_loss | 1.07e-09  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 325       |\n",
      "|    time_elapsed    | 14710     |\n",
      "|    total_timesteps | 665600    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 326           |\n",
      "|    time_elapsed         | 14742         |\n",
      "|    total_timesteps      | 667648        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5878788e-05 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000774     |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.32e+05      |\n",
      "|    n_updates            | 3250          |\n",
      "|    policy_gradient_loss | -0.000109     |\n",
      "|    value_loss           | 2.14e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 327       |\n",
      "|    time_elapsed         | 14776     |\n",
      "|    total_timesteps      | 669696    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000751 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.35e+05  |\n",
      "|    n_updates            | 3260      |\n",
      "|    policy_gradient_loss | -9.25e-11 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=-50201.07 +/- 11.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 670000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000751 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.17e+05  |\n",
      "|    n_updates            | 3270      |\n",
      "|    policy_gradient_loss | 4.29e-10  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 328       |\n",
      "|    time_elapsed    | 14840     |\n",
      "|    total_timesteps | 671744    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 329       |\n",
      "|    time_elapsed         | 14873     |\n",
      "|    total_timesteps      | 673792    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000751 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.17e+05  |\n",
      "|    n_updates            | 3280      |\n",
      "|    policy_gradient_loss | -5.57e-10 |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=675000, episode_reward=-50208.06 +/- 10.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 675000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000751 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.23e+05  |\n",
      "|    n_updates            | 3290      |\n",
      "|    policy_gradient_loss | -3.35e-10 |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 330       |\n",
      "|    time_elapsed    | 14938     |\n",
      "|    total_timesteps | 675840    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 331       |\n",
      "|    time_elapsed         | 14971     |\n",
      "|    total_timesteps      | 677888    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000751 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.13e+04  |\n",
      "|    n_updates            | 3300      |\n",
      "|    policy_gradient_loss | 5.26e-11  |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 332       |\n",
      "|    time_elapsed         | 15004     |\n",
      "|    total_timesteps      | 679936    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000751 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.59e+04  |\n",
      "|    n_updates            | 3310      |\n",
      "|    policy_gradient_loss | 2.12e-10  |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-50213.50 +/- 11.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 680000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000751 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.95e+04  |\n",
      "|    n_updates            | 3320      |\n",
      "|    policy_gradient_loss | -2.49e-10 |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 333       |\n",
      "|    time_elapsed    | 15068     |\n",
      "|    total_timesteps | 681984    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 334       |\n",
      "|    time_elapsed         | 15101     |\n",
      "|    total_timesteps      | 684032    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000751 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.35e+05  |\n",
      "|    n_updates            | 3330      |\n",
      "|    policy_gradient_loss | 5.14e-10  |\n",
      "|    value_loss           | 2.21e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=685000, episode_reward=-50208.08 +/- 20.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 685000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000751 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.04e+04  |\n",
      "|    n_updates            | 3340      |\n",
      "|    policy_gradient_loss | -4.01e-10 |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 335       |\n",
      "|    time_elapsed    | 15169     |\n",
      "|    total_timesteps | 686080    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 336       |\n",
      "|    time_elapsed         | 15211     |\n",
      "|    total_timesteps      | 688128    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000751 |\n",
      "|    explained_variance   | 1.19e-07  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.32e+05  |\n",
      "|    n_updates            | 3350      |\n",
      "|    policy_gradient_loss | -2.31e-10 |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=-50217.15 +/- 11.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 690000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000751 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.1e+05   |\n",
      "|    n_updates            | 3360      |\n",
      "|    policy_gradient_loss | -4.87e-10 |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 337       |\n",
      "|    time_elapsed    | 15275     |\n",
      "|    total_timesteps | 690176    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 338       |\n",
      "|    time_elapsed         | 15307     |\n",
      "|    total_timesteps      | 692224    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000751 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 7.6e+04   |\n",
      "|    n_updates            | 3370      |\n",
      "|    policy_gradient_loss | -1.66e-10 |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 339       |\n",
      "|    time_elapsed         | 15340     |\n",
      "|    total_timesteps      | 694272    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000751 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.94e+04  |\n",
      "|    n_updates            | 3380      |\n",
      "|    policy_gradient_loss | 6.91e-11  |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=695000, episode_reward=-50202.82 +/- 7.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 695000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000751 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.08e+05  |\n",
      "|    n_updates            | 3390      |\n",
      "|    policy_gradient_loss | 1.62e-10  |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 340       |\n",
      "|    time_elapsed    | 15405     |\n",
      "|    total_timesteps | 696320    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 341           |\n",
      "|    time_elapsed         | 15438         |\n",
      "|    total_timesteps      | 698368        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3885088e-05 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000632     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 7.71e+04      |\n",
      "|    n_updates            | 3400          |\n",
      "|    policy_gradient_loss | -0.000121     |\n",
      "|    value_loss           | 2.15e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=-50217.86 +/- 13.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 700000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.2e+05   |\n",
      "|    n_updates            | 3410      |\n",
      "|    policy_gradient_loss | -1.02e-09 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 342       |\n",
      "|    time_elapsed    | 15502     |\n",
      "|    total_timesteps | 700416    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 343       |\n",
      "|    time_elapsed         | 15535     |\n",
      "|    total_timesteps      | 702464    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.07e+05  |\n",
      "|    n_updates            | 3420      |\n",
      "|    policy_gradient_loss | 8.59e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 344       |\n",
      "|    time_elapsed         | 15568     |\n",
      "|    total_timesteps      | 704512    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.08e+05  |\n",
      "|    n_updates            | 3430      |\n",
      "|    policy_gradient_loss | 6.48e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=705000, episode_reward=-50200.71 +/- 15.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 705000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.14e+05  |\n",
      "|    n_updates            | 3440      |\n",
      "|    policy_gradient_loss | -2.81e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 345       |\n",
      "|    time_elapsed    | 15633     |\n",
      "|    total_timesteps | 706560    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 346       |\n",
      "|    time_elapsed         | 15667     |\n",
      "|    total_timesteps      | 708608    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.12e+05  |\n",
      "|    n_updates            | 3450      |\n",
      "|    policy_gradient_loss | 3.92e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=-50202.79 +/- 12.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 710000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.38e+04  |\n",
      "|    n_updates            | 3460      |\n",
      "|    policy_gradient_loss | -2.53e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 347       |\n",
      "|    time_elapsed    | 15732     |\n",
      "|    total_timesteps | 710656    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 348       |\n",
      "|    time_elapsed         | 15764     |\n",
      "|    total_timesteps      | 712704    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.14e+05  |\n",
      "|    n_updates            | 3470      |\n",
      "|    policy_gradient_loss | 1.29e-09  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 349       |\n",
      "|    time_elapsed         | 15796     |\n",
      "|    total_timesteps      | 714752    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.14e+05  |\n",
      "|    n_updates            | 3480      |\n",
      "|    policy_gradient_loss | -1.53e-10 |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=715000, episode_reward=-50210.63 +/- 10.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 715000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.04e+05  |\n",
      "|    n_updates            | 3490      |\n",
      "|    policy_gradient_loss | 3.74e-10  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 350       |\n",
      "|    time_elapsed    | 15861     |\n",
      "|    total_timesteps | 716800    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 351       |\n",
      "|    time_elapsed         | 15894     |\n",
      "|    total_timesteps      | 718848    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.23e+05  |\n",
      "|    n_updates            | 3500      |\n",
      "|    policy_gradient_loss | -1.42e-09 |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-50217.66 +/- 8.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 720000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.7e+04   |\n",
      "|    n_updates            | 3510      |\n",
      "|    policy_gradient_loss | -4.05e-10 |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 352       |\n",
      "|    time_elapsed    | 15959     |\n",
      "|    total_timesteps | 720896    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 353       |\n",
      "|    time_elapsed         | 15992     |\n",
      "|    total_timesteps      | 722944    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.24e+05  |\n",
      "|    n_updates            | 3520      |\n",
      "|    policy_gradient_loss | 8.91e-10  |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 354       |\n",
      "|    time_elapsed         | 16025     |\n",
      "|    total_timesteps      | 724992    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.22e+05  |\n",
      "|    n_updates            | 3530      |\n",
      "|    policy_gradient_loss | -6.46e-10 |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=725000, episode_reward=-50215.72 +/- 8.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 725000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.06e+04  |\n",
      "|    n_updates            | 3540      |\n",
      "|    policy_gradient_loss | -1.2e-09  |\n",
      "|    value_loss           | 2.21e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 355       |\n",
      "|    time_elapsed    | 16090     |\n",
      "|    total_timesteps | 727040    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 356       |\n",
      "|    time_elapsed         | 16123     |\n",
      "|    total_timesteps      | 729088    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.04e+05  |\n",
      "|    n_updates            | 3550      |\n",
      "|    policy_gradient_loss | 1.18e-10  |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=-50208.28 +/- 16.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 730000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.23e+05  |\n",
      "|    n_updates            | 3560      |\n",
      "|    policy_gradient_loss | 3.09e-10  |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 357       |\n",
      "|    time_elapsed    | 16187     |\n",
      "|    total_timesteps | 731136    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 358       |\n",
      "|    time_elapsed         | 16220     |\n",
      "|    total_timesteps      | 733184    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.02e+05  |\n",
      "|    n_updates            | 3570      |\n",
      "|    policy_gradient_loss | -2.76e-11 |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=735000, episode_reward=-50203.08 +/- 12.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 735000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.88e+04  |\n",
      "|    n_updates            | 3580      |\n",
      "|    policy_gradient_loss | 3.71e-11  |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 359       |\n",
      "|    time_elapsed    | 16285     |\n",
      "|    total_timesteps | 735232    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 360       |\n",
      "|    time_elapsed         | 16318     |\n",
      "|    total_timesteps      | 737280    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.04e+05  |\n",
      "|    n_updates            | 3590      |\n",
      "|    policy_gradient_loss | 7.53e-10  |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 361       |\n",
      "|    time_elapsed         | 16352     |\n",
      "|    total_timesteps      | 739328    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.05e+05  |\n",
      "|    n_updates            | 3600      |\n",
      "|    policy_gradient_loss | -2.31e-10 |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=-50213.73 +/- 16.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 740000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.08e+05  |\n",
      "|    n_updates            | 3610      |\n",
      "|    policy_gradient_loss | -3.55e-10 |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 362       |\n",
      "|    time_elapsed    | 16416     |\n",
      "|    total_timesteps | 741376    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 363       |\n",
      "|    time_elapsed         | 16448     |\n",
      "|    total_timesteps      | 743424    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.17e+05  |\n",
      "|    n_updates            | 3620      |\n",
      "|    policy_gradient_loss | 5.11e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=745000, episode_reward=-50206.71 +/- 13.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 745000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.18e+05  |\n",
      "|    n_updates            | 3630      |\n",
      "|    policy_gradient_loss | -4.48e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 364       |\n",
      "|    time_elapsed    | 16512     |\n",
      "|    total_timesteps | 745472    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 365       |\n",
      "|    time_elapsed         | 16545     |\n",
      "|    total_timesteps      | 747520    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.38e+05  |\n",
      "|    n_updates            | 3640      |\n",
      "|    policy_gradient_loss | 2.12e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 366       |\n",
      "|    time_elapsed         | 16577     |\n",
      "|    total_timesteps      | 749568    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.34e+05  |\n",
      "|    n_updates            | 3650      |\n",
      "|    policy_gradient_loss | -7.59e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=-50213.90 +/- 16.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 750000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.8e+04   |\n",
      "|    n_updates            | 3660      |\n",
      "|    policy_gradient_loss | -1.78e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 367       |\n",
      "|    time_elapsed    | 16643     |\n",
      "|    total_timesteps | 751616    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 368       |\n",
      "|    time_elapsed         | 16676     |\n",
      "|    total_timesteps      | 753664    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.21e+04  |\n",
      "|    n_updates            | 3670      |\n",
      "|    policy_gradient_loss | -1.24e-09 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=755000, episode_reward=-50211.97 +/- 4.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 755000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.12e+05  |\n",
      "|    n_updates            | 3680      |\n",
      "|    policy_gradient_loss | -3.01e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 369       |\n",
      "|    time_elapsed    | 16740     |\n",
      "|    total_timesteps | 755712    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 370       |\n",
      "|    time_elapsed         | 16773     |\n",
      "|    total_timesteps      | 757760    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.2e+05   |\n",
      "|    n_updates            | 3690      |\n",
      "|    policy_gradient_loss | -3.4e-10  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 371       |\n",
      "|    time_elapsed         | 16805     |\n",
      "|    total_timesteps      | 759808    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 7.68e+04  |\n",
      "|    n_updates            | 3700      |\n",
      "|    policy_gradient_loss | 1.12e-10  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=-50202.67 +/- 16.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 760000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000622 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.13e+05  |\n",
      "|    n_updates            | 3710      |\n",
      "|    policy_gradient_loss | -1.78e-10 |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 372       |\n",
      "|    time_elapsed    | 16870     |\n",
      "|    total_timesteps | 761856    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 373           |\n",
      "|    time_elapsed         | 16902         |\n",
      "|    total_timesteps      | 763904        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4754216e-05 |\n",
      "|    clip_fraction        | 0.000342      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000561     |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 9.52e+04      |\n",
      "|    n_updates            | 3720          |\n",
      "|    policy_gradient_loss | -4.24e-05     |\n",
      "|    value_loss           | 2.17e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=765000, episode_reward=-50210.06 +/- 12.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 765000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.15e+05  |\n",
      "|    n_updates            | 3730      |\n",
      "|    policy_gradient_loss | 4.06e-10  |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 374       |\n",
      "|    time_elapsed    | 16967     |\n",
      "|    total_timesteps | 765952    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 375       |\n",
      "|    time_elapsed         | 16999     |\n",
      "|    total_timesteps      | 768000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.08e+05  |\n",
      "|    n_updates            | 3740      |\n",
      "|    policy_gradient_loss | 1.03e-10  |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=-50199.04 +/- 8.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 770000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.21e+05  |\n",
      "|    n_updates            | 3750      |\n",
      "|    policy_gradient_loss | -3.18e-10 |\n",
      "|    value_loss           | 2.22e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 376       |\n",
      "|    time_elapsed    | 17063     |\n",
      "|    total_timesteps | 770048    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 377       |\n",
      "|    time_elapsed         | 17095     |\n",
      "|    total_timesteps      | 772096    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.08e+05  |\n",
      "|    n_updates            | 3760      |\n",
      "|    policy_gradient_loss | 3.49e-11  |\n",
      "|    value_loss           | 2.22e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 378       |\n",
      "|    time_elapsed         | 17129     |\n",
      "|    total_timesteps      | 774144    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.2e+05   |\n",
      "|    n_updates            | 3770      |\n",
      "|    policy_gradient_loss | 9.86e-10  |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=775000, episode_reward=-50219.66 +/- 9.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 775000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.21e+05  |\n",
      "|    n_updates            | 3780      |\n",
      "|    policy_gradient_loss | 4.57e-10  |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 379       |\n",
      "|    time_elapsed    | 17193     |\n",
      "|    total_timesteps | 776192    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 380       |\n",
      "|    time_elapsed         | 17226     |\n",
      "|    total_timesteps      | 778240    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.74e+04  |\n",
      "|    n_updates            | 3790      |\n",
      "|    policy_gradient_loss | 2.67e-10  |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-50217.77 +/- 5.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 780000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.15e+05  |\n",
      "|    n_updates            | 3800      |\n",
      "|    policy_gradient_loss | -4.26e-10 |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 381       |\n",
      "|    time_elapsed    | 17294     |\n",
      "|    total_timesteps | 780288    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 382       |\n",
      "|    time_elapsed         | 17331     |\n",
      "|    total_timesteps      | 782336    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.19e+05  |\n",
      "|    n_updates            | 3810      |\n",
      "|    policy_gradient_loss | 3.09e-11  |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 383       |\n",
      "|    time_elapsed         | 17363     |\n",
      "|    total_timesteps      | 784384    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.47e+05  |\n",
      "|    n_updates            | 3820      |\n",
      "|    policy_gradient_loss | 2.3e-10   |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=785000, episode_reward=-50214.22 +/- 14.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 785000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.56e+04  |\n",
      "|    n_updates            | 3830      |\n",
      "|    policy_gradient_loss | -2.54e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 384       |\n",
      "|    time_elapsed    | 17428     |\n",
      "|    total_timesteps | 786432    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 385       |\n",
      "|    time_elapsed         | 17460     |\n",
      "|    total_timesteps      | 788480    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.15e+05  |\n",
      "|    n_updates            | 3840      |\n",
      "|    policy_gradient_loss | 4.47e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=-50212.09 +/- 13.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 790000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.03e+05  |\n",
      "|    n_updates            | 3850      |\n",
      "|    policy_gradient_loss | 8.64e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 386       |\n",
      "|    time_elapsed    | 17524     |\n",
      "|    total_timesteps | 790528    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 387       |\n",
      "|    time_elapsed         | 17557     |\n",
      "|    total_timesteps      | 792576    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.82e+04  |\n",
      "|    n_updates            | 3860      |\n",
      "|    policy_gradient_loss | 4.1e-10   |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 388       |\n",
      "|    time_elapsed         | 17589     |\n",
      "|    total_timesteps      | 794624    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.12e+05  |\n",
      "|    n_updates            | 3870      |\n",
      "|    policy_gradient_loss | 3.46e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=795000, episode_reward=-50215.57 +/- 16.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 795000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.1e+05   |\n",
      "|    n_updates            | 3880      |\n",
      "|    policy_gradient_loss | 1.01e-09  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 389       |\n",
      "|    time_elapsed    | 17654     |\n",
      "|    total_timesteps | 796672    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 390       |\n",
      "|    time_elapsed         | 17686     |\n",
      "|    total_timesteps      | 798720    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000547 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.43e+05  |\n",
      "|    n_updates            | 3890      |\n",
      "|    policy_gradient_loss | -1.66e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=-50212.04 +/- 19.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 800000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0580348e-05 |\n",
      "|    clip_fraction        | 0.000342      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000601     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.13e+05      |\n",
      "|    n_updates            | 3900          |\n",
      "|    policy_gradient_loss | -3.52e-05     |\n",
      "|    value_loss           | 2.15e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 391       |\n",
      "|    time_elapsed    | 17750     |\n",
      "|    total_timesteps | 800768    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 392       |\n",
      "|    time_elapsed         | 17782     |\n",
      "|    total_timesteps      | 802816    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000611 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.12e+05  |\n",
      "|    n_updates            | 3910      |\n",
      "|    policy_gradient_loss | 3.38e-10  |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 393       |\n",
      "|    time_elapsed         | 17815     |\n",
      "|    total_timesteps      | 804864    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000611 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.66e+04  |\n",
      "|    n_updates            | 3920      |\n",
      "|    policy_gradient_loss | -2.13e-10 |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=805000, episode_reward=-50203.32 +/- 24.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 805000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000611 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.19e+04  |\n",
      "|    n_updates            | 3930      |\n",
      "|    policy_gradient_loss | 1.9e-10   |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 394       |\n",
      "|    time_elapsed    | 17893     |\n",
      "|    total_timesteps | 806912    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 395       |\n",
      "|    time_elapsed         | 17929     |\n",
      "|    total_timesteps      | 808960    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000611 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.48e+05  |\n",
      "|    n_updates            | 3940      |\n",
      "|    policy_gradient_loss | -6.42e-10 |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=-50209.89 +/- 13.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 810000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000611 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.18e+05  |\n",
      "|    n_updates            | 3950      |\n",
      "|    policy_gradient_loss | 8.53e-10  |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 396       |\n",
      "|    time_elapsed    | 18001     |\n",
      "|    total_timesteps | 811008    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 397       |\n",
      "|    time_elapsed         | 18036     |\n",
      "|    total_timesteps      | 813056    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000611 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.1e+05   |\n",
      "|    n_updates            | 3960      |\n",
      "|    policy_gradient_loss | -2.27e-10 |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=815000, episode_reward=-50203.08 +/- 15.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 815000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000611 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1e+05     |\n",
      "|    n_updates            | 3970      |\n",
      "|    policy_gradient_loss | -1.26e-09 |\n",
      "|    value_loss           | 2.21e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 398       |\n",
      "|    time_elapsed    | 18101     |\n",
      "|    total_timesteps | 815104    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 399       |\n",
      "|    time_elapsed         | 18133     |\n",
      "|    total_timesteps      | 817152    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000611 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.44e+05  |\n",
      "|    n_updates            | 3980      |\n",
      "|    policy_gradient_loss | 5.56e-10  |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 400       |\n",
      "|    time_elapsed         | 18166     |\n",
      "|    total_timesteps      | 819200    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000611 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.06e+05  |\n",
      "|    n_updates            | 3990      |\n",
      "|    policy_gradient_loss | 5.44e-10  |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=-50217.98 +/- 9.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 820000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000611 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.08e+05  |\n",
      "|    n_updates            | 4000      |\n",
      "|    policy_gradient_loss | 5.08e-10  |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 401       |\n",
      "|    time_elapsed    | 18230     |\n",
      "|    total_timesteps | 821248    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 402       |\n",
      "|    time_elapsed         | 18262     |\n",
      "|    total_timesteps      | 823296    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000611 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.32e+05  |\n",
      "|    n_updates            | 4010      |\n",
      "|    policy_gradient_loss | 2.56e-10  |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=825000, episode_reward=-50215.78 +/- 8.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 825000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000611 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.52e+04  |\n",
      "|    n_updates            | 4020      |\n",
      "|    policy_gradient_loss | 4.11e-10  |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 403       |\n",
      "|    time_elapsed    | 18326     |\n",
      "|    total_timesteps | 825344    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 404       |\n",
      "|    time_elapsed         | 18361     |\n",
      "|    total_timesteps      | 827392    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000611 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.8e+04   |\n",
      "|    n_updates            | 4030      |\n",
      "|    policy_gradient_loss | -1.81e-09 |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 405       |\n",
      "|    time_elapsed         | 18396     |\n",
      "|    total_timesteps      | 829440    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000611 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.15e+05  |\n",
      "|    n_updates            | 4040      |\n",
      "|    policy_gradient_loss | 3.11e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=-50210.14 +/- 12.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 830000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000611 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.1e+04   |\n",
      "|    n_updates            | 4050      |\n",
      "|    policy_gradient_loss | 5.48e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 406       |\n",
      "|    time_elapsed    | 18471     |\n",
      "|    total_timesteps | 831488    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 407           |\n",
      "|    time_elapsed         | 18503         |\n",
      "|    total_timesteps      | 833536        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1859462e-05 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000524     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.06e+05      |\n",
      "|    n_updates            | 4060          |\n",
      "|    policy_gradient_loss | -9.75e-05     |\n",
      "|    value_loss           | 2.14e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=835000, episode_reward=-50201.05 +/- 13.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 835000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.5560417e-05 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000683     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.21e+05      |\n",
      "|    n_updates            | 4070          |\n",
      "|    policy_gradient_loss | -8.76e-05     |\n",
      "|    value_loss           | 2.14e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 44        |\n",
      "|    iterations      | 408       |\n",
      "|    time_elapsed    | 18568     |\n",
      "|    total_timesteps | 835584    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 409       |\n",
      "|    time_elapsed         | 18600     |\n",
      "|    total_timesteps      | 837632    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000697 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.18e+05  |\n",
      "|    n_updates            | 4080      |\n",
      "|    policy_gradient_loss | 9.61e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 410       |\n",
      "|    time_elapsed         | 18633     |\n",
      "|    total_timesteps      | 839680    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000697 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.17e+05  |\n",
      "|    n_updates            | 4090      |\n",
      "|    policy_gradient_loss | -7.16e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=-50206.76 +/- 7.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 840000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000697 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.07e+05  |\n",
      "|    n_updates            | 4100      |\n",
      "|    policy_gradient_loss | 3.9e-11   |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 411       |\n",
      "|    time_elapsed    | 18697     |\n",
      "|    total_timesteps | 841728    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 412       |\n",
      "|    time_elapsed         | 18729     |\n",
      "|    total_timesteps      | 843776    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000697 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.05e+05  |\n",
      "|    n_updates            | 4110      |\n",
      "|    policy_gradient_loss | 4.34e-10  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=845000, episode_reward=-50210.49 +/- 6.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 845000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000697 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.19e+05  |\n",
      "|    n_updates            | 4120      |\n",
      "|    policy_gradient_loss | -1.84e-09 |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 413       |\n",
      "|    time_elapsed    | 18793     |\n",
      "|    total_timesteps | 845824    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 414       |\n",
      "|    time_elapsed         | 18825     |\n",
      "|    total_timesteps      | 847872    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000697 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.35e+05  |\n",
      "|    n_updates            | 4130      |\n",
      "|    policy_gradient_loss | -1.83e-09 |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 415       |\n",
      "|    time_elapsed         | 18857     |\n",
      "|    total_timesteps      | 849920    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000697 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.06e+05  |\n",
      "|    n_updates            | 4140      |\n",
      "|    policy_gradient_loss | -1.22e-10 |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=-50202.83 +/- 11.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 850000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000697 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.79e+04  |\n",
      "|    n_updates            | 4150      |\n",
      "|    policy_gradient_loss | -5.05e-10 |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 416       |\n",
      "|    time_elapsed    | 18921     |\n",
      "|    total_timesteps | 851968    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 417       |\n",
      "|    time_elapsed         | 18953     |\n",
      "|    total_timesteps      | 854016    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000697 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.96e+04  |\n",
      "|    n_updates            | 4160      |\n",
      "|    policy_gradient_loss | 4.92e-10  |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=855000, episode_reward=-50206.62 +/- 12.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 855000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000697 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.04e+04  |\n",
      "|    n_updates            | 4170      |\n",
      "|    policy_gradient_loss | -6.27e-10 |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 418       |\n",
      "|    time_elapsed    | 19017     |\n",
      "|    total_timesteps | 856064    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 419       |\n",
      "|    time_elapsed         | 19050     |\n",
      "|    total_timesteps      | 858112    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000697 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.55e+04  |\n",
      "|    n_updates            | 4180      |\n",
      "|    policy_gradient_loss | 2.78e-10  |\n",
      "|    value_loss           | 2.21e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=-50208.42 +/- 15.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 860000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000697 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.29e+05  |\n",
      "|    n_updates            | 4190      |\n",
      "|    policy_gradient_loss | 6.86e-10  |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 420       |\n",
      "|    time_elapsed    | 19114     |\n",
      "|    total_timesteps | 860160    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 421       |\n",
      "|    time_elapsed         | 19146     |\n",
      "|    total_timesteps      | 862208    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000697 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.89e+04  |\n",
      "|    n_updates            | 4200      |\n",
      "|    policy_gradient_loss | -1.3e-09  |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 422           |\n",
      "|    time_elapsed         | 19178         |\n",
      "|    total_timesteps      | 864256        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5223224e-05 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000823     |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.15e+05      |\n",
      "|    n_updates            | 4210          |\n",
      "|    policy_gradient_loss | -4.79e-05     |\n",
      "|    value_loss           | 2.17e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=865000, episode_reward=-50206.46 +/- 9.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 865000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.661515e-05 |\n",
      "|    clip_fraction        | 0.00083      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0011      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 9.84e+04     |\n",
      "|    n_updates            | 4220         |\n",
      "|    policy_gradient_loss | -0.000204    |\n",
      "|    value_loss           | 2.16e+05     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 423       |\n",
      "|    time_elapsed    | 19242     |\n",
      "|    total_timesteps | 866304    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 424       |\n",
      "|    time_elapsed         | 19274     |\n",
      "|    total_timesteps      | 868352    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00113  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.84e+04  |\n",
      "|    n_updates            | 4230      |\n",
      "|    policy_gradient_loss | 7.13e-10  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=-50189.96 +/- 5.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 870000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00113  |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.1e+05   |\n",
      "|    n_updates            | 4240      |\n",
      "|    policy_gradient_loss | 1.75e-09  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 425       |\n",
      "|    time_elapsed    | 19338     |\n",
      "|    total_timesteps | 870400    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 426       |\n",
      "|    time_elapsed         | 19370     |\n",
      "|    total_timesteps      | 872448    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00113  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.05e+05  |\n",
      "|    n_updates            | 4250      |\n",
      "|    policy_gradient_loss | 2.92e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 427           |\n",
      "|    time_elapsed         | 19402         |\n",
      "|    total_timesteps      | 874496        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8763974e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00111      |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.22e+05      |\n",
      "|    n_updates            | 4260          |\n",
      "|    policy_gradient_loss | 3.46e-07      |\n",
      "|    value_loss           | 2.14e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=875000, episode_reward=-50214.43 +/- 7.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 875000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00107  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.29e+04  |\n",
      "|    n_updates            | 4270      |\n",
      "|    policy_gradient_loss | -8.15e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 428       |\n",
      "|    time_elapsed    | 19466     |\n",
      "|    total_timesteps | 876544    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 429       |\n",
      "|    time_elapsed         | 19499     |\n",
      "|    total_timesteps      | 878592    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00107  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.45e+04  |\n",
      "|    n_updates            | 4280      |\n",
      "|    policy_gradient_loss | 2.87e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=-50212.17 +/- 9.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 880000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00107  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.81e+04  |\n",
      "|    n_updates            | 4290      |\n",
      "|    policy_gradient_loss | -6.98e-11 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 44        |\n",
      "|    iterations      | 430       |\n",
      "|    time_elapsed    | 19570     |\n",
      "|    total_timesteps | 880640    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 431       |\n",
      "|    time_elapsed         | 19602     |\n",
      "|    total_timesteps      | 882688    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00107  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.01e+05  |\n",
      "|    n_updates            | 4300      |\n",
      "|    policy_gradient_loss | -4.63e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 432       |\n",
      "|    time_elapsed         | 19634     |\n",
      "|    total_timesteps      | 884736    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00107  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.16e+05  |\n",
      "|    n_updates            | 4310      |\n",
      "|    policy_gradient_loss | 6.75e-10  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=885000, episode_reward=-50215.85 +/- 12.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 885000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00107  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.1e+05   |\n",
      "|    n_updates            | 4320      |\n",
      "|    policy_gradient_loss | -2.77e-10 |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 433       |\n",
      "|    time_elapsed    | 19698     |\n",
      "|    total_timesteps | 886784    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 434       |\n",
      "|    time_elapsed         | 19731     |\n",
      "|    total_timesteps      | 888832    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00107  |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.82e+04  |\n",
      "|    n_updates            | 4330      |\n",
      "|    policy_gradient_loss | -3.95e-10 |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=-50218.13 +/- 5.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 890000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00107  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.06e+05  |\n",
      "|    n_updates            | 4340      |\n",
      "|    policy_gradient_loss | 9.92e-10  |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 435       |\n",
      "|    time_elapsed    | 19796     |\n",
      "|    total_timesteps | 890880    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 436       |\n",
      "|    time_elapsed         | 19828     |\n",
      "|    total_timesteps      | 892928    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00107  |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.3e+04   |\n",
      "|    n_updates            | 4350      |\n",
      "|    policy_gradient_loss | 7.23e-10  |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 437           |\n",
      "|    time_elapsed         | 19861         |\n",
      "|    total_timesteps      | 894976        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5259313e-05 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000894     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 9.59e+04      |\n",
      "|    n_updates            | 4360          |\n",
      "|    policy_gradient_loss | -7.21e-05     |\n",
      "|    value_loss           | 2.19e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=895000, episode_reward=-50204.43 +/- 19.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 895000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000877 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.33e+05  |\n",
      "|    n_updates            | 4370      |\n",
      "|    policy_gradient_loss | -8.93e-11 |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 438       |\n",
      "|    time_elapsed    | 19925     |\n",
      "|    total_timesteps | 897024    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 439       |\n",
      "|    time_elapsed         | 19957     |\n",
      "|    total_timesteps      | 899072    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000877 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.03e+05  |\n",
      "|    n_updates            | 4380      |\n",
      "|    policy_gradient_loss | 5.2e-10   |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=-50208.43 +/- 9.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 900000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000877 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 7.55e+04  |\n",
      "|    n_updates            | 4390      |\n",
      "|    policy_gradient_loss | -6.69e-11 |\n",
      "|    value_loss           | 2.21e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 440       |\n",
      "|    time_elapsed    | 20021     |\n",
      "|    total_timesteps | 901120    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 441       |\n",
      "|    time_elapsed         | 20054     |\n",
      "|    total_timesteps      | 903168    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000877 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.23e+05  |\n",
      "|    n_updates            | 4400      |\n",
      "|    policy_gradient_loss | -1.19e-09 |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=905000, episode_reward=-50208.61 +/- 11.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 905000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000877 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.18e+05  |\n",
      "|    n_updates            | 4410      |\n",
      "|    policy_gradient_loss | -7.25e-10 |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 44        |\n",
      "|    iterations      | 442       |\n",
      "|    time_elapsed    | 20118     |\n",
      "|    total_timesteps | 905216    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 443       |\n",
      "|    time_elapsed         | 20150     |\n",
      "|    total_timesteps      | 907264    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000877 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.16e+04  |\n",
      "|    n_updates            | 4420      |\n",
      "|    policy_gradient_loss | -1.11e-09 |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 444       |\n",
      "|    time_elapsed         | 20182     |\n",
      "|    total_timesteps      | 909312    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000877 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.08e+05  |\n",
      "|    n_updates            | 4430      |\n",
      "|    policy_gradient_loss | 1.32e-09  |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=-50193.27 +/- 4.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 910000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000877 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.88e+04  |\n",
      "|    n_updates            | 4440      |\n",
      "|    policy_gradient_loss | 1.21e-10  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 445       |\n",
      "|    time_elapsed    | 20246     |\n",
      "|    total_timesteps | 911360    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 446       |\n",
      "|    time_elapsed         | 20278     |\n",
      "|    total_timesteps      | 913408    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000877 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.21e+05  |\n",
      "|    n_updates            | 4450      |\n",
      "|    policy_gradient_loss | 7.48e-10  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=915000, episode_reward=-50212.16 +/- 13.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 915000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0584557e-05 |\n",
      "|    clip_fraction        | 0.000879      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00102      |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.36e+05      |\n",
      "|    n_updates            | 4460          |\n",
      "|    policy_gradient_loss | -0.000139     |\n",
      "|    value_loss           | 2.14e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 447       |\n",
      "|    time_elapsed    | 20343     |\n",
      "|    total_timesteps | 915456    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 448       |\n",
      "|    time_elapsed         | 20375     |\n",
      "|    total_timesteps      | 917504    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00102  |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.06e+05  |\n",
      "|    n_updates            | 4470      |\n",
      "|    policy_gradient_loss | 7.65e-11  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 449       |\n",
      "|    time_elapsed         | 20408     |\n",
      "|    total_timesteps      | 919552    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00102  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.32e+04  |\n",
      "|    n_updates            | 4480      |\n",
      "|    policy_gradient_loss | -7.43e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=-50212.24 +/- 12.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 920000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00102  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.5e+04   |\n",
      "|    n_updates            | 4490      |\n",
      "|    policy_gradient_loss | 2.46e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 450       |\n",
      "|    time_elapsed    | 20472     |\n",
      "|    total_timesteps | 921600    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 451       |\n",
      "|    time_elapsed         | 20505     |\n",
      "|    total_timesteps      | 923648    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00102  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.64e+04  |\n",
      "|    n_updates            | 4500      |\n",
      "|    policy_gradient_loss | 6.12e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=925000, episode_reward=-50195.28 +/- 7.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 925000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00102  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.06e+05  |\n",
      "|    n_updates            | 4510      |\n",
      "|    policy_gradient_loss | 8.93e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 452       |\n",
      "|    time_elapsed    | 20569     |\n",
      "|    total_timesteps | 925696    |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 45            |\n",
      "|    iterations           | 453           |\n",
      "|    time_elapsed         | 20601         |\n",
      "|    total_timesteps      | 927744        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2236665e-05 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.000816     |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 1.32e+05      |\n",
      "|    n_updates            | 4520          |\n",
      "|    policy_gradient_loss | -0.000132     |\n",
      "|    value_loss           | 2.15e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 454       |\n",
      "|    time_elapsed         | 20633     |\n",
      "|    total_timesteps      | 929792    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0008   |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.68e+04  |\n",
      "|    n_updates            | 4530      |\n",
      "|    policy_gradient_loss | 3.27e-12  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=-50206.51 +/- 11.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 930000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0008   |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.68e+04  |\n",
      "|    n_updates            | 4540      |\n",
      "|    policy_gradient_loss | -2.21e-10 |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 455       |\n",
      "|    time_elapsed    | 20698     |\n",
      "|    total_timesteps | 931840    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 456       |\n",
      "|    time_elapsed         | 20730     |\n",
      "|    total_timesteps      | 933888    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0008   |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.09e+05  |\n",
      "|    n_updates            | 4550      |\n",
      "|    policy_gradient_loss | -6.84e-10 |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=935000, episode_reward=-50198.91 +/- 21.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 935000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0008   |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.21e+05  |\n",
      "|    n_updates            | 4560      |\n",
      "|    policy_gradient_loss | -9.53e-11 |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 457       |\n",
      "|    time_elapsed    | 20795     |\n",
      "|    total_timesteps | 935936    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 458       |\n",
      "|    time_elapsed         | 20827     |\n",
      "|    total_timesteps      | 937984    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0008   |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.38e+05  |\n",
      "|    n_updates            | 4570      |\n",
      "|    policy_gradient_loss | -1.32e-09 |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=-50201.03 +/- 14.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 940000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0008   |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.13e+05  |\n",
      "|    n_updates            | 4580      |\n",
      "|    policy_gradient_loss | -7.34e-10 |\n",
      "|    value_loss           | 2.21e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 44        |\n",
      "|    iterations      | 459       |\n",
      "|    time_elapsed    | 20892     |\n",
      "|    total_timesteps | 940032    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 460       |\n",
      "|    time_elapsed         | 20924     |\n",
      "|    total_timesteps      | 942080    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0008   |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.19e+05  |\n",
      "|    n_updates            | 4590      |\n",
      "|    policy_gradient_loss | 6.57e-10  |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 461       |\n",
      "|    time_elapsed         | 20956     |\n",
      "|    total_timesteps      | 944128    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0008   |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.17e+05  |\n",
      "|    n_updates            | 4600      |\n",
      "|    policy_gradient_loss | -1.63e-09 |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=945000, episode_reward=-50206.35 +/- 17.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 945000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0008   |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.61e+04  |\n",
      "|    n_updates            | 4610      |\n",
      "|    policy_gradient_loss | -9.29e-10 |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 462       |\n",
      "|    time_elapsed    | 21019     |\n",
      "|    total_timesteps | 946176    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 463       |\n",
      "|    time_elapsed         | 21052     |\n",
      "|    total_timesteps      | 948224    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0008   |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.4e+05   |\n",
      "|    n_updates            | 4620      |\n",
      "|    policy_gradient_loss | 4.14e-10  |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=-50199.38 +/- 10.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.02e+04     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 950000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6273203e-05 |\n",
      "|    clip_fraction        | 0.000342      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00067      |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 9.15e+04      |\n",
      "|    n_updates            | 4630          |\n",
      "|    policy_gradient_loss | -6.39e-05     |\n",
      "|    value_loss           | 2.17e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 44        |\n",
      "|    iterations      | 464       |\n",
      "|    time_elapsed    | 21122     |\n",
      "|    total_timesteps | 950272    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 465       |\n",
      "|    time_elapsed         | 21154     |\n",
      "|    total_timesteps      | 952320    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000651 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.01e+05  |\n",
      "|    n_updates            | 4640      |\n",
      "|    policy_gradient_loss | -9.47e-10 |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 466       |\n",
      "|    time_elapsed         | 21186     |\n",
      "|    total_timesteps      | 954368    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000651 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.21e+05  |\n",
      "|    n_updates            | 4650      |\n",
      "|    policy_gradient_loss | 8.53e-10  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=955000, episode_reward=-50209.93 +/- 12.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 955000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000651 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1e+05     |\n",
      "|    n_updates            | 4660      |\n",
      "|    policy_gradient_loss | -8e-12    |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 467       |\n",
      "|    time_elapsed    | 21251     |\n",
      "|    total_timesteps | 956416    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 468       |\n",
      "|    time_elapsed         | 21283     |\n",
      "|    total_timesteps      | 958464    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000651 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.06e+05  |\n",
      "|    n_updates            | 4670      |\n",
      "|    policy_gradient_loss | 6.28e-10  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=-50210.14 +/- 11.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 960000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000651 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.1e+05   |\n",
      "|    n_updates            | 4680      |\n",
      "|    policy_gradient_loss | -3.97e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 44        |\n",
      "|    iterations      | 469       |\n",
      "|    time_elapsed    | 21346     |\n",
      "|    total_timesteps | 960512    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 470       |\n",
      "|    time_elapsed         | 21379     |\n",
      "|    total_timesteps      | 962560    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000651 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.2e+05   |\n",
      "|    n_updates            | 4690      |\n",
      "|    policy_gradient_loss | -6.89e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 471       |\n",
      "|    time_elapsed         | 21410     |\n",
      "|    total_timesteps      | 964608    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000651 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.76e+04  |\n",
      "|    n_updates            | 4700      |\n",
      "|    policy_gradient_loss | -8.23e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=965000, episode_reward=-50200.71 +/- 16.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 965000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000651 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.19e+05  |\n",
      "|    n_updates            | 4710      |\n",
      "|    policy_gradient_loss | -4.05e-10 |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 472       |\n",
      "|    time_elapsed    | 21475     |\n",
      "|    total_timesteps | 966656    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -5.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 473         |\n",
      "|    time_elapsed         | 21507       |\n",
      "|    total_timesteps      | 968704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 5.09682e-05 |\n",
      "|    clip_fraction        | 0.000391    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.000472   |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 8.34e+04    |\n",
      "|    n_updates            | 4720        |\n",
      "|    policy_gradient_loss | -0.000127   |\n",
      "|    value_loss           | 2.14e+05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=-50217.46 +/- 13.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 970000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00045  |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 7.67e+04  |\n",
      "|    n_updates            | 4730      |\n",
      "|    policy_gradient_loss | 4.77e-10  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 474       |\n",
      "|    time_elapsed    | 21570     |\n",
      "|    total_timesteps | 970752    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 475       |\n",
      "|    time_elapsed         | 21602     |\n",
      "|    total_timesteps      | 972800    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00045  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.07e+05  |\n",
      "|    n_updates            | 4740      |\n",
      "|    policy_gradient_loss | 9.66e-10  |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 476       |\n",
      "|    time_elapsed         | 21634     |\n",
      "|    total_timesteps      | 974848    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00045  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.2e+05   |\n",
      "|    n_updates            | 4750      |\n",
      "|    policy_gradient_loss | 3e-10     |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=975000, episode_reward=-50214.12 +/- 9.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 975000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00045  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.17e+05  |\n",
      "|    n_updates            | 4760      |\n",
      "|    policy_gradient_loss | 9.08e-10  |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 477       |\n",
      "|    time_elapsed    | 21699     |\n",
      "|    total_timesteps | 976896    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 478       |\n",
      "|    time_elapsed         | 21731     |\n",
      "|    total_timesteps      | 978944    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00045  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.17e+04  |\n",
      "|    n_updates            | 4770      |\n",
      "|    policy_gradient_loss | 1.52e-09  |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=-50215.63 +/- 12.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 980000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00045  |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.01e+05  |\n",
      "|    n_updates            | 4780      |\n",
      "|    policy_gradient_loss | 2.89e-10  |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 45        |\n",
      "|    iterations      | 479       |\n",
      "|    time_elapsed    | 21795     |\n",
      "|    total_timesteps | 980992    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -5.02e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 480          |\n",
      "|    time_elapsed         | 21827        |\n",
      "|    total_timesteps      | 983040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.663518e-05 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000568    |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 9.36e+04     |\n",
      "|    n_updates            | 4790         |\n",
      "|    policy_gradient_loss | -0.000112    |\n",
      "|    value_loss           | 2.21e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=985000, episode_reward=-50221.11 +/- 12.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 985000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000572 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.02e+05  |\n",
      "|    n_updates            | 4800      |\n",
      "|    policy_gradient_loss | -2.89e-10 |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 44        |\n",
      "|    iterations      | 481       |\n",
      "|    time_elapsed    | 21899     |\n",
      "|    total_timesteps | 985088    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 482       |\n",
      "|    time_elapsed         | 21931     |\n",
      "|    total_timesteps      | 987136    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000572 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 8.67e+04  |\n",
      "|    n_updates            | 4810      |\n",
      "|    policy_gradient_loss | -1.45e-10 |\n",
      "|    value_loss           | 2.2e+05   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 483       |\n",
      "|    time_elapsed         | 21963     |\n",
      "|    total_timesteps      | 989184    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000572 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.49e+05  |\n",
      "|    n_updates            | 4820      |\n",
      "|    policy_gradient_loss | -3.93e-10 |\n",
      "|    value_loss           | 2.19e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=-50218.21 +/- 8.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 990000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000572 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.12e+05  |\n",
      "|    n_updates            | 4830      |\n",
      "|    policy_gradient_loss | -1.18e-09 |\n",
      "|    value_loss           | 2.18e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 44        |\n",
      "|    iterations      | 484       |\n",
      "|    time_elapsed    | 22028     |\n",
      "|    total_timesteps | 991232    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 485       |\n",
      "|    time_elapsed         | 22061     |\n",
      "|    total_timesteps      | 993280    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000572 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 9.2e+04   |\n",
      "|    n_updates            | 4840      |\n",
      "|    policy_gradient_loss | -1.65e-10 |\n",
      "|    value_loss           | 2.17e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=995000, episode_reward=-50206.97 +/- 10.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 995000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000572 |\n",
      "|    explained_variance   | 0         |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.14e+05  |\n",
      "|    n_updates            | 4850      |\n",
      "|    policy_gradient_loss | 5.88e-10  |\n",
      "|    value_loss           | 2.16e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 44        |\n",
      "|    iterations      | 486       |\n",
      "|    time_elapsed    | 22125     |\n",
      "|    total_timesteps | 995328    |\n",
      "----------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 487       |\n",
      "|    time_elapsed         | 22158     |\n",
      "|    total_timesteps      | 997376    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000572 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.13e+05  |\n",
      "|    n_updates            | 4860      |\n",
      "|    policy_gradient_loss | -9.36e-10 |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 45        |\n",
      "|    iterations           | 488       |\n",
      "|    time_elapsed         | 22190     |\n",
      "|    total_timesteps      | 999424    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000572 |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.06e+05  |\n",
      "|    n_updates            | 4870      |\n",
      "|    policy_gradient_loss | -1.41e-09 |\n",
      "|    value_loss           | 2.15e+05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=-50206.28 +/- 9.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -5.02e+04 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1000000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.000572 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 1.05e+05  |\n",
      "|    n_updates            | 4880      |\n",
      "|    policy_gradient_loss | 1.82e-09  |\n",
      "|    value_loss           | 2.14e+05  |\n",
      "---------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.02e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 44        |\n",
      "|    iterations      | 489       |\n",
      "|    time_elapsed    | 22255     |\n",
      "|    total_timesteps | 1001472   |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simple DQN architecture\n",
    "policy_kwargs = dict(net_arch=[64, 64])  # Simpler architecture with 2 layers of 64 units\n",
    "\n",
    "# Set up the model with simpler hyperparameters\n",
    "model = PPO('CnnPolicy', custom_env, policy_kwargs=policy_kwargs, \n",
    "            verbose=1, learning_rate=0.0005, batch_size=32)\n",
    "\n",
    "# Setup evaluation and checkpoint callbacks\n",
    "eval_callback = EvalCallback(custom_env, best_model_save_path=model_dir,\n",
    "                             log_path=model_dir, eval_freq=5000, n_eval_episodes=5,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=model_dir,\n",
    "                                         name_prefix='ppo_model_checkpoint')\n",
    "\n",
    "\n",
    "\n",
    "# Start training the model with callbacks for evaluation and checkpoints\n",
    "model.learn(total_timesteps=1_000_000, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "# Save the final model after training\n",
    "model.save(\"ppo_custom_env_model\")\n",
    "custom_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "del env\n",
    "foo = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust number of episodes based on the environment's characteristics\n",
    "if hasattr(env, \"max_episode_steps\"):\n",
    "    # If the environment has predefined max steps, use a higher number for evaluation\n",
    "    num_episodes = 50  \n",
    "else:\n",
    "    # For simpler environments, use fewer episodes\n",
    "    num_episodes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = foo #load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "episode_rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = model.predict(obs)  # Use trained policy\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        record_agent_dynamics(env)  # Record smoothness metrics\n",
    "        if done:\n",
    "            break\n",
    "    episode_rewards.append(total_reward)\n",
    "    obs = env.reset()\n",
    "\n",
    "print(f\"Average Reward: {np.mean(episode_rewards)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gravar os video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psuper/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.77GB > 0.74GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running episode 0\n"
     ]
    }
   ],
   "source": [
    "# DQN original\n",
    "\n",
    "# Define a Environment\n",
    "env = gym.make(\"CarRacing-v3\", continuous=False, render_mode='rgb_array') \n",
    "\n",
    "# Video recorder wrapper\n",
    "trigger = lambda t: t == 0\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=f\"./recordings/original/DQN\",\n",
    "    episode_trigger=trigger,\n",
    "    video_length=0,\n",
    "    disable_logger=True,\n",
    ")\n",
    "\n",
    "model = DQN.load(path=\"best_model/best_model_2.1.1.zip\", env=env)\n",
    "\n",
    "\n",
    "# Perform N Episodes\n",
    "for ep in range(1):\n",
    "    print(\"Running episode\", ep)\n",
    "    obs, info = env.reset()\n",
    "    trunc = False\n",
    "    while not trunc:\n",
    "        # pass observation to model to get predicted action\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "\n",
    "        # pass action to env and get info back\n",
    "        obs, rewards, trunc, done, info = env.step(action)\n",
    "\n",
    "        # show the environment on the screen\n",
    "        env.render()\n",
    "        # print(ep, rewards, trunc)\n",
    "        # print(\"---------------\")\n",
    "\n",
    "    # Close the Environment\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psuper/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/psuper/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/reinforcement-learning-with-gymnasium/recordings/PPO folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/psuper/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n",
      "/home/psuper/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Action spaces do not match: Box([-1.  0.  0.], 1.0, (3,), float32) != Discrete(5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m      9\u001b[0m trigger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m t: t \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m env \u001b[38;5;241m=\u001b[39m RecordVideo(\n\u001b[1;32m     11\u001b[0m     env,\n\u001b[1;32m     12\u001b[0m     video_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./recordings/PPO\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     disable_logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_model/best_model_2.2.2.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Perform N Episodes\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:716\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[0;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_env(env, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# Check if given env is valid\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m \u001b[43mcheck_for_correct_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservation_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Discard `_last_obs`, this will force the env to reset before training\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;66;03m# See issue https://github.com/DLR-RM/stable-baselines3/issues/597\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_reset \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Escola/Universidade/3º Ano/1_sem/isia/trab2/Gym-RL-Project-ISIA/.venv/lib/python3.10/site-packages/stable_baselines3/common/utils.py:233\u001b[0m, in \u001b[0;36mcheck_for_correct_spaces\u001b[0;34m(env, observation_space, action_space)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation spaces do not match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_space \u001b[38;5;241m!=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space:\n\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction spaces do not match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Action spaces do not match: Box([-1.  0.  0.], 1.0, (3,), float32) != Discrete(5)"
     ]
    }
   ],
   "source": [
    "# PPO custom\n",
    "\n",
    "# Define a Environment\n",
    "env = EnhancedCarRacing(render_mode=\"rgb_array\")\n",
    "env = GrayscaleObservation(env, keep_dim=True)\n",
    "# env = TimeLimit(env, max_episode_steps=1000)\n",
    "\n",
    "# Video recorder wrapper\n",
    "trigger = lambda t: t == 0\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=f\"./recordings/custom/PPO\",\n",
    "    episode_trigger=trigger,\n",
    "    video_length=0,\n",
    "    disable_logger=True,\n",
    ")\n",
    "\n",
    "model = PPO.load(path=\"best_model/best_model_2.2.2.zip\", env=env)\n",
    "\n",
    "\n",
    "# Perform N Episodes\n",
    "for ep in range(1):\n",
    "    print(\"Running episode\", ep)\n",
    "    obs, info = env.reset()\n",
    "    trunc = False\n",
    "    while not trunc:\n",
    "        # pass observation to model to get predicted action\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "\n",
    "        # pass action to env and get info back\n",
    "        obs, rewards, trunc, done, info = env.step(action)\n",
    "\n",
    "        # show the environment on the screen\n",
    "        env.render()\n",
    "        # print(ep, rewards, trunc)\n",
    "        # print(\"---------------\")\n",
    "\n",
    "    # Close the Environment\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
