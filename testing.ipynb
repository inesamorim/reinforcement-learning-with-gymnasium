{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import GrayscaleObservation, ResizeObservation, RecordEpisodeStatistics, RecordVideo, TimeLimit\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "import os\n",
    "import gc\n",
    "from eval import *\n",
    "from custom_cr import EnhancedCarRacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Task Specific Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Robustness and Adaptability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Function Definition:\n",
    "   The function `evaluate_robustness` takes a trained model, an environment, and some parameters for evaluation (number of episodes, noise standard deviation, and perturbation probability).\n",
    "\n",
    "2. Results Structure:\n",
    "   It initializes a dictionary `results` to store rewards for three different robustness scenarios: observation noise, environment perturbations, and different initial states.\n",
    "\n",
    "3. Robustness to Observation Noise:\n",
    "   - It runs `num_episodes` episodes with added Gaussian noise to the observations.\n",
    "   - For each step, it adds noise to the observation before predicting an action.\n",
    "   - It accumulates the total reward for each episode and stores it in `noise_rewards`.\n",
    "\n",
    "4. Robustness to Environment Perturbations:\n",
    "   - It runs another set of episodes, this time applying random perturbations to the environment.\n",
    "   - With probability `perturbation_prob`, it adds uniform random noise to the observation.\n",
    "   - It accumulates the total reward for each episode and stores it in `perturbation_rewards`.\n",
    "\n",
    "5. Robustness to Different Initial States:\n",
    "   - It runs a final set of episodes, each time resetting the environment with random initial conditions.\n",
    "   - It uses a custom method `set_random_initial_conditions()` (which should be implemented in the environment) to vary the starting state.\n",
    "   - It accumulates the total reward for each episode and stores it in `initial_state_rewards`.\n",
    "\n",
    "6. Results Computation and Output:\n",
    "   - For each robustness scenario, it calculates and prints the mean and standard deviation of the rewards.\n",
    "   - Finally, it returns the `results` dictionary containing all the collected rewards.\n",
    "\n",
    "This function is designed to evaluate how well the trained model performs under different types of perturbations and variations, which is crucial for assessing the robustness and generalization capabilities of the reinforcement learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_robustness(model, env, num_episodes=10, noise_std=0.1, perturbation_prob=0.1):\n",
    "    \"\"\"\n",
    "    Evaluate the robustness of a trained model under various challenging conditions.\n",
    "\n",
    "    This function tests the model's ability to handle noisy observations, random perturbations, \n",
    "    and diverse initial states in the environment. Results include performance metrics such as \n",
    "    mean rewards and standard deviations under each condition.\n",
    "\n",
    "    Args:\n",
    "        model (BaseAlgorithm): ThKeysView(NpzFile './best_model/best_model_2.1.1.zip' with keys: data, pytorch_variables.pth, policy.pth, policy.optimizer.pth, _stable_baselines3_version...)e trained model to evaluate. Should support `.predict()` for action selection.\n",
    "        env (gym.Env): The environment in which the model will be tested.\n",
    "        num_episodes (int, optional): The number of episodes to run for each robustness scenario. Defaults to 10.\n",
    "        noise_std (float, optional): Standard deviation of Gaussian noise added to observations. Defaults to 0.1.\n",
    "        perturbation_prob (float, optional): Probability of applying random perturbations to observations. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with keys:\n",
    "            - \"noise_rewards\": List of total rewards for episodes with noisy observations.\n",
    "            - \"perturbation_rewards\": List of total rewards for episodes with random perturbations.\n",
    "            - \"initial_state_rewards\": List of total rewards for episodes starting from diverse initial states.\n",
    "        Each list includes rewards from `num_episodes` episodes.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"noise_rewards\": [],\n",
    "        \"perturbation_rewards\": [],\n",
    "        \"initial_state_rewards\": []\n",
    "    }\n",
    "    \n",
    "    # Evaluate robustness to observation noise\n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Add Gaussian noise to observation\n",
    "            noisy_obs = obs + np.random.normal(0, noise_std, obs.shape)\n",
    "            action = model.predict(noisy_obs, deterministic=True)[0]\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        results[\"noise_rewards\"].append(total_reward)\n",
    "\n",
    "    # Evaluate robustness to environment perturbations\n",
    "    for _ in range(num_episodes):\n",
    "        print(f\"Running episode {_}\")\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = model.predict(obs, deterministic=True)[0]\n",
    "            \n",
    "            # Apply random perturbations\n",
    "            if np.random.random() < perturbation_prob:\n",
    "                obs = obs + np.random.uniform(-0.5, 0.5, obs.shape)\n",
    "                \n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        print(f\"Perturbation episode: Total Reward = {total_reward}\")  # For debugging purposes only, remove in production code.  # Evaluate robustness to environment perturb\n",
    "        results[\"perturbation_rewards\"].append(total_reward)\n",
    "\n",
    "    # Evaluate robustness across different initial states\n",
    "    for _ in range(num_episodes):\n",
    "        env.reset()\n",
    "        # Set custom initial conditions (e.g., random car position)\n",
    "        env.set_random_initial_conditions()\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = model.predict(obs, deterministic=True)[0]\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        print(f\"Initial State episode: Total Reward = {total_reward}\")  # For debugging purposes only, remove in production code.  # Evaluate robustness across different initial states\n",
    "        results[\"initial_state_rewards\"].append(total_reward)\n",
    "\n",
    "    # Compute and return mean and standard deviation for all scenarios\n",
    "    for key in results:\n",
    "        rewards = np.array(results[key])\n",
    "        print(f\"{key}: Mean = {rewards.mean()}, Std Dev = {rewards.std()}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Baseline - DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object exploration_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n",
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.77GB > 0.72GB\n",
      "  warnings.warn(\n",
      "[W1218 15:52:44.824795611 NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "best_model_baseline_dqn = DQN.load('./best_model/best_model_2.1.1.zip', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running episode 0\n",
      "Perturbation episode: Total Reward = 892.3999999999816\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TimeLimit' object has no attribute 'set_random_initial_conditions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m robustness_results_baseline_dqn \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_robustness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_baseline_dqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 64\u001b[0m, in \u001b[0;36mevaluate_robustness\u001b[0;34m(model, env, num_episodes, noise_std, perturbation_prob)\u001b[0m\n\u001b[1;32m     62\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Set custom initial conditions (e.g., random car position)\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_random_initial_conditions\u001b[49m()\n\u001b[1;32m     65\u001b[0m obs, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     66\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TimeLimit' object has no attribute 'set_random_initial_conditions'"
     ]
    }
   ],
   "source": [
    "robustness_results_baseline_dqn = evaluate_robustness(best_model_baseline_dqn, env, num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_results_baseline_dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Baseline - PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Custom Environment - DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Custom Environment - PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Environment\n",
    "env = EnhancedCarRacing(render_mode=\"rgb_array\")\n",
    "env = GrayscaleObservation(env, keep_dim=True)\n",
    "# env = TimeLimit(env, max_episode_steps=1000)\n",
    "\n",
    "# Video recorder wrapper\n",
    "trigger = lambda t: t == 0\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=f\"./recordings/PPO\",\n",
    "    episode_trigger=trigger,\n",
    "    video_length=0,\n",
    "    disable_logger=True,\n",
    ")\n",
    "\n",
    "best_model_custom_ppo = PPO.load(path=\"best_model/best_model_2.2.2.zip\", env=env)\n",
    "\n",
    "\n",
    "# Perform N Episodes\n",
    "for ep in range(1):\n",
    "    print(\"Running episode\", ep)\n",
    "    obs, info = env.reset()\n",
    "    trunc = False\n",
    "    while not trunc:\n",
    "        # pass observation to model to get predicted action\n",
    "        action, _states = best_model_custom_ppo.predict(obs, deterministic=True)\n",
    "\n",
    "        # pass action to env and get info back\n",
    "        obs, rewards, trunc, done, info = env.step(action)\n",
    "\n",
    "        # show the environment on the screen\n",
    "        env.render()\n",
    "        # print(ep, rewards, trunc)\n",
    "        # print(\"---------------\")\n",
    "\n",
    "    # Close the Environment\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_results_custom_ppo = evaluate_robustness(best_model_custom_ppo, env, num_episodes=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
