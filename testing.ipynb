{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import GrayscaleObservation, ResizeObservation, RecordEpisodeStatistics, RecordVideo, TimeLimit\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "import os\n",
    "import gc\n",
    "from eval import *\n",
    "from custom_cr import EnhancedCarRacing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Task Specific Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Robustness and Adaptability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Robustness to Observation Noise:\n",
    "   - It runs `num_episodes` episodes with added Gaussian noise to the observations.\n",
    "   - For each step, it adds noise to the observation before predicting an action.\n",
    "   - It accumulates the total reward for each episode and stores it in `noise_rewards`.\n",
    "\n",
    "2. Robustness to Environment Perturbations:\n",
    "   - It runs another set of episodes, this time applying random perturbations to the environment.\n",
    "   - With probability `perturbation_prob`, it adds uniform random noise to the observation.\n",
    "   - It accumulates the total reward for each episode and stores it in `perturbation_rewards`.\n",
    "\n",
    "3. Results Computation and Output:\n",
    "   - For each robustness scenario, it calculates and prints the mean and standard deviation of the rewards.\n",
    "   \n",
    "This function is designed to evaluate how well the trained model performs under different types of perturbations and variations, which is crucial for assessing the robustness and generalization capabilities of the reinforcement learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_robustness(model, env, num_episodes=10, noise_std=0.1, perturbation_prob=0.1):\n",
    "    \"\"\"\n",
    "    Evaluate the robustness of a trained model under various challenging conditions.\n",
    "\n",
    "    This function tests the model's ability to handle noisy observations, random perturbations, \n",
    "    and diverse initial states in the environment. Results include performance metrics such as \n",
    "    mean rewards and standard deviations under each condition.\n",
    "\n",
    "    Args:\n",
    "        model (BaseAlgorithm): ThKeysView(NpzFile './best_model/best_model_2.1.1.zip' with keys: data, pytorch_variables.pth, policy.pth, policy.optimizer.pth, _stable_baselines3_version...)e trained model to evaluate. Should support `.predict()` for action selection.\n",
    "        env (gym.Env): The environment in which the model will be tested.\n",
    "        num_episodes (int, optional): The number of episodes to run for each robustness scenario. Defaults to 10.\n",
    "        noise_std (float, optional): Standard deviation of Gaussian noise added to observations. Defaults to 0.1.\n",
    "        perturbation_prob (float, optional): Probability of applying random perturbations to observations. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with keys:\n",
    "            - \"noise_rewards\": List of total rewards for episodes with noisy observations.\n",
    "            - \"perturbation_rewards\": List of total rewards for episodes with random perturbations.\n",
    "            - \"initial_state_rewards\": List of total rewards for episodes starting from diverse initial states.\n",
    "        Each list includes rewards from `num_episodes` episodes.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"noise_rewards\": [],\n",
    "        \"perturbation_rewards\": []\n",
    "    }\n",
    "    \n",
    "    # Evaluate robustness to observation noise\n",
    "    print(\"Evaluating robustness to observation noise...\")\n",
    "    for _ in range(num_episodes):\n",
    "        print(f\"Running episode {_}\")\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Add Gaussian noise to observation\n",
    "            noisy_obs = obs + np.random.normal(0, noise_std, obs.shape)\n",
    "            action = model.predict(noisy_obs, deterministic=True)[0]\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        results[\"noise_rewards\"].append(total_reward)\n",
    "        print(f\"Perturbation episode: Total Reward = {total_reward}\")  # For debugging \n",
    "\n",
    "    # Evaluate robustness to environment perturbations\n",
    "    print(\"Evaluating robustness to environment perturbations...\")\n",
    "    for _ in range(num_episodes):\n",
    "        print(f\"Running episode {_}\")\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = model.predict(obs, deterministic=True)[0]\n",
    "            \n",
    "            # Apply random perturbations\n",
    "            if np.random.random() < perturbation_prob:\n",
    "                obs = obs + np.random.uniform(-0.5, 0.5, obs.shape)\n",
    "                \n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        print(f\"Perturbation episode: Total Reward = {total_reward}\")  # For debugging purposes only, remove in production code.  # Evaluate robustness to environment perturb\n",
    "        results[\"perturbation_rewards\"].append(total_reward)\n",
    "\n",
    "    # Compute and return mean and standard deviation for all scenarios\n",
    "    for key in results:\n",
    "        rewards = np.array(results[key])\n",
    "        print(f\"{key}: Mean = {rewards.mean()}, Std Dev = {rewards.std()}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Baseline - DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n",
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object exploration_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n",
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.77GB > 1.04GB\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_model_baseline_dqn = DQN.load('./models/baseline/DQN/best_model.zip', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_results_baseline_dqn = evaluate_robustness(best_model_baseline_dqn, env, num_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating robustness to observation noise...\n",
      "Running episode 0\n",
      "Perturbation episode: Total Reward = 914.0999999999913\n",
      "Evaluating robustness to environment perturbations...\n",
      "Running episode 0\n",
      "Perturbation episode: Total Reward = 907.1999999999932\n",
      "noise_rewards: Mean = 914.0999999999913, Std Dev = 0.0\n",
      "perturbation_rewards: Mean = 907.1999999999932, Std Dev = 0.0\n"
     ]
    }
   ],
   "source": [
    "robustness_results_baseline_dqn = evaluate_robustness(best_model_baseline_dqn, env, num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'noise_rewards': [914.0999999999913],\n",
       " 'perturbation_rewards': [907.1999999999932]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robustness_results_baseline_dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating robustness to observation noise...\n",
      "Running episode 0\n",
      "Perturbation episode: Total Reward = 912.0999999999848\n",
      "Evaluating robustness to environment perturbations...\n",
      "Running episode 0\n",
      "Perturbation episode: Total Reward = 835.899999999969\n",
      "noise_rewards: Mean = 912.0999999999848, Std Dev = 0.0\n",
      "perturbation_rewards: Mean = 835.899999999969, Std Dev = 0.0\n"
     ]
    }
   ],
   "source": [
    "foo = evaluate_robustness(best_model_baseline_dqn, env, num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_results_baseline_dqn['noise_rewards'].append(foo['noise_rewards'])\n",
    "robustness_results_baseline_dqn['perturbation_rewards'].append(foo['perturbation_rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_results_baseline_dqn = pd.DataFrame(robustness_results_baseline_dqn)\n",
    "robustness_results_baseline_dqn.to_csv('results/robustness_results_baseline_dqn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noise_rewards</th>\n",
       "      <th>perturbation_rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>914.1</td>\n",
       "      <td>907.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>823.0</td>\n",
       "      <td>718.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>844.3</td>\n",
       "      <td>897.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>908.7</td>\n",
       "      <td>916.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>914.0</td>\n",
       "      <td>905.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>912.1</td>\n",
       "      <td>835.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   noise_rewards  perturbation_rewards\n",
       "0          914.1                 907.2\n",
       "1          823.0                 718.9\n",
       "2          844.3                 897.7\n",
       "3          908.7                 916.7\n",
       "4          914.0                 905.1\n",
       "5          912.1                 835.9"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robustness_results_baseline_dqn = pd.read_csv('results/robustness_results_baseline_dqn.csv')\n",
    "robustness_results_baseline_dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_convert(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip('[]')\n",
    "    return float(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_398771/4249244110.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  robustness_results_baseline_dqn = robustness_results_baseline_dqn.applymap(clean_and_convert)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noise_rewards</th>\n",
       "      <th>perturbation_rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>914.1</td>\n",
       "      <td>907.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>823.0</td>\n",
       "      <td>718.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>844.3</td>\n",
       "      <td>897.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>908.7</td>\n",
       "      <td>916.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>914.0</td>\n",
       "      <td>905.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>912.1</td>\n",
       "      <td>835.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   noise_rewards  perturbation_rewards\n",
       "0          914.1                 907.2\n",
       "1          823.0                 718.9\n",
       "2          844.3                 897.7\n",
       "3          908.7                 916.7\n",
       "4          914.0                 905.1\n",
       "5          912.1                 835.9"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robustness_results_baseline_dqn = robustness_results_baseline_dqn.applymap(clean_and_convert)\n",
    "robustness_results_baseline_dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Baseline - PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Custom Environment - DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env = EnhancedCarRacing(render_mode=\"rgb_array\")\n",
    "custom_env = GrayscaleObservation(custom_env, keep_dim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1218 20:29:59.936213148 NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "best_model_custom_dqn = DQN.load('./models/custom/DQN/best_model.zip', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating robustness to observation noise...\n",
      "Running episode 0\n",
      "Perturbation episode: Total Reward = -7759.953796605264\n",
      "Evaluating robustness to environment perturbations...\n",
      "Running episode 0\n",
      "Perturbation episode: Total Reward = -4434.551418120928\n",
      "noise_rewards: Mean = -7759.953796605264, Std Dev = 0.0\n",
      "perturbation_rewards: Mean = -4434.551418120928, Std Dev = 0.0\n"
     ]
    }
   ],
   "source": [
    "robustness_results_custom_dqn = evaluate_robustness(best_model_custom_dqn, custom_env, num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating robustness to observation noise...\n",
      "Running episode 0\n",
      "Perturbation episode: Total Reward = -7584.80748308475\n",
      "Running episode 1\n",
      "Perturbation episode: Total Reward = -3840.5075875620796\n",
      "Running episode 2\n",
      "Perturbation episode: Total Reward = -7460.315858936726\n",
      "Running episode 3\n",
      "Perturbation episode: Total Reward = -7861.306102609049\n",
      "Running episode 4\n",
      "Perturbation episode: Total Reward = -7541.33890571855\n",
      "Evaluating robustness to environment perturbations...\n",
      "Running episode 0\n",
      "Perturbation episode: Total Reward = -7514.179581865233\n",
      "Running episode 1\n",
      "Perturbation episode: Total Reward = -3519.7626820016026\n",
      "Running episode 2\n",
      "Perturbation episode: Total Reward = -7947.5827114948215\n",
      "Running episode 3\n",
      "Perturbation episode: Total Reward = -9068.230437492533\n",
      "Running episode 4\n",
      "Perturbation episode: Total Reward = -4008.980884820603\n",
      "noise_rewards: Mean = -6857.655187582231, Std Dev = 1514.5869781322078\n",
      "perturbation_rewards: Mean = -6411.747259534958, Std Dev = 2225.664461851139\n"
     ]
    }
   ],
   "source": [
    "foo = evaluate_robustness(best_model_custom_dqn, custom_env, num_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_results_custom_dqn['noise_rewards'].append(foo['noise_rewards'])\n",
    "robustness_results_custom_dqn['perturbation_rewards'].append(foo['perturbation_rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'noise_rewards': [-7759.953796605264,\n",
       "  [-7858.43471366518],\n",
       "  [-4357.975981461611],\n",
       "  [-7663.295497849881],\n",
       "  [-8118.9301815847275]],\n",
       " 'perturbation_rewards': [-4434.551418120928,\n",
       "  [-7241.206349396435],\n",
       "  [-7743.716855095754],\n",
       "  [-7901.444085518807],\n",
       "  [-7828.554151708476]]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robustness_results_custom_dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m robustness_results_custom_dqn \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrobustness_results_custom_dqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m robustness_results_custom_dqn\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/robustness_results_custom_dqn.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m robustness_results_custom_dqn \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/robustness_results_custom_dqn.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "robustness_results_custom_dqn = pd.DataFrame(robustness_results_custom_dqn)\n",
    "robustness_results_custom_dqn.to_csv('results/robustness_results_custom_dqn.csv', index=False)\n",
    "robustness_results_custom_dqn = pd.read_csv('results/robustness_results_custom_dqn.csv')\n",
    "robustness_results_custom_dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Custom Environment - PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated or pre-loaded CSV paths\n",
    "# Replace these with your actual file paths\n",
    "baseline_dqn = pd.read_csv('results/robustness_results_baseline_dqn.csv')\n",
    "baseline_ppo = pd.read_csv('results/robustness_results_baseline_ppo.csv')\n",
    "custom_dqn = pd.read_csv('results/robustness_results_custom_dqn.csv')\n",
    "custom_ppo = pd.read_csv('results/robustness_results_custom_ppo.csv')\n",
    "\n",
    "# Calculate the mean of rewards for each model\n",
    "noise_means = {\n",
    "    \"Baseline DQN\": baseline_dqn[\"noise_rewards\"].mean(),\n",
    "    \"Baseline PPO\": baseline_ppo[\"noise_rewards\"].mean(),\n",
    "    \"Custom DQN\": custom_dqn[\"noise_rewards\"].mean(),\n",
    "    \"Custom PPO\": custom_ppo[\"noise_rewards\"].mean(),\n",
    "}\n",
    "\n",
    "perturbation_means = {\n",
    "    \"Baseline DQN\": baseline_dqn[\"perturbation_rewards\"].mean(),\n",
    "    \"Baseline PPO\": baseline_ppo[\"perturbation_rewards\"].mean(),\n",
    "    \"Custom DQN\": custom_dqn[\"perturbation_rewards\"].mean(),\n",
    "    \"Custom PPO\": custom_ppo[\"perturbation_rewards\"].mean(),\n",
    "}\n",
    "\n",
    "# Plot histogram for noise rewards\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(noise_means.keys(), noise_means.values(), color=['blue', 'green', 'orange', 'red'])\n",
    "plt.title(\"Comparison of Mean Noise Rewards\")\n",
    "plt.ylabel(\"Mean Noise Reward\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram for perturbation rewards\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(perturbation_means.keys(), perturbation_means.values(), color=['blue', 'green', 'orange', 'red'])\n",
    "plt.title(\"Comparison of Mean Perturbation Rewards\")\n",
    "plt.ylabel(\"Mean Perturbation Reward\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
