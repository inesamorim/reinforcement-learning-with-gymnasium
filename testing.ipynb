{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import GrayscaleObservation, ResizeObservation, RecordEpisodeStatistics, RecordVideo, TimeLimit\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "import os\n",
    "import gc\n",
    "from eval import *\n",
    "from custom_cr import EnhancedCarRacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Task Specific Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Robustness and Adaptability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Robustness to Observation Noise:\n",
    "   - It runs `num_episodes` episodes with added Gaussian noise to the observations.\n",
    "   - For each step, it adds noise to the observation before predicting an action.\n",
    "   - It accumulates the total reward for each episode and stores it in `noise_rewards`.\n",
    "\n",
    "2. Robustness to Environment Perturbations:\n",
    "   - It runs another set of episodes, this time applying random perturbations to the environment.\n",
    "   - With probability `perturbation_prob`, it adds uniform random noise to the observation.\n",
    "   - It accumulates the total reward for each episode and stores it in `perturbation_rewards`.\n",
    "\n",
    "3. Results Computation and Output:\n",
    "   - For each robustness scenario, it calculates and prints the mean and standard deviation of the rewards.\n",
    "   \n",
    "This function is designed to evaluate how well the trained model performs under different types of perturbations and variations, which is crucial for assessing the robustness and generalization capabilities of the reinforcement learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_robustness(model, env, num_episodes=10, noise_std=0.1, perturbation_prob=0.1):\n",
    "    \"\"\"\n",
    "    Evaluate the robustness of a trained model under various challenging conditions.\n",
    "\n",
    "    This function tests the model's ability to handle noisy observations, random perturbations, \n",
    "    and diverse initial states in the environment. Results include performance metrics such as \n",
    "    mean rewards and standard deviations under each condition.\n",
    "\n",
    "    Args:\n",
    "        model (BaseAlgorithm): ThKeysView(NpzFile './best_model/best_model_2.1.1.zip' with keys: data, pytorch_variables.pth, policy.pth, policy.optimizer.pth, _stable_baselines3_version...)e trained model to evaluate. Should support `.predict()` for action selection.\n",
    "        env (gym.Env): The environment in which the model will be tested.\n",
    "        num_episodes (int, optional): The number of episodes to run for each robustness scenario. Defaults to 10.\n",
    "        noise_std (float, optional): Standard deviation of Gaussian noise added to observations. Defaults to 0.1.\n",
    "        perturbation_prob (float, optional): Probability of applying random perturbations to observations. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with keys:\n",
    "            - \"noise_rewards\": List of total rewards for episodes with noisy observations.\n",
    "            - \"perturbation_rewards\": List of total rewards for episodes with random perturbations.\n",
    "            - \"initial_state_rewards\": List of total rewards for episodes starting from diverse initial states.\n",
    "        Each list includes rewards from `num_episodes` episodes.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"noise_rewards\": [],\n",
    "        \"perturbation_rewards\": []\n",
    "    }\n",
    "    \n",
    "    # Evaluate robustness to observation noise\n",
    "    print(\"Evaluating robustness to observation noise...\")\n",
    "    for _ in range(num_episodes):\n",
    "        print(f\"Running episode {_}\")\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Add Gaussian noise to observation\n",
    "            noisy_obs = obs + np.random.normal(0, noise_std, obs.shape)\n",
    "            action = model.predict(noisy_obs, deterministic=True)[0]\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        results[\"noise_rewards\"].append(total_reward)\n",
    "        print(f\"Perturbation episode: Total Reward = {total_reward}\")  # For debugging \n",
    "\n",
    "    # Evaluate robustness to environment perturbations\n",
    "    print(\"Evaluating robustness to environment perturbations...\")\n",
    "    for _ in range(num_episodes):\n",
    "        print(f\"Running episode {_}\")\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = model.predict(obs, deterministic=True)[0]\n",
    "            \n",
    "            # Apply random perturbations\n",
    "            if np.random.random() < perturbation_prob:\n",
    "                obs = obs + np.random.uniform(-0.5, 0.5, obs.shape)\n",
    "                \n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        print(f\"Perturbation episode: Total Reward = {total_reward}\")  # For debugging purposes only, remove in production code.  # Evaluate robustness to environment perturb\n",
    "        results[\"perturbation_rewards\"].append(total_reward)\n",
    "\n",
    "    # Compute and return mean and standard deviation for all scenarios\n",
    "    for key in results:\n",
    "        rewards = np.array(results[key])\n",
    "        print(f\"{key}: Mean = {rewards.mean()}, Std Dev = {rewards.std()}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Baseline - DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n",
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object exploration_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n",
      "/home/inesamorim46/3ano1sem/isia/reinforcement-learning-with-gymnasium/.venv/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.77GB > 1.04GB\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_model_baseline_dqn = DQN.load('./models/baseline/DQN/best_model.zip', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_results_baseline_dqn = evaluate_robustness(best_model_baseline_dqn, env, num_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating robustness to observation noise...\n",
      "Running episode 0\n",
      "Perturbation episode: Total Reward = 914.0999999999913\n",
      "Evaluating robustness to environment perturbations...\n",
      "Running episode 0\n",
      "Perturbation episode: Total Reward = 907.1999999999932\n",
      "noise_rewards: Mean = 914.0999999999913, Std Dev = 0.0\n",
      "perturbation_rewards: Mean = 907.1999999999932, Std Dev = 0.0\n"
     ]
    }
   ],
   "source": [
    "robustness_results_baseline_dqn = evaluate_robustness(best_model_baseline_dqn, env, num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'noise_rewards': [914.0999999999913],\n",
       " 'perturbation_rewards': [907.1999999999932]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robustness_results_baseline_dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating robustness to observation noise...\n",
      "Running episode 0\n",
      "Perturbation episode: Total Reward = 913.9999999999923\n",
      "Evaluating robustness to environment perturbations...\n",
      "Running episode 0\n",
      "Perturbation episode: Total Reward = 905.0999999999838\n",
      "noise_rewards: Mean = 913.9999999999923, Std Dev = 0.0\n",
      "perturbation_rewards: Mean = 905.0999999999838, Std Dev = 0.0\n"
     ]
    }
   ],
   "source": [
    "foo = evaluate_robustness(best_model_baseline_dqn, env, num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_results_baseline_dqn['noise_rewards'].append(foo['noise_rewards'])\n",
    "robustness_results_baseline_dqn['perturbation_rewards'].append(foo['perturbation_rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'noise_rewards': [914.0999999999913,\n",
       "  [822.9999999999645],\n",
       "  [844.2999999999753],\n",
       "  [908.6999999999848],\n",
       "  [913.9999999999923]],\n",
       " 'perturbation_rewards': [907.1999999999932,\n",
       "  [718.899999999942],\n",
       "  [897.69999999998],\n",
       "  [916.6999999999894],\n",
       "  [905.0999999999838]]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robustness_results_baseline_dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Baseline - PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Custom Environment - DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env = EnhancedCarRacing(render_mode=\"rgb_array\")\n",
    "custom_env = GrayscaleObservation(custom_env, keep_dim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_custom_dqn = DQN.load('./models/custom/best_model', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_results_custom_dqn = evaluate_robustness(best_model_custom_dqn, custom_env, num_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_results_custom_dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Custom Environment - PPO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
